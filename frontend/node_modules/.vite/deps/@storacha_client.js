import {
  bytes,
  sha256
} from "./chunk-OCOHWGTA.js";
import {
  __commonJS,
  __export,
  __privateAdd,
  __privateGet,
  __privateMethod,
  __privateSet,
  __publicField,
  __reExport,
  __toESM
} from "./chunk-547O27LD.js";

// node_modules/@ucanto/interface/src/lib.js
var require_lib = __commonJS({
  "node_modules/@ucanto/interface/src/lib.js"() {
  }
});

// node_modules/varint/encode.js
var require_encode = __commonJS({
  "node_modules/varint/encode.js"(exports2, module2) {
    module2.exports = encode32;
    var MSB3 = 128;
    var REST3 = 127;
    var MSBALL3 = ~REST3;
    var INT3 = Math.pow(2, 31);
    function encode32(num, out, offset2) {
      if (Number.MAX_SAFE_INTEGER && num > Number.MAX_SAFE_INTEGER) {
        encode32.bytes = 0;
        throw new RangeError("Could not encode varint");
      }
      out = out || [];
      offset2 = offset2 || 0;
      var oldOffset = offset2;
      while (num >= INT3) {
        out[offset2++] = num & 255 | MSB3;
        num /= 128;
      }
      while (num & MSBALL3) {
        out[offset2++] = num & 255 | MSB3;
        num >>>= 7;
      }
      out[offset2] = num | 0;
      encode32.bytes = offset2 - oldOffset + 1;
      return out;
    }
  }
});

// node_modules/varint/decode.js
var require_decode = __commonJS({
  "node_modules/varint/decode.js"(exports2, module2) {
    module2.exports = read8;
    var MSB3 = 128;
    var REST3 = 127;
    function read8(buf2, offset2) {
      var res = 0, offset2 = offset2 || 0, shift = 0, counter = offset2, b, l = buf2.length;
      do {
        if (counter >= l || shift > 49) {
          read8.bytes = 0;
          throw new RangeError("Could not decode varint");
        }
        b = buf2[counter++];
        res += shift < 28 ? (b & REST3) << shift : (b & REST3) * Math.pow(2, shift);
        shift += 7;
      } while (b >= MSB3);
      read8.bytes = counter - offset2;
      return res;
    }
  }
});

// node_modules/varint/length.js
var require_length = __commonJS({
  "node_modules/varint/length.js"(exports2, module2) {
    var N12 = Math.pow(2, 7);
    var N22 = Math.pow(2, 14);
    var N32 = Math.pow(2, 21);
    var N42 = Math.pow(2, 28);
    var N52 = Math.pow(2, 35);
    var N62 = Math.pow(2, 42);
    var N72 = Math.pow(2, 49);
    var N82 = Math.pow(2, 56);
    var N92 = Math.pow(2, 63);
    module2.exports = function(value) {
      return value < N12 ? 1 : value < N22 ? 2 : value < N32 ? 3 : value < N42 ? 4 : value < N52 ? 5 : value < N62 ? 6 : value < N72 ? 7 : value < N82 ? 8 : value < N92 ? 9 : 10;
    };
  }
});

// node_modules/varint/index.js
var require_varint = __commonJS({
  "node_modules/varint/index.js"(exports2, module2) {
    module2.exports = {
      encode: require_encode(),
      decode: require_decode(),
      encodingLength: require_length()
    };
  }
});

// browser-external:crypto
var require_crypto = __commonJS({
  "browser-external:crypto"(exports2, module2) {
    module2.exports = Object.create(new Proxy({}, {
      get(_, key) {
        if (key !== "__esModule" && key !== "__proto__" && key !== "constructor" && key !== "splice") {
          console.warn(`Module "crypto" has been externalized for browser compatibility. Cannot access "crypto.${key}" in client code. See https://vite.dev/guide/troubleshooting.html#module-externalized-for-browser-compatibility for more details.`);
        }
      }
    }));
  }
});

// node_modules/retry/lib/retry_operation.js
var require_retry_operation = __commonJS({
  "node_modules/retry/lib/retry_operation.js"(exports2, module2) {
    function RetryOperation(timeouts, options) {
      if (typeof options === "boolean") {
        options = { forever: options };
      }
      this._originalTimeouts = JSON.parse(JSON.stringify(timeouts));
      this._timeouts = timeouts;
      this._options = options || {};
      this._maxRetryTime = options && options.maxRetryTime || Infinity;
      this._fn = null;
      this._errors = [];
      this._attempts = 1;
      this._operationTimeout = null;
      this._operationTimeoutCb = null;
      this._timeout = null;
      this._operationStart = null;
      this._timer = null;
      if (this._options.forever) {
        this._cachedTimeouts = this._timeouts.slice(0);
      }
    }
    module2.exports = RetryOperation;
    RetryOperation.prototype.reset = function() {
      this._attempts = 1;
      this._timeouts = this._originalTimeouts.slice(0);
    };
    RetryOperation.prototype.stop = function() {
      if (this._timeout) {
        clearTimeout(this._timeout);
      }
      if (this._timer) {
        clearTimeout(this._timer);
      }
      this._timeouts = [];
      this._cachedTimeouts = null;
    };
    RetryOperation.prototype.retry = function(err) {
      if (this._timeout) {
        clearTimeout(this._timeout);
      }
      if (!err) {
        return false;
      }
      var currentTime = (/* @__PURE__ */ new Date()).getTime();
      if (err && currentTime - this._operationStart >= this._maxRetryTime) {
        this._errors.push(err);
        this._errors.unshift(new Error("RetryOperation timeout occurred"));
        return false;
      }
      this._errors.push(err);
      var timeout = this._timeouts.shift();
      if (timeout === void 0) {
        if (this._cachedTimeouts) {
          this._errors.splice(0, this._errors.length - 1);
          timeout = this._cachedTimeouts.slice(-1);
        } else {
          return false;
        }
      }
      var self2 = this;
      this._timer = setTimeout(function() {
        self2._attempts++;
        if (self2._operationTimeoutCb) {
          self2._timeout = setTimeout(function() {
            self2._operationTimeoutCb(self2._attempts);
          }, self2._operationTimeout);
          if (self2._options.unref) {
            self2._timeout.unref();
          }
        }
        self2._fn(self2._attempts);
      }, timeout);
      if (this._options.unref) {
        this._timer.unref();
      }
      return true;
    };
    RetryOperation.prototype.attempt = function(fn, timeoutOps) {
      this._fn = fn;
      if (timeoutOps) {
        if (timeoutOps.timeout) {
          this._operationTimeout = timeoutOps.timeout;
        }
        if (timeoutOps.cb) {
          this._operationTimeoutCb = timeoutOps.cb;
        }
      }
      var self2 = this;
      if (this._operationTimeoutCb) {
        this._timeout = setTimeout(function() {
          self2._operationTimeoutCb();
        }, self2._operationTimeout);
      }
      this._operationStart = (/* @__PURE__ */ new Date()).getTime();
      this._fn(this._attempts);
    };
    RetryOperation.prototype.try = function(fn) {
      console.log("Using RetryOperation.try() is deprecated");
      this.attempt(fn);
    };
    RetryOperation.prototype.start = function(fn) {
      console.log("Using RetryOperation.start() is deprecated");
      this.attempt(fn);
    };
    RetryOperation.prototype.start = RetryOperation.prototype.try;
    RetryOperation.prototype.errors = function() {
      return this._errors;
    };
    RetryOperation.prototype.attempts = function() {
      return this._attempts;
    };
    RetryOperation.prototype.mainError = function() {
      if (this._errors.length === 0) {
        return null;
      }
      var counts = {};
      var mainError = null;
      var mainErrorCount = 0;
      for (var i = 0; i < this._errors.length; i++) {
        var error4 = this._errors[i];
        var message = error4.message;
        var count = (counts[message] || 0) + 1;
        counts[message] = count;
        if (count >= mainErrorCount) {
          mainError = error4;
          mainErrorCount = count;
        }
      }
      return mainError;
    };
  }
});

// node_modules/retry/lib/retry.js
var require_retry = __commonJS({
  "node_modules/retry/lib/retry.js"(exports2) {
    var RetryOperation = require_retry_operation();
    exports2.operation = function(options) {
      var timeouts = exports2.timeouts(options);
      return new RetryOperation(timeouts, {
        forever: options && (options.forever || options.retries === Infinity),
        unref: options && options.unref,
        maxRetryTime: options && options.maxRetryTime
      });
    };
    exports2.timeouts = function(options) {
      if (options instanceof Array) {
        return [].concat(options);
      }
      var opts = {
        retries: 10,
        factor: 2,
        minTimeout: 1 * 1e3,
        maxTimeout: Infinity,
        randomize: false
      };
      for (var key in options) {
        opts[key] = options[key];
      }
      if (opts.minTimeout > opts.maxTimeout) {
        throw new Error("minTimeout is greater than maxTimeout");
      }
      var timeouts = [];
      for (var i = 0; i < opts.retries; i++) {
        timeouts.push(this.createTimeout(i, opts));
      }
      if (options && options.forever && !timeouts.length) {
        timeouts.push(this.createTimeout(i, opts));
      }
      timeouts.sort(function(a, b) {
        return a - b;
      });
      return timeouts;
    };
    exports2.createTimeout = function(attempt, opts) {
      var random = opts.randomize ? Math.random() + 1 : 1;
      var timeout = Math.round(random * Math.max(opts.minTimeout, 1) * Math.pow(opts.factor, attempt));
      timeout = Math.min(timeout, opts.maxTimeout);
      return timeout;
    };
    exports2.wrap = function(obj, options, methods) {
      if (options instanceof Array) {
        methods = options;
        options = null;
      }
      if (!methods) {
        methods = [];
        for (var key in obj) {
          if (typeof obj[key] === "function") {
            methods.push(key);
          }
        }
      }
      for (var i = 0; i < methods.length; i++) {
        var method = methods[i];
        var original = obj[method];
        obj[method] = (function retryWrapper(original2) {
          var op = exports2.operation(options);
          var args = Array.prototype.slice.call(arguments, 1);
          var callback = args.pop();
          args.push(function(err) {
            if (op.retry(err)) {
              return;
            }
            if (err) {
              arguments[0] = op.mainError();
            }
            callback.apply(this, arguments);
          });
          op.attempt(function() {
            original2.apply(obj, args);
          });
        }).bind(obj, original);
        obj[method].options = options;
      }
    };
  }
});

// node_modules/retry/index.js
var require_retry2 = __commonJS({
  "node_modules/retry/index.js"(exports2, module2) {
    module2.exports = require_retry();
  }
});

// node_modules/@protobufjs/aspromise/index.js
var require_aspromise = __commonJS({
  "node_modules/@protobufjs/aspromise/index.js"(exports2, module2) {
    "use strict";
    module2.exports = asPromise;
    function asPromise(fn, ctx) {
      var params = new Array(arguments.length - 1), offset2 = 0, index3 = 2, pending = true;
      while (index3 < arguments.length)
        params[offset2++] = arguments[index3++];
      return new Promise(function executor(resolve, reject) {
        params[offset2] = function callback(err) {
          if (pending) {
            pending = false;
            if (err)
              reject(err);
            else {
              var params2 = new Array(arguments.length - 1), offset3 = 0;
              while (offset3 < params2.length)
                params2[offset3++] = arguments[offset3];
              resolve.apply(null, params2);
            }
          }
        };
        try {
          fn.apply(ctx || null, params);
        } catch (err) {
          if (pending) {
            pending = false;
            reject(err);
          }
        }
      });
    }
  }
});

// node_modules/@protobufjs/base64/index.js
var require_base64 = __commonJS({
  "node_modules/@protobufjs/base64/index.js"(exports2) {
    "use strict";
    var base643 = exports2;
    base643.length = function length2(string3) {
      var p = string3.length;
      if (!p)
        return 0;
      var n = 0;
      while (--p % 4 > 1 && string3.charAt(p) === "=")
        ++n;
      return Math.ceil(string3.length * 3) / 4 - n;
    };
    var b64 = new Array(64);
    var s64 = new Array(123);
    for (i = 0; i < 64; )
      s64[b64[i] = i < 26 ? i + 65 : i < 52 ? i + 71 : i < 62 ? i - 4 : i - 59 | 43] = i++;
    var i;
    base643.encode = function encode32(buffer2, start, end) {
      var parts = null, chunk = [];
      var i2 = 0, j = 0, t;
      while (start < end) {
        var b = buffer2[start++];
        switch (j) {
          case 0:
            chunk[i2++] = b64[b >> 2];
            t = (b & 3) << 4;
            j = 1;
            break;
          case 1:
            chunk[i2++] = b64[t | b >> 4];
            t = (b & 15) << 2;
            j = 2;
            break;
          case 2:
            chunk[i2++] = b64[t | b >> 6];
            chunk[i2++] = b64[b & 63];
            j = 0;
            break;
        }
        if (i2 > 8191) {
          (parts || (parts = [])).push(String.fromCharCode.apply(String, chunk));
          i2 = 0;
        }
      }
      if (j) {
        chunk[i2++] = b64[t];
        chunk[i2++] = 61;
        if (j === 1)
          chunk[i2++] = 61;
      }
      if (parts) {
        if (i2)
          parts.push(String.fromCharCode.apply(String, chunk.slice(0, i2)));
        return parts.join("");
      }
      return String.fromCharCode.apply(String, chunk.slice(0, i2));
    };
    var invalidEncoding = "invalid encoding";
    base643.decode = function decode36(string3, buffer2, offset2) {
      var start = offset2;
      var j = 0, t;
      for (var i2 = 0; i2 < string3.length; ) {
        var c = string3.charCodeAt(i2++);
        if (c === 61 && j > 1)
          break;
        if ((c = s64[c]) === void 0)
          throw Error(invalidEncoding);
        switch (j) {
          case 0:
            t = c;
            j = 1;
            break;
          case 1:
            buffer2[offset2++] = t << 2 | (c & 48) >> 4;
            t = c;
            j = 2;
            break;
          case 2:
            buffer2[offset2++] = (t & 15) << 4 | (c & 60) >> 2;
            t = c;
            j = 3;
            break;
          case 3:
            buffer2[offset2++] = (t & 3) << 6 | c;
            j = 0;
            break;
        }
      }
      if (j === 1)
        throw Error(invalidEncoding);
      return offset2 - start;
    };
    base643.test = function test(string3) {
      return /^(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?$/.test(string3);
    };
  }
});

// node_modules/@protobufjs/eventemitter/index.js
var require_eventemitter = __commonJS({
  "node_modules/@protobufjs/eventemitter/index.js"(exports2, module2) {
    "use strict";
    module2.exports = EventEmitter;
    function EventEmitter() {
      this._listeners = {};
    }
    EventEmitter.prototype.on = function on(evt, fn, ctx) {
      (this._listeners[evt] || (this._listeners[evt] = [])).push({
        fn,
        ctx: ctx || this
      });
      return this;
    };
    EventEmitter.prototype.off = function off(evt, fn) {
      if (evt === void 0)
        this._listeners = {};
      else {
        if (fn === void 0)
          this._listeners[evt] = [];
        else {
          var listeners = this._listeners[evt];
          for (var i = 0; i < listeners.length; )
            if (listeners[i].fn === fn)
              listeners.splice(i, 1);
            else
              ++i;
        }
      }
      return this;
    };
    EventEmitter.prototype.emit = function emit(evt) {
      var listeners = this._listeners[evt];
      if (listeners) {
        var args = [], i = 1;
        for (; i < arguments.length; )
          args.push(arguments[i++]);
        for (i = 0; i < listeners.length; )
          listeners[i].fn.apply(listeners[i++].ctx, args);
      }
      return this;
    };
  }
});

// node_modules/@protobufjs/float/index.js
var require_float = __commonJS({
  "node_modules/@protobufjs/float/index.js"(exports2, module2) {
    "use strict";
    module2.exports = factory(factory);
    function factory(exports3) {
      if (typeof Float32Array !== "undefined") (function() {
        var f32 = new Float32Array([-0]), f8b = new Uint8Array(f32.buffer), le = f8b[3] === 128;
        function writeFloat_f32_cpy(val, buf2, pos) {
          f32[0] = val;
          buf2[pos] = f8b[0];
          buf2[pos + 1] = f8b[1];
          buf2[pos + 2] = f8b[2];
          buf2[pos + 3] = f8b[3];
        }
        function writeFloat_f32_rev(val, buf2, pos) {
          f32[0] = val;
          buf2[pos] = f8b[3];
          buf2[pos + 1] = f8b[2];
          buf2[pos + 2] = f8b[1];
          buf2[pos + 3] = f8b[0];
        }
        exports3.writeFloatLE = le ? writeFloat_f32_cpy : writeFloat_f32_rev;
        exports3.writeFloatBE = le ? writeFloat_f32_rev : writeFloat_f32_cpy;
        function readFloat_f32_cpy(buf2, pos) {
          f8b[0] = buf2[pos];
          f8b[1] = buf2[pos + 1];
          f8b[2] = buf2[pos + 2];
          f8b[3] = buf2[pos + 3];
          return f32[0];
        }
        function readFloat_f32_rev(buf2, pos) {
          f8b[3] = buf2[pos];
          f8b[2] = buf2[pos + 1];
          f8b[1] = buf2[pos + 2];
          f8b[0] = buf2[pos + 3];
          return f32[0];
        }
        exports3.readFloatLE = le ? readFloat_f32_cpy : readFloat_f32_rev;
        exports3.readFloatBE = le ? readFloat_f32_rev : readFloat_f32_cpy;
      })();
      else (function() {
        function writeFloat_ieee754(writeUint, val, buf2, pos) {
          var sign2 = val < 0 ? 1 : 0;
          if (sign2)
            val = -val;
          if (val === 0)
            writeUint(1 / val > 0 ? (
              /* positive */
              0
            ) : (
              /* negative 0 */
              2147483648
            ), buf2, pos);
          else if (isNaN(val))
            writeUint(2143289344, buf2, pos);
          else if (val > 34028234663852886e22)
            writeUint((sign2 << 31 | 2139095040) >>> 0, buf2, pos);
          else if (val < 11754943508222875e-54)
            writeUint((sign2 << 31 | Math.round(val / 1401298464324817e-60)) >>> 0, buf2, pos);
          else {
            var exponent = Math.floor(Math.log(val) / Math.LN2), mantissa = Math.round(val * Math.pow(2, -exponent) * 8388608) & 8388607;
            writeUint((sign2 << 31 | exponent + 127 << 23 | mantissa) >>> 0, buf2, pos);
          }
        }
        exports3.writeFloatLE = writeFloat_ieee754.bind(null, writeUintLE);
        exports3.writeFloatBE = writeFloat_ieee754.bind(null, writeUintBE);
        function readFloat_ieee754(readUint, buf2, pos) {
          var uint = readUint(buf2, pos), sign2 = (uint >> 31) * 2 + 1, exponent = uint >>> 23 & 255, mantissa = uint & 8388607;
          return exponent === 255 ? mantissa ? NaN : sign2 * Infinity : exponent === 0 ? sign2 * 1401298464324817e-60 * mantissa : sign2 * Math.pow(2, exponent - 150) * (mantissa + 8388608);
        }
        exports3.readFloatLE = readFloat_ieee754.bind(null, readUintLE);
        exports3.readFloatBE = readFloat_ieee754.bind(null, readUintBE);
      })();
      if (typeof Float64Array !== "undefined") (function() {
        var f64 = new Float64Array([-0]), f8b = new Uint8Array(f64.buffer), le = f8b[7] === 128;
        function writeDouble_f64_cpy(val, buf2, pos) {
          f64[0] = val;
          buf2[pos] = f8b[0];
          buf2[pos + 1] = f8b[1];
          buf2[pos + 2] = f8b[2];
          buf2[pos + 3] = f8b[3];
          buf2[pos + 4] = f8b[4];
          buf2[pos + 5] = f8b[5];
          buf2[pos + 6] = f8b[6];
          buf2[pos + 7] = f8b[7];
        }
        function writeDouble_f64_rev(val, buf2, pos) {
          f64[0] = val;
          buf2[pos] = f8b[7];
          buf2[pos + 1] = f8b[6];
          buf2[pos + 2] = f8b[5];
          buf2[pos + 3] = f8b[4];
          buf2[pos + 4] = f8b[3];
          buf2[pos + 5] = f8b[2];
          buf2[pos + 6] = f8b[1];
          buf2[pos + 7] = f8b[0];
        }
        exports3.writeDoubleLE = le ? writeDouble_f64_cpy : writeDouble_f64_rev;
        exports3.writeDoubleBE = le ? writeDouble_f64_rev : writeDouble_f64_cpy;
        function readDouble_f64_cpy(buf2, pos) {
          f8b[0] = buf2[pos];
          f8b[1] = buf2[pos + 1];
          f8b[2] = buf2[pos + 2];
          f8b[3] = buf2[pos + 3];
          f8b[4] = buf2[pos + 4];
          f8b[5] = buf2[pos + 5];
          f8b[6] = buf2[pos + 6];
          f8b[7] = buf2[pos + 7];
          return f64[0];
        }
        function readDouble_f64_rev(buf2, pos) {
          f8b[7] = buf2[pos];
          f8b[6] = buf2[pos + 1];
          f8b[5] = buf2[pos + 2];
          f8b[4] = buf2[pos + 3];
          f8b[3] = buf2[pos + 4];
          f8b[2] = buf2[pos + 5];
          f8b[1] = buf2[pos + 6];
          f8b[0] = buf2[pos + 7];
          return f64[0];
        }
        exports3.readDoubleLE = le ? readDouble_f64_cpy : readDouble_f64_rev;
        exports3.readDoubleBE = le ? readDouble_f64_rev : readDouble_f64_cpy;
      })();
      else (function() {
        function writeDouble_ieee754(writeUint, off0, off1, val, buf2, pos) {
          var sign2 = val < 0 ? 1 : 0;
          if (sign2)
            val = -val;
          if (val === 0) {
            writeUint(0, buf2, pos + off0);
            writeUint(1 / val > 0 ? (
              /* positive */
              0
            ) : (
              /* negative 0 */
              2147483648
            ), buf2, pos + off1);
          } else if (isNaN(val)) {
            writeUint(0, buf2, pos + off0);
            writeUint(2146959360, buf2, pos + off1);
          } else if (val > 17976931348623157e292) {
            writeUint(0, buf2, pos + off0);
            writeUint((sign2 << 31 | 2146435072) >>> 0, buf2, pos + off1);
          } else {
            var mantissa;
            if (val < 22250738585072014e-324) {
              mantissa = val / 5e-324;
              writeUint(mantissa >>> 0, buf2, pos + off0);
              writeUint((sign2 << 31 | mantissa / 4294967296) >>> 0, buf2, pos + off1);
            } else {
              var exponent = Math.floor(Math.log(val) / Math.LN2);
              if (exponent === 1024)
                exponent = 1023;
              mantissa = val * Math.pow(2, -exponent);
              writeUint(mantissa * 4503599627370496 >>> 0, buf2, pos + off0);
              writeUint((sign2 << 31 | exponent + 1023 << 20 | mantissa * 1048576 & 1048575) >>> 0, buf2, pos + off1);
            }
          }
        }
        exports3.writeDoubleLE = writeDouble_ieee754.bind(null, writeUintLE, 0, 4);
        exports3.writeDoubleBE = writeDouble_ieee754.bind(null, writeUintBE, 4, 0);
        function readDouble_ieee754(readUint, off0, off1, buf2, pos) {
          var lo = readUint(buf2, pos + off0), hi = readUint(buf2, pos + off1);
          var sign2 = (hi >> 31) * 2 + 1, exponent = hi >>> 20 & 2047, mantissa = 4294967296 * (hi & 1048575) + lo;
          return exponent === 2047 ? mantissa ? NaN : sign2 * Infinity : exponent === 0 ? sign2 * 5e-324 * mantissa : sign2 * Math.pow(2, exponent - 1075) * (mantissa + 4503599627370496);
        }
        exports3.readDoubleLE = readDouble_ieee754.bind(null, readUintLE, 0, 4);
        exports3.readDoubleBE = readDouble_ieee754.bind(null, readUintBE, 4, 0);
      })();
      return exports3;
    }
    function writeUintLE(val, buf2, pos) {
      buf2[pos] = val & 255;
      buf2[pos + 1] = val >>> 8 & 255;
      buf2[pos + 2] = val >>> 16 & 255;
      buf2[pos + 3] = val >>> 24;
    }
    function writeUintBE(val, buf2, pos) {
      buf2[pos] = val >>> 24;
      buf2[pos + 1] = val >>> 16 & 255;
      buf2[pos + 2] = val >>> 8 & 255;
      buf2[pos + 3] = val & 255;
    }
    function readUintLE(buf2, pos) {
      return (buf2[pos] | buf2[pos + 1] << 8 | buf2[pos + 2] << 16 | buf2[pos + 3] << 24) >>> 0;
    }
    function readUintBE(buf2, pos) {
      return (buf2[pos] << 24 | buf2[pos + 1] << 16 | buf2[pos + 2] << 8 | buf2[pos + 3]) >>> 0;
    }
  }
});

// node_modules/@protobufjs/inquire/index.js
var require_inquire = __commonJS({
  "node_modules/@protobufjs/inquire/index.js"(exports, module) {
    "use strict";
    module.exports = inquire;
    function inquire(moduleName) {
      try {
        var mod = eval("quire".replace(/^/, "re"))(moduleName);
        if (mod && (mod.length || Object.keys(mod).length))
          return mod;
      } catch (e) {
      }
      return null;
    }
  }
});

// node_modules/@protobufjs/utf8/index.js
var require_utf8 = __commonJS({
  "node_modules/@protobufjs/utf8/index.js"(exports2) {
    "use strict";
    var utf84 = exports2;
    utf84.length = function utf8_length(string3) {
      var len = 0, c = 0;
      for (var i = 0; i < string3.length; ++i) {
        c = string3.charCodeAt(i);
        if (c < 128)
          len += 1;
        else if (c < 2048)
          len += 2;
        else if ((c & 64512) === 55296 && (string3.charCodeAt(i + 1) & 64512) === 56320) {
          ++i;
          len += 4;
        } else
          len += 3;
      }
      return len;
    };
    utf84.read = function utf8_read(buffer2, start, end) {
      var len = end - start;
      if (len < 1)
        return "";
      var parts = null, chunk = [], i = 0, t;
      while (start < end) {
        t = buffer2[start++];
        if (t < 128)
          chunk[i++] = t;
        else if (t > 191 && t < 224)
          chunk[i++] = (t & 31) << 6 | buffer2[start++] & 63;
        else if (t > 239 && t < 365) {
          t = ((t & 7) << 18 | (buffer2[start++] & 63) << 12 | (buffer2[start++] & 63) << 6 | buffer2[start++] & 63) - 65536;
          chunk[i++] = 55296 + (t >> 10);
          chunk[i++] = 56320 + (t & 1023);
        } else
          chunk[i++] = (t & 15) << 12 | (buffer2[start++] & 63) << 6 | buffer2[start++] & 63;
        if (i > 8191) {
          (parts || (parts = [])).push(String.fromCharCode.apply(String, chunk));
          i = 0;
        }
      }
      if (parts) {
        if (i)
          parts.push(String.fromCharCode.apply(String, chunk.slice(0, i)));
        return parts.join("");
      }
      return String.fromCharCode.apply(String, chunk.slice(0, i));
    };
    utf84.write = function utf8_write(string3, buffer2, offset2) {
      var start = offset2, c1, c2;
      for (var i = 0; i < string3.length; ++i) {
        c1 = string3.charCodeAt(i);
        if (c1 < 128) {
          buffer2[offset2++] = c1;
        } else if (c1 < 2048) {
          buffer2[offset2++] = c1 >> 6 | 192;
          buffer2[offset2++] = c1 & 63 | 128;
        } else if ((c1 & 64512) === 55296 && ((c2 = string3.charCodeAt(i + 1)) & 64512) === 56320) {
          c1 = 65536 + ((c1 & 1023) << 10) + (c2 & 1023);
          ++i;
          buffer2[offset2++] = c1 >> 18 | 240;
          buffer2[offset2++] = c1 >> 12 & 63 | 128;
          buffer2[offset2++] = c1 >> 6 & 63 | 128;
          buffer2[offset2++] = c1 & 63 | 128;
        } else {
          buffer2[offset2++] = c1 >> 12 | 224;
          buffer2[offset2++] = c1 >> 6 & 63 | 128;
          buffer2[offset2++] = c1 & 63 | 128;
        }
      }
      return offset2 - start;
    };
  }
});

// node_modules/@protobufjs/pool/index.js
var require_pool = __commonJS({
  "node_modules/@protobufjs/pool/index.js"(exports2, module2) {
    "use strict";
    module2.exports = pool;
    function pool(alloc3, slice3, size5) {
      var SIZE3 = size5 || 8192;
      var MAX = SIZE3 >>> 1;
      var slab = null;
      var offset2 = SIZE3;
      return function pool_alloc(size6) {
        if (size6 < 1 || size6 > MAX)
          return alloc3(size6);
        if (offset2 + size6 > SIZE3) {
          slab = alloc3(SIZE3);
          offset2 = 0;
        }
        var buf2 = slice3.call(slab, offset2, offset2 += size6);
        if (offset2 & 7)
          offset2 = (offset2 | 7) + 1;
        return buf2;
      };
    }
  }
});

// node_modules/protobufjs/src/util/longbits.js
var require_longbits = __commonJS({
  "node_modules/protobufjs/src/util/longbits.js"(exports2, module2) {
    "use strict";
    module2.exports = LongBits;
    var util = require_minimal();
    function LongBits(lo, hi) {
      this.lo = lo >>> 0;
      this.hi = hi >>> 0;
    }
    var zero = LongBits.zero = new LongBits(0, 0);
    zero.toNumber = function() {
      return 0;
    };
    zero.zzEncode = zero.zzDecode = function() {
      return this;
    };
    zero.length = function() {
      return 1;
    };
    var zeroHash = LongBits.zeroHash = "\0\0\0\0\0\0\0\0";
    LongBits.fromNumber = function fromNumber(value) {
      if (value === 0)
        return zero;
      var sign2 = value < 0;
      if (sign2)
        value = -value;
      var lo = value >>> 0, hi = (value - lo) / 4294967296 >>> 0;
      if (sign2) {
        hi = ~hi >>> 0;
        lo = ~lo >>> 0;
        if (++lo > 4294967295) {
          lo = 0;
          if (++hi > 4294967295)
            hi = 0;
        }
      }
      return new LongBits(lo, hi);
    };
    LongBits.from = function from18(value) {
      if (typeof value === "number")
        return LongBits.fromNumber(value);
      if (util.isString(value)) {
        if (util.Long)
          value = util.Long.fromString(value);
        else
          return LongBits.fromNumber(parseInt(value, 10));
      }
      return value.low || value.high ? new LongBits(value.low >>> 0, value.high >>> 0) : zero;
    };
    LongBits.prototype.toNumber = function toNumber(unsigned) {
      if (!unsigned && this.hi >>> 31) {
        var lo = ~this.lo + 1 >>> 0, hi = ~this.hi >>> 0;
        if (!lo)
          hi = hi + 1 >>> 0;
        return -(lo + hi * 4294967296);
      }
      return this.lo + this.hi * 4294967296;
    };
    LongBits.prototype.toLong = function toLong(unsigned) {
      return util.Long ? new util.Long(this.lo | 0, this.hi | 0, Boolean(unsigned)) : { low: this.lo | 0, high: this.hi | 0, unsigned: Boolean(unsigned) };
    };
    var charCodeAt = String.prototype.charCodeAt;
    LongBits.fromHash = function fromHash(hash) {
      if (hash === zeroHash)
        return zero;
      return new LongBits(
        (charCodeAt.call(hash, 0) | charCodeAt.call(hash, 1) << 8 | charCodeAt.call(hash, 2) << 16 | charCodeAt.call(hash, 3) << 24) >>> 0,
        (charCodeAt.call(hash, 4) | charCodeAt.call(hash, 5) << 8 | charCodeAt.call(hash, 6) << 16 | charCodeAt.call(hash, 7) << 24) >>> 0
      );
    };
    LongBits.prototype.toHash = function toHash() {
      return String.fromCharCode(
        this.lo & 255,
        this.lo >>> 8 & 255,
        this.lo >>> 16 & 255,
        this.lo >>> 24,
        this.hi & 255,
        this.hi >>> 8 & 255,
        this.hi >>> 16 & 255,
        this.hi >>> 24
      );
    };
    LongBits.prototype.zzEncode = function zzEncode() {
      var mask2 = this.hi >> 31;
      this.hi = ((this.hi << 1 | this.lo >>> 31) ^ mask2) >>> 0;
      this.lo = (this.lo << 1 ^ mask2) >>> 0;
      return this;
    };
    LongBits.prototype.zzDecode = function zzDecode() {
      var mask2 = -(this.lo & 1);
      this.lo = ((this.lo >>> 1 | this.hi << 31) ^ mask2) >>> 0;
      this.hi = (this.hi >>> 1 ^ mask2) >>> 0;
      return this;
    };
    LongBits.prototype.length = function length2() {
      var part0 = this.lo, part1 = (this.lo >>> 28 | this.hi << 4) >>> 0, part2 = this.hi >>> 24;
      return part2 === 0 ? part1 === 0 ? part0 < 16384 ? part0 < 128 ? 1 : 2 : part0 < 2097152 ? 3 : 4 : part1 < 16384 ? part1 < 128 ? 5 : 6 : part1 < 2097152 ? 7 : 8 : part2 < 128 ? 9 : 10;
    };
  }
});

// node_modules/protobufjs/src/util/minimal.js
var require_minimal = __commonJS({
  "node_modules/protobufjs/src/util/minimal.js"(exports2) {
    "use strict";
    var util = exports2;
    util.asPromise = require_aspromise();
    util.base64 = require_base64();
    util.EventEmitter = require_eventemitter();
    util.float = require_float();
    util.inquire = require_inquire();
    util.utf8 = require_utf8();
    util.pool = require_pool();
    util.LongBits = require_longbits();
    util.isNode = Boolean(typeof global !== "undefined" && global && global.process && global.process.versions && global.process.versions.node);
    util.global = util.isNode && global || typeof window !== "undefined" && window || typeof self !== "undefined" && self || exports2;
    util.emptyArray = Object.freeze ? Object.freeze([]) : (
      /* istanbul ignore next */
      []
    );
    util.emptyObject = Object.freeze ? Object.freeze({}) : (
      /* istanbul ignore next */
      {}
    );
    util.isInteger = Number.isInteger || /* istanbul ignore next */
    function isInteger(value) {
      return typeof value === "number" && isFinite(value) && Math.floor(value) === value;
    };
    util.isString = function isString(value) {
      return typeof value === "string" || value instanceof String;
    };
    util.isObject = function isObject(value) {
      return value && typeof value === "object";
    };
    util.isset = /**
     * Checks if a property on a message is considered to be present.
     * @param {Object} obj Plain object or message instance
     * @param {string} prop Property name
     * @returns {boolean} `true` if considered to be present, otherwise `false`
     */
    util.isSet = function isSet(obj, prop) {
      var value = obj[prop];
      if (value != null && obj.hasOwnProperty(prop))
        return typeof value !== "object" || (Array.isArray(value) ? value.length : Object.keys(value).length) > 0;
      return false;
    };
    util.Buffer = function() {
      try {
        var Buffer = util.inquire("buffer").Buffer;
        return Buffer.prototype.utf8Write ? Buffer : (
          /* istanbul ignore next */
          null
        );
      } catch (e) {
        return null;
      }
    }();
    util._Buffer_from = null;
    util._Buffer_allocUnsafe = null;
    util.newBuffer = function newBuffer(sizeOrArray) {
      return typeof sizeOrArray === "number" ? util.Buffer ? util._Buffer_allocUnsafe(sizeOrArray) : new util.Array(sizeOrArray) : util.Buffer ? util._Buffer_from(sizeOrArray) : typeof Uint8Array === "undefined" ? sizeOrArray : new Uint8Array(sizeOrArray);
    };
    util.Array = typeof Uint8Array !== "undefined" ? Uint8Array : Array;
    util.Long = /* istanbul ignore next */
    util.global.dcodeIO && /* istanbul ignore next */
    util.global.dcodeIO.Long || /* istanbul ignore next */
    util.global.Long || util.inquire("long");
    util.key2Re = /^true|false|0|1$/;
    util.key32Re = /^-?(?:0|[1-9][0-9]*)$/;
    util.key64Re = /^(?:[\\x00-\\xff]{8}|-?(?:0|[1-9][0-9]*))$/;
    util.longToHash = function longToHash(value) {
      return value ? util.LongBits.from(value).toHash() : util.LongBits.zeroHash;
    };
    util.longFromHash = function longFromHash(hash, unsigned) {
      var bits = util.LongBits.fromHash(hash);
      if (util.Long)
        return util.Long.fromBits(bits.lo, bits.hi, unsigned);
      return bits.toNumber(Boolean(unsigned));
    };
    function merge(dst, src2, ifNotSet) {
      for (var keys2 = Object.keys(src2), i = 0; i < keys2.length; ++i)
        if (dst[keys2[i]] === void 0 || !ifNotSet)
          dst[keys2[i]] = src2[keys2[i]];
      return dst;
    }
    util.merge = merge;
    util.lcFirst = function lcFirst(str) {
      return str.charAt(0).toLowerCase() + str.substring(1);
    };
    function newError(name15) {
      function CustomError(message, properties) {
        if (!(this instanceof CustomError))
          return new CustomError(message, properties);
        Object.defineProperty(this, "message", { get: function() {
          return message;
        } });
        if (Error.captureStackTrace)
          Error.captureStackTrace(this, CustomError);
        else
          Object.defineProperty(this, "stack", { value: new Error().stack || "" });
        if (properties)
          merge(this, properties);
      }
      CustomError.prototype = Object.create(Error.prototype, {
        constructor: {
          value: CustomError,
          writable: true,
          enumerable: false,
          configurable: true
        },
        name: {
          get: function get19() {
            return name15;
          },
          set: void 0,
          enumerable: false,
          // configurable: false would accurately preserve the behavior of
          // the original, but I'm guessing that was not intentional.
          // For an actual error subclass, this property would
          // be configurable.
          configurable: true
        },
        toString: {
          value: function value() {
            return this.name + ": " + this.message;
          },
          writable: true,
          enumerable: false,
          configurable: true
        }
      });
      return CustomError;
    }
    util.newError = newError;
    util.ProtocolError = newError("ProtocolError");
    util.oneOfGetter = function getOneOf(fieldNames) {
      var fieldMap = {};
      for (var i = 0; i < fieldNames.length; ++i)
        fieldMap[fieldNames[i]] = 1;
      return function() {
        for (var keys2 = Object.keys(this), i2 = keys2.length - 1; i2 > -1; --i2)
          if (fieldMap[keys2[i2]] === 1 && this[keys2[i2]] !== void 0 && this[keys2[i2]] !== null)
            return keys2[i2];
      };
    };
    util.oneOfSetter = function setOneOf(fieldNames) {
      return function(name15) {
        for (var i = 0; i < fieldNames.length; ++i)
          if (fieldNames[i] !== name15)
            delete this[fieldNames[i]];
      };
    };
    util.toJSONOptions = {
      longs: String,
      enums: String,
      bytes: String,
      json: true
    };
    util._configure = function() {
      var Buffer = util.Buffer;
      if (!Buffer) {
        util._Buffer_from = util._Buffer_allocUnsafe = null;
        return;
      }
      util._Buffer_from = Buffer.from !== Uint8Array.from && Buffer.from || /* istanbul ignore next */
      function Buffer_from(value, encoding) {
        return new Buffer(value, encoding);
      };
      util._Buffer_allocUnsafe = Buffer.allocUnsafe || /* istanbul ignore next */
      function Buffer_allocUnsafe(size5) {
        return new Buffer(size5);
      };
    };
  }
});

// node_modules/protobufjs/src/writer.js
var require_writer = __commonJS({
  "node_modules/protobufjs/src/writer.js"(exports2, module2) {
    "use strict";
    module2.exports = Writer2;
    var util = require_minimal();
    var BufferWriter;
    var LongBits = util.LongBits;
    var base643 = util.base64;
    var utf84 = util.utf8;
    function Op(fn, len, val) {
      this.fn = fn;
      this.len = len;
      this.next = void 0;
      this.val = val;
    }
    function noop2() {
    }
    function State2(writer) {
      this.head = writer.head;
      this.tail = writer.tail;
      this.len = writer.len;
      this.next = writer.states;
    }
    function Writer2() {
      this.len = 0;
      this.head = new Op(noop2, 0, 0);
      this.tail = this.head;
      this.states = null;
    }
    var create16 = function create17() {
      return util.Buffer ? function create_buffer_setup() {
        return (Writer2.create = function create_buffer() {
          return new BufferWriter();
        })();
      } : function create_array() {
        return new Writer2();
      };
    };
    Writer2.create = create16();
    Writer2.alloc = function alloc3(size5) {
      return new util.Array(size5);
    };
    if (util.Array !== Array)
      Writer2.alloc = util.pool(Writer2.alloc, util.Array.prototype.subarray);
    Writer2.prototype._push = function push2(fn, len, val) {
      this.tail = this.tail.next = new Op(fn, len, val);
      this.len += len;
      return this;
    };
    function writeByte(val, buf2, pos) {
      buf2[pos] = val & 255;
    }
    function writeVarint32(val, buf2, pos) {
      while (val > 127) {
        buf2[pos++] = val & 127 | 128;
        val >>>= 7;
      }
      buf2[pos] = val;
    }
    function VarintOp(len, val) {
      this.len = len;
      this.next = void 0;
      this.val = val;
    }
    VarintOp.prototype = Object.create(Op.prototype);
    VarintOp.prototype.fn = writeVarint32;
    Writer2.prototype.uint32 = function write_uint32(value) {
      this.len += (this.tail = this.tail.next = new VarintOp(
        (value = value >>> 0) < 128 ? 1 : value < 16384 ? 2 : value < 2097152 ? 3 : value < 268435456 ? 4 : 5,
        value
      )).len;
      return this;
    };
    Writer2.prototype.int32 = function write_int32(value) {
      return value < 0 ? this._push(writeVarint64, 10, LongBits.fromNumber(value)) : this.uint32(value);
    };
    Writer2.prototype.sint32 = function write_sint32(value) {
      return this.uint32((value << 1 ^ value >> 31) >>> 0);
    };
    function writeVarint64(val, buf2, pos) {
      while (val.hi) {
        buf2[pos++] = val.lo & 127 | 128;
        val.lo = (val.lo >>> 7 | val.hi << 25) >>> 0;
        val.hi >>>= 7;
      }
      while (val.lo > 127) {
        buf2[pos++] = val.lo & 127 | 128;
        val.lo = val.lo >>> 7;
      }
      buf2[pos++] = val.lo;
    }
    Writer2.prototype.uint64 = function write_uint64(value) {
      var bits = LongBits.from(value);
      return this._push(writeVarint64, bits.length(), bits);
    };
    Writer2.prototype.int64 = Writer2.prototype.uint64;
    Writer2.prototype.sint64 = function write_sint64(value) {
      var bits = LongBits.from(value).zzEncode();
      return this._push(writeVarint64, bits.length(), bits);
    };
    Writer2.prototype.bool = function write_bool(value) {
      return this._push(writeByte, 1, value ? 1 : 0);
    };
    function writeFixed32(val, buf2, pos) {
      buf2[pos] = val & 255;
      buf2[pos + 1] = val >>> 8 & 255;
      buf2[pos + 2] = val >>> 16 & 255;
      buf2[pos + 3] = val >>> 24;
    }
    Writer2.prototype.fixed32 = function write_fixed32(value) {
      return this._push(writeFixed32, 4, value >>> 0);
    };
    Writer2.prototype.sfixed32 = Writer2.prototype.fixed32;
    Writer2.prototype.fixed64 = function write_fixed64(value) {
      var bits = LongBits.from(value);
      return this._push(writeFixed32, 4, bits.lo)._push(writeFixed32, 4, bits.hi);
    };
    Writer2.prototype.sfixed64 = Writer2.prototype.fixed64;
    Writer2.prototype.float = function write_float(value) {
      return this._push(util.float.writeFloatLE, 4, value);
    };
    Writer2.prototype.double = function write_double(value) {
      return this._push(util.float.writeDoubleLE, 8, value);
    };
    var writeBytes2 = util.Array.prototype.set ? function writeBytes_set(val, buf2, pos) {
      buf2.set(val, pos);
    } : function writeBytes_for(val, buf2, pos) {
      for (var i = 0; i < val.length; ++i)
        buf2[pos + i] = val[i];
    };
    Writer2.prototype.bytes = function write_bytes(value) {
      var len = value.length >>> 0;
      if (!len)
        return this._push(writeByte, 1, 0);
      if (util.isString(value)) {
        var buf2 = Writer2.alloc(len = base643.length(value));
        base643.decode(value, buf2, 0);
        value = buf2;
      }
      return this.uint32(len)._push(writeBytes2, len, value);
    };
    Writer2.prototype.string = function write_string(value) {
      var len = utf84.length(value);
      return len ? this.uint32(len)._push(utf84.write, len, value) : this._push(writeByte, 1, 0);
    };
    Writer2.prototype.fork = function fork5() {
      this.states = new State2(this);
      this.head = this.tail = new Op(noop2, 0, 0);
      this.len = 0;
      return this;
    };
    Writer2.prototype.reset = function reset() {
      if (this.states) {
        this.head = this.states.head;
        this.tail = this.states.tail;
        this.len = this.states.len;
        this.states = this.states.next;
      } else {
        this.head = this.tail = new Op(noop2, 0, 0);
        this.len = 0;
      }
      return this;
    };
    Writer2.prototype.ldelim = function ldelim() {
      var head = this.head, tail = this.tail, len = this.len;
      this.reset().uint32(len);
      if (len) {
        this.tail.next = head.next;
        this.tail = tail;
        this.len += len;
      }
      return this;
    };
    Writer2.prototype.finish = function finish() {
      var head = this.head.next, buf2 = this.constructor.alloc(this.len), pos = 0;
      while (head) {
        head.fn(head.val, buf2, pos);
        pos += head.len;
        head = head.next;
      }
      return buf2;
    };
    Writer2._configure = function(BufferWriter_) {
      BufferWriter = BufferWriter_;
      Writer2.create = create16();
      BufferWriter._configure();
    };
  }
});

// node_modules/protobufjs/src/writer_buffer.js
var require_writer_buffer = __commonJS({
  "node_modules/protobufjs/src/writer_buffer.js"(exports2, module2) {
    "use strict";
    module2.exports = BufferWriter;
    var Writer2 = require_writer();
    (BufferWriter.prototype = Object.create(Writer2.prototype)).constructor = BufferWriter;
    var util = require_minimal();
    function BufferWriter() {
      Writer2.call(this);
    }
    BufferWriter._configure = function() {
      BufferWriter.alloc = util._Buffer_allocUnsafe;
      BufferWriter.writeBytesBuffer = util.Buffer && util.Buffer.prototype instanceof Uint8Array && util.Buffer.prototype.set.name === "set" ? function writeBytesBuffer_set(val, buf2, pos) {
        buf2.set(val, pos);
      } : function writeBytesBuffer_copy(val, buf2, pos) {
        if (val.copy)
          val.copy(buf2, pos, 0, val.length);
        else for (var i = 0; i < val.length; )
          buf2[pos++] = val[i++];
      };
    };
    BufferWriter.prototype.bytes = function write_bytes_buffer(value) {
      if (util.isString(value))
        value = util._Buffer_from(value, "base64");
      var len = value.length >>> 0;
      this.uint32(len);
      if (len)
        this._push(BufferWriter.writeBytesBuffer, len, value);
      return this;
    };
    function writeStringBuffer(val, buf2, pos) {
      if (val.length < 40)
        util.utf8.write(val, buf2, pos);
      else if (buf2.utf8Write)
        buf2.utf8Write(val, pos);
      else
        buf2.write(val, pos);
    }
    BufferWriter.prototype.string = function write_string_buffer(value) {
      var len = util.Buffer.byteLength(value);
      this.uint32(len);
      if (len)
        this._push(writeStringBuffer, len, value);
      return this;
    };
    BufferWriter._configure();
  }
});

// node_modules/protobufjs/src/reader.js
var require_reader = __commonJS({
  "node_modules/protobufjs/src/reader.js"(exports2, module2) {
    "use strict";
    module2.exports = Reader;
    var util = require_minimal();
    var BufferReader;
    var LongBits = util.LongBits;
    var utf84 = util.utf8;
    function indexOutOfRange(reader, writeLength) {
      return RangeError("index out of range: " + reader.pos + " + " + (writeLength || 1) + " > " + reader.len);
    }
    function Reader(buffer2) {
      this.buf = buffer2;
      this.pos = 0;
      this.len = buffer2.length;
    }
    var create_array = typeof Uint8Array !== "undefined" ? function create_typed_array(buffer2) {
      if (buffer2 instanceof Uint8Array || Array.isArray(buffer2))
        return new Reader(buffer2);
      throw Error("illegal buffer");
    } : function create_array2(buffer2) {
      if (Array.isArray(buffer2))
        return new Reader(buffer2);
      throw Error("illegal buffer");
    };
    var create16 = function create17() {
      return util.Buffer ? function create_buffer_setup(buffer2) {
        return (Reader.create = function create_buffer(buffer3) {
          return util.Buffer.isBuffer(buffer3) ? new BufferReader(buffer3) : create_array(buffer3);
        })(buffer2);
      } : create_array;
    };
    Reader.create = create16();
    Reader.prototype._slice = util.Array.prototype.subarray || /* istanbul ignore next */
    util.Array.prototype.slice;
    Reader.prototype.uint32 = /* @__PURE__ */ function read_uint32_setup() {
      var value = 4294967295;
      return function read_uint32() {
        value = (this.buf[this.pos] & 127) >>> 0;
        if (this.buf[this.pos++] < 128) return value;
        value = (value | (this.buf[this.pos] & 127) << 7) >>> 0;
        if (this.buf[this.pos++] < 128) return value;
        value = (value | (this.buf[this.pos] & 127) << 14) >>> 0;
        if (this.buf[this.pos++] < 128) return value;
        value = (value | (this.buf[this.pos] & 127) << 21) >>> 0;
        if (this.buf[this.pos++] < 128) return value;
        value = (value | (this.buf[this.pos] & 15) << 28) >>> 0;
        if (this.buf[this.pos++] < 128) return value;
        if ((this.pos += 5) > this.len) {
          this.pos = this.len;
          throw indexOutOfRange(this, 10);
        }
        return value;
      };
    }();
    Reader.prototype.int32 = function read_int32() {
      return this.uint32() | 0;
    };
    Reader.prototype.sint32 = function read_sint32() {
      var value = this.uint32();
      return value >>> 1 ^ -(value & 1) | 0;
    };
    function readLongVarint() {
      var bits = new LongBits(0, 0);
      var i = 0;
      if (this.len - this.pos > 4) {
        for (; i < 4; ++i) {
          bits.lo = (bits.lo | (this.buf[this.pos] & 127) << i * 7) >>> 0;
          if (this.buf[this.pos++] < 128)
            return bits;
        }
        bits.lo = (bits.lo | (this.buf[this.pos] & 127) << 28) >>> 0;
        bits.hi = (bits.hi | (this.buf[this.pos] & 127) >> 4) >>> 0;
        if (this.buf[this.pos++] < 128)
          return bits;
        i = 0;
      } else {
        for (; i < 3; ++i) {
          if (this.pos >= this.len)
            throw indexOutOfRange(this);
          bits.lo = (bits.lo | (this.buf[this.pos] & 127) << i * 7) >>> 0;
          if (this.buf[this.pos++] < 128)
            return bits;
        }
        bits.lo = (bits.lo | (this.buf[this.pos++] & 127) << i * 7) >>> 0;
        return bits;
      }
      if (this.len - this.pos > 4) {
        for (; i < 5; ++i) {
          bits.hi = (bits.hi | (this.buf[this.pos] & 127) << i * 7 + 3) >>> 0;
          if (this.buf[this.pos++] < 128)
            return bits;
        }
      } else {
        for (; i < 5; ++i) {
          if (this.pos >= this.len)
            throw indexOutOfRange(this);
          bits.hi = (bits.hi | (this.buf[this.pos] & 127) << i * 7 + 3) >>> 0;
          if (this.buf[this.pos++] < 128)
            return bits;
        }
      }
      throw Error("invalid varint encoding");
    }
    Reader.prototype.bool = function read_bool() {
      return this.uint32() !== 0;
    };
    function readFixed32_end(buf2, end) {
      return (buf2[end - 4] | buf2[end - 3] << 8 | buf2[end - 2] << 16 | buf2[end - 1] << 24) >>> 0;
    }
    Reader.prototype.fixed32 = function read_fixed32() {
      if (this.pos + 4 > this.len)
        throw indexOutOfRange(this, 4);
      return readFixed32_end(this.buf, this.pos += 4);
    };
    Reader.prototype.sfixed32 = function read_sfixed32() {
      if (this.pos + 4 > this.len)
        throw indexOutOfRange(this, 4);
      return readFixed32_end(this.buf, this.pos += 4) | 0;
    };
    function readFixed64() {
      if (this.pos + 8 > this.len)
        throw indexOutOfRange(this, 8);
      return new LongBits(readFixed32_end(this.buf, this.pos += 4), readFixed32_end(this.buf, this.pos += 4));
    }
    Reader.prototype.float = function read_float() {
      if (this.pos + 4 > this.len)
        throw indexOutOfRange(this, 4);
      var value = util.float.readFloatLE(this.buf, this.pos);
      this.pos += 4;
      return value;
    };
    Reader.prototype.double = function read_double() {
      if (this.pos + 8 > this.len)
        throw indexOutOfRange(this, 4);
      var value = util.float.readDoubleLE(this.buf, this.pos);
      this.pos += 8;
      return value;
    };
    Reader.prototype.bytes = function read_bytes() {
      var length2 = this.uint32(), start = this.pos, end = this.pos + length2;
      if (end > this.len)
        throw indexOutOfRange(this, length2);
      this.pos += length2;
      if (Array.isArray(this.buf))
        return this.buf.slice(start, end);
      if (start === end) {
        var nativeBuffer = util.Buffer;
        return nativeBuffer ? nativeBuffer.alloc(0) : new this.buf.constructor(0);
      }
      return this._slice.call(this.buf, start, end);
    };
    Reader.prototype.string = function read_string() {
      var bytes3 = this.bytes();
      return utf84.read(bytes3, 0, bytes3.length);
    };
    Reader.prototype.skip = function skip2(length2) {
      if (typeof length2 === "number") {
        if (this.pos + length2 > this.len)
          throw indexOutOfRange(this, length2);
        this.pos += length2;
      } else {
        do {
          if (this.pos >= this.len)
            throw indexOutOfRange(this);
        } while (this.buf[this.pos++] & 128);
      }
      return this;
    };
    Reader.prototype.skipType = function(wireType) {
      switch (wireType) {
        case 0:
          this.skip();
          break;
        case 1:
          this.skip(8);
          break;
        case 2:
          this.skip(this.uint32());
          break;
        case 3:
          while ((wireType = this.uint32() & 7) !== 4) {
            this.skipType(wireType);
          }
          break;
        case 5:
          this.skip(4);
          break;
        default:
          throw Error("invalid wire type " + wireType + " at offset " + this.pos);
      }
      return this;
    };
    Reader._configure = function(BufferReader_) {
      BufferReader = BufferReader_;
      Reader.create = create16();
      BufferReader._configure();
      var fn = util.Long ? "toLong" : (
        /* istanbul ignore next */
        "toNumber"
      );
      util.merge(Reader.prototype, {
        int64: function read_int64() {
          return readLongVarint.call(this)[fn](false);
        },
        uint64: function read_uint64() {
          return readLongVarint.call(this)[fn](true);
        },
        sint64: function read_sint64() {
          return readLongVarint.call(this).zzDecode()[fn](false);
        },
        fixed64: function read_fixed64() {
          return readFixed64.call(this)[fn](true);
        },
        sfixed64: function read_sfixed64() {
          return readFixed64.call(this)[fn](false);
        }
      });
    };
  }
});

// node_modules/protobufjs/src/reader_buffer.js
var require_reader_buffer = __commonJS({
  "node_modules/protobufjs/src/reader_buffer.js"(exports2, module2) {
    "use strict";
    module2.exports = BufferReader;
    var Reader = require_reader();
    (BufferReader.prototype = Object.create(Reader.prototype)).constructor = BufferReader;
    var util = require_minimal();
    function BufferReader(buffer2) {
      Reader.call(this, buffer2);
    }
    BufferReader._configure = function() {
      if (util.Buffer)
        BufferReader.prototype._slice = util.Buffer.prototype.slice;
    };
    BufferReader.prototype.string = function read_string_buffer() {
      var len = this.uint32();
      return this.buf.utf8Slice ? this.buf.utf8Slice(this.pos, this.pos = Math.min(this.pos + len, this.len)) : this.buf.toString("utf-8", this.pos, this.pos = Math.min(this.pos + len, this.len));
    };
    BufferReader._configure();
  }
});

// node_modules/protobufjs/src/rpc/service.js
var require_service = __commonJS({
  "node_modules/protobufjs/src/rpc/service.js"(exports2, module2) {
    "use strict";
    module2.exports = Service;
    var util = require_minimal();
    (Service.prototype = Object.create(util.EventEmitter.prototype)).constructor = Service;
    function Service(rpcImpl, requestDelimited, responseDelimited) {
      if (typeof rpcImpl !== "function")
        throw TypeError("rpcImpl must be a function");
      util.EventEmitter.call(this);
      this.rpcImpl = rpcImpl;
      this.requestDelimited = Boolean(requestDelimited);
      this.responseDelimited = Boolean(responseDelimited);
    }
    Service.prototype.rpcCall = function rpcCall(method, requestCtor, responseCtor, request3, callback) {
      if (!request3)
        throw TypeError("request must be specified");
      var self2 = this;
      if (!callback)
        return util.asPromise(rpcCall, self2, method, requestCtor, responseCtor, request3);
      if (!self2.rpcImpl) {
        setTimeout(function() {
          callback(Error("already ended"));
        }, 0);
        return void 0;
      }
      try {
        return self2.rpcImpl(
          method,
          requestCtor[self2.requestDelimited ? "encodeDelimited" : "encode"](request3).finish(),
          function rpcCallback(err, response) {
            if (err) {
              self2.emit("error", err, method);
              return callback(err);
            }
            if (response === null) {
              self2.end(
                /* endedByRPC */
                true
              );
              return void 0;
            }
            if (!(response instanceof responseCtor)) {
              try {
                response = responseCtor[self2.responseDelimited ? "decodeDelimited" : "decode"](response);
              } catch (err2) {
                self2.emit("error", err2, method);
                return callback(err2);
              }
            }
            self2.emit("data", response, method);
            return callback(null, response);
          }
        );
      } catch (err) {
        self2.emit("error", err, method);
        setTimeout(function() {
          callback(err);
        }, 0);
        return void 0;
      }
    };
    Service.prototype.end = function end(endedByRPC) {
      if (this.rpcImpl) {
        if (!endedByRPC)
          this.rpcImpl(null, null, null);
        this.rpcImpl = null;
        this.emit("end").off();
      }
      return this;
    };
  }
});

// node_modules/protobufjs/src/rpc.js
var require_rpc = __commonJS({
  "node_modules/protobufjs/src/rpc.js"(exports2) {
    "use strict";
    var rpc = exports2;
    rpc.Service = require_service();
  }
});

// node_modules/protobufjs/src/roots.js
var require_roots = __commonJS({
  "node_modules/protobufjs/src/roots.js"(exports2, module2) {
    "use strict";
    module2.exports = {};
  }
});

// node_modules/protobufjs/src/index-minimal.js
var require_index_minimal = __commonJS({
  "node_modules/protobufjs/src/index-minimal.js"(exports2) {
    "use strict";
    var protobuf = exports2;
    protobuf.build = "minimal";
    protobuf.Writer = require_writer();
    protobuf.BufferWriter = require_writer_buffer();
    protobuf.Reader = require_reader();
    protobuf.BufferReader = require_reader_buffer();
    protobuf.util = require_minimal();
    protobuf.rpc = require_rpc();
    protobuf.roots = require_roots();
    protobuf.configure = configure6;
    function configure6() {
      protobuf.util._configure();
      protobuf.Writer._configure(protobuf.BufferWriter);
      protobuf.Reader._configure(protobuf.BufferReader);
    }
    configure6();
  }
});

// node_modules/protobufjs/minimal.js
var require_minimal2 = __commonJS({
  "node_modules/protobufjs/minimal.js"(exports2, module2) {
    "use strict";
    module2.exports = require_index_minimal();
  }
});

// node_modules/murmurhash3js-revisited/lib/murmurHash3js.js
var require_murmurHash3js = __commonJS({
  "node_modules/murmurhash3js-revisited/lib/murmurHash3js.js"(exports2, module2) {
    (function(root2, undefined2) {
      "use strict";
      var library = {
        "version": "3.0.0",
        "x86": {},
        "x64": {},
        "inputValidation": true
      };
      function _validBytes(bytes3) {
        if (!Array.isArray(bytes3) && !ArrayBuffer.isView(bytes3)) {
          return false;
        }
        for (var i = 0; i < bytes3.length; i++) {
          if (!Number.isInteger(bytes3[i]) || bytes3[i] < 0 || bytes3[i] > 255) {
            return false;
          }
        }
        return true;
      }
      function _x86Multiply(m, n) {
        return (m & 65535) * n + (((m >>> 16) * n & 65535) << 16);
      }
      function _x86Rotl(m, n) {
        return m << n | m >>> 32 - n;
      }
      function _x86Fmix(h) {
        h ^= h >>> 16;
        h = _x86Multiply(h, 2246822507);
        h ^= h >>> 13;
        h = _x86Multiply(h, 3266489909);
        h ^= h >>> 16;
        return h;
      }
      function _x64Add(m, n) {
        m = [m[0] >>> 16, m[0] & 65535, m[1] >>> 16, m[1] & 65535];
        n = [n[0] >>> 16, n[0] & 65535, n[1] >>> 16, n[1] & 65535];
        var o = [0, 0, 0, 0];
        o[3] += m[3] + n[3];
        o[2] += o[3] >>> 16;
        o[3] &= 65535;
        o[2] += m[2] + n[2];
        o[1] += o[2] >>> 16;
        o[2] &= 65535;
        o[1] += m[1] + n[1];
        o[0] += o[1] >>> 16;
        o[1] &= 65535;
        o[0] += m[0] + n[0];
        o[0] &= 65535;
        return [o[0] << 16 | o[1], o[2] << 16 | o[3]];
      }
      function _x64Multiply(m, n) {
        m = [m[0] >>> 16, m[0] & 65535, m[1] >>> 16, m[1] & 65535];
        n = [n[0] >>> 16, n[0] & 65535, n[1] >>> 16, n[1] & 65535];
        var o = [0, 0, 0, 0];
        o[3] += m[3] * n[3];
        o[2] += o[3] >>> 16;
        o[3] &= 65535;
        o[2] += m[2] * n[3];
        o[1] += o[2] >>> 16;
        o[2] &= 65535;
        o[2] += m[3] * n[2];
        o[1] += o[2] >>> 16;
        o[2] &= 65535;
        o[1] += m[1] * n[3];
        o[0] += o[1] >>> 16;
        o[1] &= 65535;
        o[1] += m[2] * n[2];
        o[0] += o[1] >>> 16;
        o[1] &= 65535;
        o[1] += m[3] * n[1];
        o[0] += o[1] >>> 16;
        o[1] &= 65535;
        o[0] += m[0] * n[3] + m[1] * n[2] + m[2] * n[1] + m[3] * n[0];
        o[0] &= 65535;
        return [o[0] << 16 | o[1], o[2] << 16 | o[3]];
      }
      function _x64Rotl(m, n) {
        n %= 64;
        if (n === 32) {
          return [m[1], m[0]];
        } else if (n < 32) {
          return [m[0] << n | m[1] >>> 32 - n, m[1] << n | m[0] >>> 32 - n];
        } else {
          n -= 32;
          return [m[1] << n | m[0] >>> 32 - n, m[0] << n | m[1] >>> 32 - n];
        }
      }
      function _x64LeftShift(m, n) {
        n %= 64;
        if (n === 0) {
          return m;
        } else if (n < 32) {
          return [m[0] << n | m[1] >>> 32 - n, m[1] << n];
        } else {
          return [m[1] << n - 32, 0];
        }
      }
      function _x64Xor(m, n) {
        return [m[0] ^ n[0], m[1] ^ n[1]];
      }
      function _x64Fmix(h) {
        h = _x64Xor(h, [0, h[0] >>> 1]);
        h = _x64Multiply(h, [4283543511, 3981806797]);
        h = _x64Xor(h, [0, h[0] >>> 1]);
        h = _x64Multiply(h, [3301882366, 444984403]);
        h = _x64Xor(h, [0, h[0] >>> 1]);
        return h;
      }
      library.x86.hash32 = function(bytes3, seed) {
        if (library.inputValidation && !_validBytes(bytes3)) {
          return undefined2;
        }
        seed = seed || 0;
        var remainder = bytes3.length % 4;
        var blocks = bytes3.length - remainder;
        var h1 = seed;
        var k1 = 0;
        var c1 = 3432918353;
        var c2 = 461845907;
        for (var i = 0; i < blocks; i = i + 4) {
          k1 = bytes3[i] | bytes3[i + 1] << 8 | bytes3[i + 2] << 16 | bytes3[i + 3] << 24;
          k1 = _x86Multiply(k1, c1);
          k1 = _x86Rotl(k1, 15);
          k1 = _x86Multiply(k1, c2);
          h1 ^= k1;
          h1 = _x86Rotl(h1, 13);
          h1 = _x86Multiply(h1, 5) + 3864292196;
        }
        k1 = 0;
        switch (remainder) {
          case 3:
            k1 ^= bytes3[i + 2] << 16;
          case 2:
            k1 ^= bytes3[i + 1] << 8;
          case 1:
            k1 ^= bytes3[i];
            k1 = _x86Multiply(k1, c1);
            k1 = _x86Rotl(k1, 15);
            k1 = _x86Multiply(k1, c2);
            h1 ^= k1;
        }
        h1 ^= bytes3.length;
        h1 = _x86Fmix(h1);
        return h1 >>> 0;
      };
      library.x86.hash128 = function(bytes3, seed) {
        if (library.inputValidation && !_validBytes(bytes3)) {
          return undefined2;
        }
        seed = seed || 0;
        var remainder = bytes3.length % 16;
        var blocks = bytes3.length - remainder;
        var h1 = seed;
        var h2 = seed;
        var h3 = seed;
        var h4 = seed;
        var k1 = 0;
        var k2 = 0;
        var k3 = 0;
        var k4 = 0;
        var c1 = 597399067;
        var c2 = 2869860233;
        var c3 = 951274213;
        var c4 = 2716044179;
        for (var i = 0; i < blocks; i = i + 16) {
          k1 = bytes3[i] | bytes3[i + 1] << 8 | bytes3[i + 2] << 16 | bytes3[i + 3] << 24;
          k2 = bytes3[i + 4] | bytes3[i + 5] << 8 | bytes3[i + 6] << 16 | bytes3[i + 7] << 24;
          k3 = bytes3[i + 8] | bytes3[i + 9] << 8 | bytes3[i + 10] << 16 | bytes3[i + 11] << 24;
          k4 = bytes3[i + 12] | bytes3[i + 13] << 8 | bytes3[i + 14] << 16 | bytes3[i + 15] << 24;
          k1 = _x86Multiply(k1, c1);
          k1 = _x86Rotl(k1, 15);
          k1 = _x86Multiply(k1, c2);
          h1 ^= k1;
          h1 = _x86Rotl(h1, 19);
          h1 += h2;
          h1 = _x86Multiply(h1, 5) + 1444728091;
          k2 = _x86Multiply(k2, c2);
          k2 = _x86Rotl(k2, 16);
          k2 = _x86Multiply(k2, c3);
          h2 ^= k2;
          h2 = _x86Rotl(h2, 17);
          h2 += h3;
          h2 = _x86Multiply(h2, 5) + 197830471;
          k3 = _x86Multiply(k3, c3);
          k3 = _x86Rotl(k3, 17);
          k3 = _x86Multiply(k3, c4);
          h3 ^= k3;
          h3 = _x86Rotl(h3, 15);
          h3 += h4;
          h3 = _x86Multiply(h3, 5) + 2530024501;
          k4 = _x86Multiply(k4, c4);
          k4 = _x86Rotl(k4, 18);
          k4 = _x86Multiply(k4, c1);
          h4 ^= k4;
          h4 = _x86Rotl(h4, 13);
          h4 += h1;
          h4 = _x86Multiply(h4, 5) + 850148119;
        }
        k1 = 0;
        k2 = 0;
        k3 = 0;
        k4 = 0;
        switch (remainder) {
          case 15:
            k4 ^= bytes3[i + 14] << 16;
          case 14:
            k4 ^= bytes3[i + 13] << 8;
          case 13:
            k4 ^= bytes3[i + 12];
            k4 = _x86Multiply(k4, c4);
            k4 = _x86Rotl(k4, 18);
            k4 = _x86Multiply(k4, c1);
            h4 ^= k4;
          case 12:
            k3 ^= bytes3[i + 11] << 24;
          case 11:
            k3 ^= bytes3[i + 10] << 16;
          case 10:
            k3 ^= bytes3[i + 9] << 8;
          case 9:
            k3 ^= bytes3[i + 8];
            k3 = _x86Multiply(k3, c3);
            k3 = _x86Rotl(k3, 17);
            k3 = _x86Multiply(k3, c4);
            h3 ^= k3;
          case 8:
            k2 ^= bytes3[i + 7] << 24;
          case 7:
            k2 ^= bytes3[i + 6] << 16;
          case 6:
            k2 ^= bytes3[i + 5] << 8;
          case 5:
            k2 ^= bytes3[i + 4];
            k2 = _x86Multiply(k2, c2);
            k2 = _x86Rotl(k2, 16);
            k2 = _x86Multiply(k2, c3);
            h2 ^= k2;
          case 4:
            k1 ^= bytes3[i + 3] << 24;
          case 3:
            k1 ^= bytes3[i + 2] << 16;
          case 2:
            k1 ^= bytes3[i + 1] << 8;
          case 1:
            k1 ^= bytes3[i];
            k1 = _x86Multiply(k1, c1);
            k1 = _x86Rotl(k1, 15);
            k1 = _x86Multiply(k1, c2);
            h1 ^= k1;
        }
        h1 ^= bytes3.length;
        h2 ^= bytes3.length;
        h3 ^= bytes3.length;
        h4 ^= bytes3.length;
        h1 += h2;
        h1 += h3;
        h1 += h4;
        h2 += h1;
        h3 += h1;
        h4 += h1;
        h1 = _x86Fmix(h1);
        h2 = _x86Fmix(h2);
        h3 = _x86Fmix(h3);
        h4 = _x86Fmix(h4);
        h1 += h2;
        h1 += h3;
        h1 += h4;
        h2 += h1;
        h3 += h1;
        h4 += h1;
        return ("00000000" + (h1 >>> 0).toString(16)).slice(-8) + ("00000000" + (h2 >>> 0).toString(16)).slice(-8) + ("00000000" + (h3 >>> 0).toString(16)).slice(-8) + ("00000000" + (h4 >>> 0).toString(16)).slice(-8);
      };
      library.x64.hash128 = function(bytes3, seed) {
        if (library.inputValidation && !_validBytes(bytes3)) {
          return undefined2;
        }
        seed = seed || 0;
        var remainder = bytes3.length % 16;
        var blocks = bytes3.length - remainder;
        var h1 = [0, seed];
        var h2 = [0, seed];
        var k1 = [0, 0];
        var k2 = [0, 0];
        var c1 = [2277735313, 289559509];
        var c2 = [1291169091, 658871167];
        for (var i = 0; i < blocks; i = i + 16) {
          k1 = [bytes3[i + 4] | bytes3[i + 5] << 8 | bytes3[i + 6] << 16 | bytes3[i + 7] << 24, bytes3[i] | bytes3[i + 1] << 8 | bytes3[i + 2] << 16 | bytes3[i + 3] << 24];
          k2 = [bytes3[i + 12] | bytes3[i + 13] << 8 | bytes3[i + 14] << 16 | bytes3[i + 15] << 24, bytes3[i + 8] | bytes3[i + 9] << 8 | bytes3[i + 10] << 16 | bytes3[i + 11] << 24];
          k1 = _x64Multiply(k1, c1);
          k1 = _x64Rotl(k1, 31);
          k1 = _x64Multiply(k1, c2);
          h1 = _x64Xor(h1, k1);
          h1 = _x64Rotl(h1, 27);
          h1 = _x64Add(h1, h2);
          h1 = _x64Add(_x64Multiply(h1, [0, 5]), [0, 1390208809]);
          k2 = _x64Multiply(k2, c2);
          k2 = _x64Rotl(k2, 33);
          k2 = _x64Multiply(k2, c1);
          h2 = _x64Xor(h2, k2);
          h2 = _x64Rotl(h2, 31);
          h2 = _x64Add(h2, h1);
          h2 = _x64Add(_x64Multiply(h2, [0, 5]), [0, 944331445]);
        }
        k1 = [0, 0];
        k2 = [0, 0];
        switch (remainder) {
          case 15:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes3[i + 14]], 48));
          case 14:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes3[i + 13]], 40));
          case 13:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes3[i + 12]], 32));
          case 12:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes3[i + 11]], 24));
          case 11:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes3[i + 10]], 16));
          case 10:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes3[i + 9]], 8));
          case 9:
            k2 = _x64Xor(k2, [0, bytes3[i + 8]]);
            k2 = _x64Multiply(k2, c2);
            k2 = _x64Rotl(k2, 33);
            k2 = _x64Multiply(k2, c1);
            h2 = _x64Xor(h2, k2);
          case 8:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes3[i + 7]], 56));
          case 7:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes3[i + 6]], 48));
          case 6:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes3[i + 5]], 40));
          case 5:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes3[i + 4]], 32));
          case 4:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes3[i + 3]], 24));
          case 3:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes3[i + 2]], 16));
          case 2:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes3[i + 1]], 8));
          case 1:
            k1 = _x64Xor(k1, [0, bytes3[i]]);
            k1 = _x64Multiply(k1, c1);
            k1 = _x64Rotl(k1, 31);
            k1 = _x64Multiply(k1, c2);
            h1 = _x64Xor(h1, k1);
        }
        h1 = _x64Xor(h1, [0, bytes3.length]);
        h2 = _x64Xor(h2, [0, bytes3.length]);
        h1 = _x64Add(h1, h2);
        h2 = _x64Add(h2, h1);
        h1 = _x64Fmix(h1);
        h2 = _x64Fmix(h2);
        h1 = _x64Add(h1, h2);
        h2 = _x64Add(h2, h1);
        return ("00000000" + (h1[0] >>> 0).toString(16)).slice(-8) + ("00000000" + (h1[1] >>> 0).toString(16)).slice(-8) + ("00000000" + (h2[0] >>> 0).toString(16)).slice(-8) + ("00000000" + (h2[1] >>> 0).toString(16)).slice(-8);
      };
      if (typeof exports2 !== "undefined") {
        if (typeof module2 !== "undefined" && module2.exports) {
          exports2 = module2.exports = library;
        }
        exports2.murmurHash3 = library;
      } else if (typeof define === "function" && define.amd) {
        define([], function() {
          return library;
        });
      } else {
        library._murmurHash3 = root2.murmurHash3;
        library.noConflict = function() {
          root2.murmurHash3 = library._murmurHash3;
          library._murmurHash3 = undefined2;
          library.noConflict = undefined2;
          return library;
        };
        root2.murmurHash3 = library;
      }
    })(exports2);
  }
});

// node_modules/murmurhash3js-revisited/index.js
var require_murmurhash3js_revisited = __commonJS({
  "node_modules/murmurhash3js-revisited/index.js"(exports2, module2) {
    module2.exports = require_murmurHash3js();
  }
});

// node_modules/@storacha/access/dist/agent.js
var agent_exports = {};
__export(agent_exports, {
  Access: () => access_exports2,
  Agent: () => Agent,
  AgentData: () => AgentData,
  AppName: () => AppName,
  Delegation: () => delegation_exports,
  Schema: () => schema_exports3,
  Space: () => space_exports2,
  addProvider: () => addProvider,
  addProviderAndDelegateToAccount: () => addProviderAndDelegateToAccount,
  addSpacesFromDelegations: () => addSpacesFromDelegations,
  authorizeAndWait: () => authorizeAndWait,
  authorizeWaitAndClaim: () => authorizeWaitAndClaim,
  canDelegateCapability: () => canDelegateCapability,
  claimAccess: () => claimAccess,
  connection: () => connection,
  delegationsIncludeSessionProof: () => delegationsIncludeSessionProof,
  email: () => email,
  fromEmail: () => fromEmail,
  fromString: () => fromString3,
  getAccountPlan: () => getAccountPlan,
  importAuthorization: () => importAuthorization,
  isExpired: () => isExpired2,
  isTooEarly: () => isTooEarly2,
  matchResource: () => matchResource,
  pollAccessClaimUntil: () => pollAccessClaimUntil,
  requestAccess: () => requestAccess,
  toEmail: () => toEmail,
  validate: () => validate,
  waitForAuthorizationByPolling: () => waitForAuthorizationByPolling
});

// node_modules/@ucanto/client/src/lib.js
var lib_exports2 = {};
__export(lib_exports2, {
  DAG: () => dag_exports,
  Schema: () => schema_exports3,
  connect: () => connect,
  delegate: () => delegate2,
  error: () => error,
  execute: () => execute,
  invoke: () => invoke,
  ok: () => ok
});

// node_modules/@ucanto/client/src/connection.js
var API15 = __toESM(require_lib(), 1);

// node_modules/@ucanto/core/src/lib.js
var API14 = __toESM(require_lib());

// node_modules/@ucanto/core/src/delegation.js
var delegation_exports = {};
__export(delegation_exports, {
  ArchiveSchema: () => ArchiveSchema,
  Delegation: () => Delegation,
  View: () => Delegation,
  allows: () => allows,
  archive: () => archive,
  create: () => create4,
  delegate: () => delegate,
  exportDAG: () => exportDAG,
  extract: () => extract,
  importDAG: () => importDAG,
  isDelegation: () => isDelegation,
  isLink: () => isLink2,
  view: () => view2
});

// node_modules/@ipld/dag-ucan/src/lib.js
var lib_exports = {};
__export(lib_exports, {
  VERSION: () => VERSION,
  code: () => code5,
  decode: () => decode15,
  encode: () => encode14,
  format: () => format6,
  isExpired: () => isExpired,
  isTooEarly: () => isTooEarly,
  issue: () => issue,
  link: () => link,
  name: () => name4,
  now: () => now,
  parse: () => parse5,
  verifySignature: () => verifySignature,
  write: () => write
});

// node_modules/cborg/lib/is.js
var typeofs = [
  "string",
  "number",
  "bigint",
  "symbol"
];
var objectTypeNames = [
  "Function",
  "Generator",
  "AsyncGenerator",
  "GeneratorFunction",
  "AsyncGeneratorFunction",
  "AsyncFunction",
  "Observable",
  "Array",
  "Buffer",
  "Object",
  "RegExp",
  "Date",
  "Error",
  "Map",
  "Set",
  "WeakMap",
  "WeakSet",
  "ArrayBuffer",
  "SharedArrayBuffer",
  "DataView",
  "Promise",
  "URL",
  "HTMLElement",
  "Int8Array",
  "Uint8Array",
  "Uint8ClampedArray",
  "Int16Array",
  "Uint16Array",
  "Int32Array",
  "Uint32Array",
  "Float32Array",
  "Float64Array",
  "BigInt64Array",
  "BigUint64Array"
];
function is(value) {
  if (value === null) {
    return "null";
  }
  if (value === void 0) {
    return "undefined";
  }
  if (value === true || value === false) {
    return "boolean";
  }
  const typeOf = typeof value;
  if (typeofs.includes(typeOf)) {
    return typeOf;
  }
  if (typeOf === "function") {
    return "Function";
  }
  if (Array.isArray(value)) {
    return "Array";
  }
  if (isBuffer(value)) {
    return "Buffer";
  }
  const objectType = getObjectType(value);
  if (objectType) {
    return objectType;
  }
  return "Object";
}
function isBuffer(value) {
  return value && value.constructor && value.constructor.isBuffer && value.constructor.isBuffer.call(null, value);
}
function getObjectType(value) {
  const objectTypeName = Object.prototype.toString.call(value).slice(8, -1);
  if (objectTypeNames.includes(objectTypeName)) {
    return objectTypeName;
  }
  return void 0;
}

// node_modules/cborg/lib/token.js
var Type = class {
  /**
   * @param {number} major
   * @param {string} name
   * @param {boolean} terminal
   */
  constructor(major, name15, terminal) {
    this.major = major;
    this.majorEncoded = major << 5;
    this.name = name15;
    this.terminal = terminal;
  }
  /* c8 ignore next 3 */
  toString() {
    return `Type[${this.major}].${this.name}`;
  }
  /**
   * @param {Type} typ
   * @returns {number}
   */
  compare(typ) {
    return this.major < typ.major ? -1 : this.major > typ.major ? 1 : 0;
  }
};
Type.uint = new Type(0, "uint", true);
Type.negint = new Type(1, "negint", true);
Type.bytes = new Type(2, "bytes", true);
Type.string = new Type(3, "string", true);
Type.array = new Type(4, "array", false);
Type.map = new Type(5, "map", false);
Type.tag = new Type(6, "tag", false);
Type.float = new Type(7, "float", true);
Type.false = new Type(7, "false", true);
Type.true = new Type(7, "true", true);
Type.null = new Type(7, "null", true);
Type.undefined = new Type(7, "undefined", true);
Type.break = new Type(7, "break", true);
var Token = class {
  /**
   * @param {Type} type
   * @param {any} [value]
   * @param {number} [encodedLength]
   */
  constructor(type2, value, encodedLength) {
    this.type = type2;
    this.value = value;
    this.encodedLength = encodedLength;
    this.encodedBytes = void 0;
    this.byteValue = void 0;
  }
  /* c8 ignore next 3 */
  toString() {
    return `Token[${this.type}].${this.value}`;
  }
};

// node_modules/cborg/lib/byte-utils.js
var useBuffer = globalThis.process && // @ts-ignore
!globalThis.process.browser && // @ts-ignore
globalThis.Buffer && // @ts-ignore
typeof globalThis.Buffer.isBuffer === "function";
var textDecoder = new TextDecoder();
var textEncoder = new TextEncoder();
function isBuffer2(buf2) {
  return useBuffer && globalThis.Buffer.isBuffer(buf2);
}
function asU8A(buf2) {
  if (!(buf2 instanceof Uint8Array)) {
    return Uint8Array.from(buf2);
  }
  return isBuffer2(buf2) ? new Uint8Array(buf2.buffer, buf2.byteOffset, buf2.byteLength) : buf2;
}
var toString = useBuffer ? (
  // eslint-disable-line operator-linebreak
  /**
   * @param {Uint8Array} bytes
   * @param {number} start
   * @param {number} end
   */
  (bytes3, start, end) => {
    return end - start > 64 ? (
      // eslint-disable-line operator-linebreak
      // @ts-ignore
      globalThis.Buffer.from(bytes3.subarray(start, end)).toString("utf8")
    ) : utf8Slice(bytes3, start, end);
  }
) : (
  // eslint-disable-line operator-linebreak
  /**
   * @param {Uint8Array} bytes
   * @param {number} start
   * @param {number} end
   */
  (bytes3, start, end) => {
    return end - start > 64 ? textDecoder.decode(bytes3.subarray(start, end)) : utf8Slice(bytes3, start, end);
  }
);
var fromString = useBuffer ? (
  // eslint-disable-line operator-linebreak
  /**
   * @param {string} string
   */
  (string3) => {
    return string3.length > 64 ? (
      // eslint-disable-line operator-linebreak
      // @ts-ignore
      globalThis.Buffer.from(string3)
    ) : utf8ToBytes(string3);
  }
) : (
  // eslint-disable-line operator-linebreak
  /**
   * @param {string} string
   */
  (string3) => {
    return string3.length > 64 ? textEncoder.encode(string3) : utf8ToBytes(string3);
  }
);
var fromArray = (arr) => {
  return Uint8Array.from(arr);
};
var slice = useBuffer ? (
  // eslint-disable-line operator-linebreak
  /**
   * @param {Uint8Array} bytes
   * @param {number} start
   * @param {number} end
   */
  (bytes3, start, end) => {
    if (isBuffer2(bytes3)) {
      return new Uint8Array(bytes3.subarray(start, end));
    }
    return bytes3.slice(start, end);
  }
) : (
  // eslint-disable-line operator-linebreak
  /**
   * @param {Uint8Array} bytes
   * @param {number} start
   * @param {number} end
   */
  (bytes3, start, end) => {
    return bytes3.slice(start, end);
  }
);
var concat = useBuffer ? (
  // eslint-disable-line operator-linebreak
  /**
   * @param {Uint8Array[]} chunks
   * @param {number} length
   * @returns {Uint8Array}
   */
  (chunks, length2) => {
    chunks = chunks.map((c) => c instanceof Uint8Array ? c : (
      // eslint-disable-line operator-linebreak
      // @ts-ignore
      globalThis.Buffer.from(c)
    ));
    return asU8A(globalThis.Buffer.concat(chunks, length2));
  }
) : (
  // eslint-disable-line operator-linebreak
  /**
   * @param {Uint8Array[]} chunks
   * @param {number} length
   * @returns {Uint8Array}
   */
  (chunks, length2) => {
    const out = new Uint8Array(length2);
    let off = 0;
    for (let b of chunks) {
      if (off + b.length > out.length) {
        b = b.subarray(0, out.length - off);
      }
      out.set(b, off);
      off += b.length;
    }
    return out;
  }
);
var alloc = useBuffer ? (
  // eslint-disable-line operator-linebreak
  /**
   * @param {number} size
   * @returns {Uint8Array}
   */
  (size5) => {
    return globalThis.Buffer.allocUnsafe(size5);
  }
) : (
  // eslint-disable-line operator-linebreak
  /**
   * @param {number} size
   * @returns {Uint8Array}
   */
  (size5) => {
    return new Uint8Array(size5);
  }
);
function compare(b1, b2) {
  if (isBuffer2(b1) && isBuffer2(b2)) {
    return b1.compare(b2);
  }
  for (let i = 0; i < b1.length; i++) {
    if (b1[i] === b2[i]) {
      continue;
    }
    return b1[i] < b2[i] ? -1 : 1;
  }
  return 0;
}
function utf8ToBytes(str) {
  const out = [];
  let p = 0;
  for (let i = 0; i < str.length; i++) {
    let c = str.charCodeAt(i);
    if (c < 128) {
      out[p++] = c;
    } else if (c < 2048) {
      out[p++] = c >> 6 | 192;
      out[p++] = c & 63 | 128;
    } else if ((c & 64512) === 55296 && i + 1 < str.length && (str.charCodeAt(i + 1) & 64512) === 56320) {
      c = 65536 + ((c & 1023) << 10) + (str.charCodeAt(++i) & 1023);
      out[p++] = c >> 18 | 240;
      out[p++] = c >> 12 & 63 | 128;
      out[p++] = c >> 6 & 63 | 128;
      out[p++] = c & 63 | 128;
    } else {
      out[p++] = c >> 12 | 224;
      out[p++] = c >> 6 & 63 | 128;
      out[p++] = c & 63 | 128;
    }
  }
  return out;
}
function utf8Slice(buf2, offset2, end) {
  const res = [];
  while (offset2 < end) {
    const firstByte = buf2[offset2];
    let codePoint = null;
    let bytesPerSequence = firstByte > 239 ? 4 : firstByte > 223 ? 3 : firstByte > 191 ? 2 : 1;
    if (offset2 + bytesPerSequence <= end) {
      let secondByte, thirdByte, fourthByte, tempCodePoint;
      switch (bytesPerSequence) {
        case 1:
          if (firstByte < 128) {
            codePoint = firstByte;
          }
          break;
        case 2:
          secondByte = buf2[offset2 + 1];
          if ((secondByte & 192) === 128) {
            tempCodePoint = (firstByte & 31) << 6 | secondByte & 63;
            if (tempCodePoint > 127) {
              codePoint = tempCodePoint;
            }
          }
          break;
        case 3:
          secondByte = buf2[offset2 + 1];
          thirdByte = buf2[offset2 + 2];
          if ((secondByte & 192) === 128 && (thirdByte & 192) === 128) {
            tempCodePoint = (firstByte & 15) << 12 | (secondByte & 63) << 6 | thirdByte & 63;
            if (tempCodePoint > 2047 && (tempCodePoint < 55296 || tempCodePoint > 57343)) {
              codePoint = tempCodePoint;
            }
          }
          break;
        case 4:
          secondByte = buf2[offset2 + 1];
          thirdByte = buf2[offset2 + 2];
          fourthByte = buf2[offset2 + 3];
          if ((secondByte & 192) === 128 && (thirdByte & 192) === 128 && (fourthByte & 192) === 128) {
            tempCodePoint = (firstByte & 15) << 18 | (secondByte & 63) << 12 | (thirdByte & 63) << 6 | fourthByte & 63;
            if (tempCodePoint > 65535 && tempCodePoint < 1114112) {
              codePoint = tempCodePoint;
            }
          }
      }
    }
    if (codePoint === null) {
      codePoint = 65533;
      bytesPerSequence = 1;
    } else if (codePoint > 65535) {
      codePoint -= 65536;
      res.push(codePoint >>> 10 & 1023 | 55296);
      codePoint = 56320 | codePoint & 1023;
    }
    res.push(codePoint);
    offset2 += bytesPerSequence;
  }
  return decodeCodePointsArray(res);
}
var MAX_ARGUMENTS_LENGTH = 4096;
function decodeCodePointsArray(codePoints) {
  const len = codePoints.length;
  if (len <= MAX_ARGUMENTS_LENGTH) {
    return String.fromCharCode.apply(String, codePoints);
  }
  let res = "";
  let i = 0;
  while (i < len) {
    res += String.fromCharCode.apply(
      String,
      codePoints.slice(i, i += MAX_ARGUMENTS_LENGTH)
    );
  }
  return res;
}

// node_modules/cborg/lib/bl.js
var defaultChunkSize = 256;
var Bl = class {
  /**
   * @param {number} [chunkSize]
   */
  constructor(chunkSize = defaultChunkSize) {
    this.chunkSize = chunkSize;
    this.cursor = 0;
    this.maxCursor = -1;
    this.chunks = [];
    this._initReuseChunk = null;
  }
  reset() {
    this.cursor = 0;
    this.maxCursor = -1;
    if (this.chunks.length) {
      this.chunks = [];
    }
    if (this._initReuseChunk !== null) {
      this.chunks.push(this._initReuseChunk);
      this.maxCursor = this._initReuseChunk.length - 1;
    }
  }
  /**
   * @param {Uint8Array|number[]} bytes
   */
  push(bytes3) {
    let topChunk = this.chunks[this.chunks.length - 1];
    const newMax = this.cursor + bytes3.length;
    if (newMax <= this.maxCursor + 1) {
      const chunkPos = topChunk.length - (this.maxCursor - this.cursor) - 1;
      topChunk.set(bytes3, chunkPos);
    } else {
      if (topChunk) {
        const chunkPos = topChunk.length - (this.maxCursor - this.cursor) - 1;
        if (chunkPos < topChunk.length) {
          this.chunks[this.chunks.length - 1] = topChunk.subarray(0, chunkPos);
          this.maxCursor = this.cursor - 1;
        }
      }
      if (bytes3.length < 64 && bytes3.length < this.chunkSize) {
        topChunk = alloc(this.chunkSize);
        this.chunks.push(topChunk);
        this.maxCursor += topChunk.length;
        if (this._initReuseChunk === null) {
          this._initReuseChunk = topChunk;
        }
        topChunk.set(bytes3, 0);
      } else {
        this.chunks.push(bytes3);
        this.maxCursor += bytes3.length;
      }
    }
    this.cursor += bytes3.length;
  }
  /**
   * @param {boolean} [reset]
   * @returns {Uint8Array}
   */
  toBytes(reset = false) {
    let byts;
    if (this.chunks.length === 1) {
      const chunk = this.chunks[0];
      if (reset && this.cursor > chunk.length / 2) {
        byts = this.cursor === chunk.length ? chunk : chunk.subarray(0, this.cursor);
        this._initReuseChunk = null;
        this.chunks = [];
      } else {
        byts = slice(chunk, 0, this.cursor);
      }
    } else {
      byts = concat(this.chunks, this.cursor);
    }
    if (reset) {
      this.reset();
    }
    return byts;
  }
};

// node_modules/cborg/lib/common.js
var decodeErrPrefix = "CBOR decode error:";
var encodeErrPrefix = "CBOR encode error:";
var uintMinorPrefixBytes = [];
uintMinorPrefixBytes[23] = 1;
uintMinorPrefixBytes[24] = 2;
uintMinorPrefixBytes[25] = 3;
uintMinorPrefixBytes[26] = 5;
uintMinorPrefixBytes[27] = 9;
function assertEnoughData(data, pos, need) {
  if (data.length - pos < need) {
    throw new Error(`${decodeErrPrefix} not enough data for type`);
  }
}

// node_modules/cborg/lib/0uint.js
var uintBoundaries = [24, 256, 65536, 4294967296, BigInt("18446744073709551616")];
function readUint8(data, offset2, options) {
  assertEnoughData(data, offset2, 1);
  const value = data[offset2];
  if (options.strict === true && value < uintBoundaries[0]) {
    throw new Error(`${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`);
  }
  return value;
}
function readUint16(data, offset2, options) {
  assertEnoughData(data, offset2, 2);
  const value = data[offset2] << 8 | data[offset2 + 1];
  if (options.strict === true && value < uintBoundaries[1]) {
    throw new Error(`${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`);
  }
  return value;
}
function readUint32(data, offset2, options) {
  assertEnoughData(data, offset2, 4);
  const value = data[offset2] * 16777216 + (data[offset2 + 1] << 16) + (data[offset2 + 2] << 8) + data[offset2 + 3];
  if (options.strict === true && value < uintBoundaries[2]) {
    throw new Error(`${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`);
  }
  return value;
}
function readUint64(data, offset2, options) {
  assertEnoughData(data, offset2, 8);
  const hi = data[offset2] * 16777216 + (data[offset2 + 1] << 16) + (data[offset2 + 2] << 8) + data[offset2 + 3];
  const lo = data[offset2 + 4] * 16777216 + (data[offset2 + 5] << 16) + (data[offset2 + 6] << 8) + data[offset2 + 7];
  const value = (BigInt(hi) << BigInt(32)) + BigInt(lo);
  if (options.strict === true && value < uintBoundaries[3]) {
    throw new Error(`${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`);
  }
  if (value <= Number.MAX_SAFE_INTEGER) {
    return Number(value);
  }
  if (options.allowBigInt === true) {
    return value;
  }
  throw new Error(`${decodeErrPrefix} integers outside of the safe integer range are not supported`);
}
function decodeUint8(data, pos, _minor, options) {
  return new Token(Type.uint, readUint8(data, pos + 1, options), 2);
}
function decodeUint16(data, pos, _minor, options) {
  return new Token(Type.uint, readUint16(data, pos + 1, options), 3);
}
function decodeUint32(data, pos, _minor, options) {
  return new Token(Type.uint, readUint32(data, pos + 1, options), 5);
}
function decodeUint64(data, pos, _minor, options) {
  return new Token(Type.uint, readUint64(data, pos + 1, options), 9);
}
function encodeUint(buf2, token) {
  return encodeUintValue(buf2, 0, token.value);
}
function encodeUintValue(buf2, major, uint) {
  if (uint < uintBoundaries[0]) {
    const nuint = Number(uint);
    buf2.push([major | nuint]);
  } else if (uint < uintBoundaries[1]) {
    const nuint = Number(uint);
    buf2.push([major | 24, nuint]);
  } else if (uint < uintBoundaries[2]) {
    const nuint = Number(uint);
    buf2.push([major | 25, nuint >>> 8, nuint & 255]);
  } else if (uint < uintBoundaries[3]) {
    const nuint = Number(uint);
    buf2.push([major | 26, nuint >>> 24 & 255, nuint >>> 16 & 255, nuint >>> 8 & 255, nuint & 255]);
  } else {
    const buint = BigInt(uint);
    if (buint < uintBoundaries[4]) {
      const set7 = [major | 27, 0, 0, 0, 0, 0, 0, 0];
      let lo = Number(buint & BigInt(4294967295));
      let hi = Number(buint >> BigInt(32) & BigInt(4294967295));
      set7[8] = lo & 255;
      lo = lo >> 8;
      set7[7] = lo & 255;
      lo = lo >> 8;
      set7[6] = lo & 255;
      lo = lo >> 8;
      set7[5] = lo & 255;
      set7[4] = hi & 255;
      hi = hi >> 8;
      set7[3] = hi & 255;
      hi = hi >> 8;
      set7[2] = hi & 255;
      hi = hi >> 8;
      set7[1] = hi & 255;
      buf2.push(set7);
    } else {
      throw new Error(`${decodeErrPrefix} encountered BigInt larger than allowable range`);
    }
  }
}
encodeUint.encodedSize = function encodedSize(token) {
  return encodeUintValue.encodedSize(token.value);
};
encodeUintValue.encodedSize = function encodedSize2(uint) {
  if (uint < uintBoundaries[0]) {
    return 1;
  }
  if (uint < uintBoundaries[1]) {
    return 2;
  }
  if (uint < uintBoundaries[2]) {
    return 3;
  }
  if (uint < uintBoundaries[3]) {
    return 5;
  }
  return 9;
};
encodeUint.compareTokens = function compareTokens(tok1, tok2) {
  return tok1.value < tok2.value ? -1 : tok1.value > tok2.value ? 1 : (
    /* c8 ignore next */
    0
  );
};

// node_modules/cborg/lib/1negint.js
function decodeNegint8(data, pos, _minor, options) {
  return new Token(Type.negint, -1 - readUint8(data, pos + 1, options), 2);
}
function decodeNegint16(data, pos, _minor, options) {
  return new Token(Type.negint, -1 - readUint16(data, pos + 1, options), 3);
}
function decodeNegint32(data, pos, _minor, options) {
  return new Token(Type.negint, -1 - readUint32(data, pos + 1, options), 5);
}
var neg1b = BigInt(-1);
var pos1b = BigInt(1);
function decodeNegint64(data, pos, _minor, options) {
  const int = readUint64(data, pos + 1, options);
  if (typeof int !== "bigint") {
    const value = -1 - int;
    if (value >= Number.MIN_SAFE_INTEGER) {
      return new Token(Type.negint, value, 9);
    }
  }
  if (options.allowBigInt !== true) {
    throw new Error(`${decodeErrPrefix} integers outside of the safe integer range are not supported`);
  }
  return new Token(Type.negint, neg1b - BigInt(int), 9);
}
function encodeNegint(buf2, token) {
  const negint = token.value;
  const unsigned = typeof negint === "bigint" ? negint * neg1b - pos1b : negint * -1 - 1;
  encodeUintValue(buf2, token.type.majorEncoded, unsigned);
}
encodeNegint.encodedSize = function encodedSize3(token) {
  const negint = token.value;
  const unsigned = typeof negint === "bigint" ? negint * neg1b - pos1b : negint * -1 - 1;
  if (unsigned < uintBoundaries[0]) {
    return 1;
  }
  if (unsigned < uintBoundaries[1]) {
    return 2;
  }
  if (unsigned < uintBoundaries[2]) {
    return 3;
  }
  if (unsigned < uintBoundaries[3]) {
    return 5;
  }
  return 9;
};
encodeNegint.compareTokens = function compareTokens2(tok1, tok2) {
  return tok1.value < tok2.value ? 1 : tok1.value > tok2.value ? -1 : (
    /* c8 ignore next */
    0
  );
};

// node_modules/cborg/lib/2bytes.js
function toToken(data, pos, prefix2, length2) {
  assertEnoughData(data, pos, prefix2 + length2);
  const buf2 = slice(data, pos + prefix2, pos + prefix2 + length2);
  return new Token(Type.bytes, buf2, prefix2 + length2);
}
function decodeBytesCompact(data, pos, minor, _options2) {
  return toToken(data, pos, 1, minor);
}
function decodeBytes8(data, pos, _minor, options) {
  return toToken(data, pos, 2, readUint8(data, pos + 1, options));
}
function decodeBytes16(data, pos, _minor, options) {
  return toToken(data, pos, 3, readUint16(data, pos + 1, options));
}
function decodeBytes32(data, pos, _minor, options) {
  return toToken(data, pos, 5, readUint32(data, pos + 1, options));
}
function decodeBytes64(data, pos, _minor, options) {
  const l = readUint64(data, pos + 1, options);
  if (typeof l === "bigint") {
    throw new Error(`${decodeErrPrefix} 64-bit integer bytes lengths not supported`);
  }
  return toToken(data, pos, 9, l);
}
function tokenBytes(token) {
  if (token.encodedBytes === void 0) {
    token.encodedBytes = token.type === Type.string ? fromString(token.value) : token.value;
  }
  return token.encodedBytes;
}
function encodeBytes(buf2, token) {
  const bytes3 = tokenBytes(token);
  encodeUintValue(buf2, token.type.majorEncoded, bytes3.length);
  buf2.push(bytes3);
}
encodeBytes.encodedSize = function encodedSize4(token) {
  const bytes3 = tokenBytes(token);
  return encodeUintValue.encodedSize(bytes3.length) + bytes3.length;
};
encodeBytes.compareTokens = function compareTokens3(tok1, tok2) {
  return compareBytes(tokenBytes(tok1), tokenBytes(tok2));
};
function compareBytes(b1, b2) {
  return b1.length < b2.length ? -1 : b1.length > b2.length ? 1 : compare(b1, b2);
}

// node_modules/cborg/lib/3string.js
function toToken2(data, pos, prefix2, length2, options) {
  const totLength = prefix2 + length2;
  assertEnoughData(data, pos, totLength);
  const tok = new Token(Type.string, toString(data, pos + prefix2, pos + totLength), totLength);
  if (options.retainStringBytes === true) {
    tok.byteValue = slice(data, pos + prefix2, pos + totLength);
  }
  return tok;
}
function decodeStringCompact(data, pos, minor, options) {
  return toToken2(data, pos, 1, minor, options);
}
function decodeString8(data, pos, _minor, options) {
  return toToken2(data, pos, 2, readUint8(data, pos + 1, options), options);
}
function decodeString16(data, pos, _minor, options) {
  return toToken2(data, pos, 3, readUint16(data, pos + 1, options), options);
}
function decodeString32(data, pos, _minor, options) {
  return toToken2(data, pos, 5, readUint32(data, pos + 1, options), options);
}
function decodeString64(data, pos, _minor, options) {
  const l = readUint64(data, pos + 1, options);
  if (typeof l === "bigint") {
    throw new Error(`${decodeErrPrefix} 64-bit integer string lengths not supported`);
  }
  return toToken2(data, pos, 9, l, options);
}
var encodeString = encodeBytes;

// node_modules/cborg/lib/4array.js
function toToken3(_data3, _pos, prefix2, length2) {
  return new Token(Type.array, length2, prefix2);
}
function decodeArrayCompact(data, pos, minor, _options2) {
  return toToken3(data, pos, 1, minor);
}
function decodeArray8(data, pos, _minor, options) {
  return toToken3(data, pos, 2, readUint8(data, pos + 1, options));
}
function decodeArray16(data, pos, _minor, options) {
  return toToken3(data, pos, 3, readUint16(data, pos + 1, options));
}
function decodeArray32(data, pos, _minor, options) {
  return toToken3(data, pos, 5, readUint32(data, pos + 1, options));
}
function decodeArray64(data, pos, _minor, options) {
  const l = readUint64(data, pos + 1, options);
  if (typeof l === "bigint") {
    throw new Error(`${decodeErrPrefix} 64-bit integer array lengths not supported`);
  }
  return toToken3(data, pos, 9, l);
}
function decodeArrayIndefinite(data, pos, _minor, options) {
  if (options.allowIndefinite === false) {
    throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
  }
  return toToken3(data, pos, 1, Infinity);
}
function encodeArray(buf2, token) {
  encodeUintValue(buf2, Type.array.majorEncoded, token.value);
}
encodeArray.compareTokens = encodeUint.compareTokens;
encodeArray.encodedSize = function encodedSize5(token) {
  return encodeUintValue.encodedSize(token.value);
};

// node_modules/cborg/lib/5map.js
function toToken4(_data3, _pos, prefix2, length2) {
  return new Token(Type.map, length2, prefix2);
}
function decodeMapCompact(data, pos, minor, _options2) {
  return toToken4(data, pos, 1, minor);
}
function decodeMap8(data, pos, _minor, options) {
  return toToken4(data, pos, 2, readUint8(data, pos + 1, options));
}
function decodeMap16(data, pos, _minor, options) {
  return toToken4(data, pos, 3, readUint16(data, pos + 1, options));
}
function decodeMap32(data, pos, _minor, options) {
  return toToken4(data, pos, 5, readUint32(data, pos + 1, options));
}
function decodeMap64(data, pos, _minor, options) {
  const l = readUint64(data, pos + 1, options);
  if (typeof l === "bigint") {
    throw new Error(`${decodeErrPrefix} 64-bit integer map lengths not supported`);
  }
  return toToken4(data, pos, 9, l);
}
function decodeMapIndefinite(data, pos, _minor, options) {
  if (options.allowIndefinite === false) {
    throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
  }
  return toToken4(data, pos, 1, Infinity);
}
function encodeMap(buf2, token) {
  encodeUintValue(buf2, Type.map.majorEncoded, token.value);
}
encodeMap.compareTokens = encodeUint.compareTokens;
encodeMap.encodedSize = function encodedSize6(token) {
  return encodeUintValue.encodedSize(token.value);
};

// node_modules/cborg/lib/6tag.js
function decodeTagCompact(_data3, _pos, minor, _options2) {
  return new Token(Type.tag, minor, 1);
}
function decodeTag8(data, pos, _minor, options) {
  return new Token(Type.tag, readUint8(data, pos + 1, options), 2);
}
function decodeTag16(data, pos, _minor, options) {
  return new Token(Type.tag, readUint16(data, pos + 1, options), 3);
}
function decodeTag32(data, pos, _minor, options) {
  return new Token(Type.tag, readUint32(data, pos + 1, options), 5);
}
function decodeTag64(data, pos, _minor, options) {
  return new Token(Type.tag, readUint64(data, pos + 1, options), 9);
}
function encodeTag(buf2, token) {
  encodeUintValue(buf2, Type.tag.majorEncoded, token.value);
}
encodeTag.compareTokens = encodeUint.compareTokens;
encodeTag.encodedSize = function encodedSize7(token) {
  return encodeUintValue.encodedSize(token.value);
};

// node_modules/cborg/lib/7float.js
var MINOR_FALSE = 20;
var MINOR_TRUE = 21;
var MINOR_NULL = 22;
var MINOR_UNDEFINED = 23;
function decodeUndefined(_data3, _pos, _minor, options) {
  if (options.allowUndefined === false) {
    throw new Error(`${decodeErrPrefix} undefined values are not supported`);
  } else if (options.coerceUndefinedToNull === true) {
    return new Token(Type.null, null, 1);
  }
  return new Token(Type.undefined, void 0, 1);
}
function decodeBreak(_data3, _pos, _minor, options) {
  if (options.allowIndefinite === false) {
    throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
  }
  return new Token(Type.break, void 0, 1);
}
function createToken(value, bytes3, options) {
  if (options) {
    if (options.allowNaN === false && Number.isNaN(value)) {
      throw new Error(`${decodeErrPrefix} NaN values are not supported`);
    }
    if (options.allowInfinity === false && (value === Infinity || value === -Infinity)) {
      throw new Error(`${decodeErrPrefix} Infinity values are not supported`);
    }
  }
  return new Token(Type.float, value, bytes3);
}
function decodeFloat16(data, pos, _minor, options) {
  return createToken(readFloat16(data, pos + 1), 3, options);
}
function decodeFloat32(data, pos, _minor, options) {
  return createToken(readFloat32(data, pos + 1), 5, options);
}
function decodeFloat64(data, pos, _minor, options) {
  return createToken(readFloat64(data, pos + 1), 9, options);
}
function encodeFloat(buf2, token, options) {
  const float2 = token.value;
  if (float2 === false) {
    buf2.push([Type.float.majorEncoded | MINOR_FALSE]);
  } else if (float2 === true) {
    buf2.push([Type.float.majorEncoded | MINOR_TRUE]);
  } else if (float2 === null) {
    buf2.push([Type.float.majorEncoded | MINOR_NULL]);
  } else if (float2 === void 0) {
    buf2.push([Type.float.majorEncoded | MINOR_UNDEFINED]);
  } else {
    let decoded;
    let success = false;
    if (!options || options.float64 !== true) {
      encodeFloat16(float2);
      decoded = readFloat16(ui8a, 1);
      if (float2 === decoded || Number.isNaN(float2)) {
        ui8a[0] = 249;
        buf2.push(ui8a.slice(0, 3));
        success = true;
      } else {
        encodeFloat32(float2);
        decoded = readFloat32(ui8a, 1);
        if (float2 === decoded) {
          ui8a[0] = 250;
          buf2.push(ui8a.slice(0, 5));
          success = true;
        }
      }
    }
    if (!success) {
      encodeFloat64(float2);
      decoded = readFloat64(ui8a, 1);
      ui8a[0] = 251;
      buf2.push(ui8a.slice(0, 9));
    }
  }
}
encodeFloat.encodedSize = function encodedSize8(token, options) {
  const float2 = token.value;
  if (float2 === false || float2 === true || float2 === null || float2 === void 0) {
    return 1;
  }
  if (!options || options.float64 !== true) {
    encodeFloat16(float2);
    let decoded = readFloat16(ui8a, 1);
    if (float2 === decoded || Number.isNaN(float2)) {
      return 3;
    }
    encodeFloat32(float2);
    decoded = readFloat32(ui8a, 1);
    if (float2 === decoded) {
      return 5;
    }
  }
  return 9;
};
var buffer = new ArrayBuffer(9);
var dataView = new DataView(buffer, 1);
var ui8a = new Uint8Array(buffer, 0);
function encodeFloat16(inp) {
  if (inp === Infinity) {
    dataView.setUint16(0, 31744, false);
  } else if (inp === -Infinity) {
    dataView.setUint16(0, 64512, false);
  } else if (Number.isNaN(inp)) {
    dataView.setUint16(0, 32256, false);
  } else {
    dataView.setFloat32(0, inp);
    const valu32 = dataView.getUint32(0);
    const exponent = (valu32 & 2139095040) >> 23;
    const mantissa = valu32 & 8388607;
    if (exponent === 255) {
      dataView.setUint16(0, 31744, false);
    } else if (exponent === 0) {
      dataView.setUint16(0, (inp & 2147483648) >> 16 | mantissa >> 13, false);
    } else {
      const logicalExponent = exponent - 127;
      if (logicalExponent < -24) {
        dataView.setUint16(0, 0);
      } else if (logicalExponent < -14) {
        dataView.setUint16(0, (valu32 & 2147483648) >> 16 | /* sign bit */
        1 << 24 + logicalExponent, false);
      } else {
        dataView.setUint16(0, (valu32 & 2147483648) >> 16 | logicalExponent + 15 << 10 | mantissa >> 13, false);
      }
    }
  }
}
function readFloat16(ui8a2, pos) {
  if (ui8a2.length - pos < 2) {
    throw new Error(`${decodeErrPrefix} not enough data for float16`);
  }
  const half = (ui8a2[pos] << 8) + ui8a2[pos + 1];
  if (half === 31744) {
    return Infinity;
  }
  if (half === 64512) {
    return -Infinity;
  }
  if (half === 32256) {
    return NaN;
  }
  const exp = half >> 10 & 31;
  const mant = half & 1023;
  let val;
  if (exp === 0) {
    val = mant * 2 ** -24;
  } else if (exp !== 31) {
    val = (mant + 1024) * 2 ** (exp - 25);
  } else {
    val = mant === 0 ? Infinity : NaN;
  }
  return half & 32768 ? -val : val;
}
function encodeFloat32(inp) {
  dataView.setFloat32(0, inp, false);
}
function readFloat32(ui8a2, pos) {
  if (ui8a2.length - pos < 4) {
    throw new Error(`${decodeErrPrefix} not enough data for float32`);
  }
  const offset2 = (ui8a2.byteOffset || 0) + pos;
  return new DataView(ui8a2.buffer, offset2, 4).getFloat32(0, false);
}
function encodeFloat64(inp) {
  dataView.setFloat64(0, inp, false);
}
function readFloat64(ui8a2, pos) {
  if (ui8a2.length - pos < 8) {
    throw new Error(`${decodeErrPrefix} not enough data for float64`);
  }
  const offset2 = (ui8a2.byteOffset || 0) + pos;
  return new DataView(ui8a2.buffer, offset2, 8).getFloat64(0, false);
}
encodeFloat.compareTokens = encodeUint.compareTokens;

// node_modules/cborg/lib/jump.js
function invalidMinor(data, pos, minor) {
  throw new Error(`${decodeErrPrefix} encountered invalid minor (${minor}) for major ${data[pos] >>> 5}`);
}
function errorer(msg) {
  return () => {
    throw new Error(`${decodeErrPrefix} ${msg}`);
  };
}
var jump = [];
for (let i = 0; i <= 23; i++) {
  jump[i] = invalidMinor;
}
jump[24] = decodeUint8;
jump[25] = decodeUint16;
jump[26] = decodeUint32;
jump[27] = decodeUint64;
jump[28] = invalidMinor;
jump[29] = invalidMinor;
jump[30] = invalidMinor;
jump[31] = invalidMinor;
for (let i = 32; i <= 55; i++) {
  jump[i] = invalidMinor;
}
jump[56] = decodeNegint8;
jump[57] = decodeNegint16;
jump[58] = decodeNegint32;
jump[59] = decodeNegint64;
jump[60] = invalidMinor;
jump[61] = invalidMinor;
jump[62] = invalidMinor;
jump[63] = invalidMinor;
for (let i = 64; i <= 87; i++) {
  jump[i] = decodeBytesCompact;
}
jump[88] = decodeBytes8;
jump[89] = decodeBytes16;
jump[90] = decodeBytes32;
jump[91] = decodeBytes64;
jump[92] = invalidMinor;
jump[93] = invalidMinor;
jump[94] = invalidMinor;
jump[95] = errorer("indefinite length bytes/strings are not supported");
for (let i = 96; i <= 119; i++) {
  jump[i] = decodeStringCompact;
}
jump[120] = decodeString8;
jump[121] = decodeString16;
jump[122] = decodeString32;
jump[123] = decodeString64;
jump[124] = invalidMinor;
jump[125] = invalidMinor;
jump[126] = invalidMinor;
jump[127] = errorer("indefinite length bytes/strings are not supported");
for (let i = 128; i <= 151; i++) {
  jump[i] = decodeArrayCompact;
}
jump[152] = decodeArray8;
jump[153] = decodeArray16;
jump[154] = decodeArray32;
jump[155] = decodeArray64;
jump[156] = invalidMinor;
jump[157] = invalidMinor;
jump[158] = invalidMinor;
jump[159] = decodeArrayIndefinite;
for (let i = 160; i <= 183; i++) {
  jump[i] = decodeMapCompact;
}
jump[184] = decodeMap8;
jump[185] = decodeMap16;
jump[186] = decodeMap32;
jump[187] = decodeMap64;
jump[188] = invalidMinor;
jump[189] = invalidMinor;
jump[190] = invalidMinor;
jump[191] = decodeMapIndefinite;
for (let i = 192; i <= 215; i++) {
  jump[i] = decodeTagCompact;
}
jump[216] = decodeTag8;
jump[217] = decodeTag16;
jump[218] = decodeTag32;
jump[219] = decodeTag64;
jump[220] = invalidMinor;
jump[221] = invalidMinor;
jump[222] = invalidMinor;
jump[223] = invalidMinor;
for (let i = 224; i <= 243; i++) {
  jump[i] = errorer("simple values are not supported");
}
jump[244] = invalidMinor;
jump[245] = invalidMinor;
jump[246] = invalidMinor;
jump[247] = decodeUndefined;
jump[248] = errorer("simple values are not supported");
jump[249] = decodeFloat16;
jump[250] = decodeFloat32;
jump[251] = decodeFloat64;
jump[252] = invalidMinor;
jump[253] = invalidMinor;
jump[254] = invalidMinor;
jump[255] = decodeBreak;
var quick = [];
for (let i = 0; i < 24; i++) {
  quick[i] = new Token(Type.uint, i, 1);
}
for (let i = -1; i >= -24; i--) {
  quick[31 - i] = new Token(Type.negint, i, 1);
}
quick[64] = new Token(Type.bytes, new Uint8Array(0), 1);
quick[96] = new Token(Type.string, "", 1);
quick[128] = new Token(Type.array, 0, 1);
quick[160] = new Token(Type.map, 0, 1);
quick[244] = new Token(Type.false, false, 1);
quick[245] = new Token(Type.true, true, 1);
quick[246] = new Token(Type.null, null, 1);
function quickEncodeToken(token) {
  switch (token.type) {
    case Type.false:
      return fromArray([244]);
    case Type.true:
      return fromArray([245]);
    case Type.null:
      return fromArray([246]);
    case Type.bytes:
      if (!token.value.length) {
        return fromArray([64]);
      }
      return;
    case Type.string:
      if (token.value === "") {
        return fromArray([96]);
      }
      return;
    case Type.array:
      if (token.value === 0) {
        return fromArray([128]);
      }
      return;
    case Type.map:
      if (token.value === 0) {
        return fromArray([160]);
      }
      return;
    case Type.uint:
      if (token.value < 24) {
        return fromArray([Number(token.value)]);
      }
      return;
    case Type.negint:
      if (token.value >= -24) {
        return fromArray([31 - Number(token.value)]);
      }
  }
}

// node_modules/cborg/lib/encode.js
var defaultEncodeOptions = {
  float64: false,
  mapSorter,
  quickEncodeToken
};
function makeCborEncoders() {
  const encoders = [];
  encoders[Type.uint.major] = encodeUint;
  encoders[Type.negint.major] = encodeNegint;
  encoders[Type.bytes.major] = encodeBytes;
  encoders[Type.string.major] = encodeString;
  encoders[Type.array.major] = encodeArray;
  encoders[Type.map.major] = encodeMap;
  encoders[Type.tag.major] = encodeTag;
  encoders[Type.float.major] = encodeFloat;
  return encoders;
}
var cborEncoders = makeCborEncoders();
var buf = new Bl();
var Ref = class _Ref {
  /**
   * @param {object|any[]} obj
   * @param {Reference|undefined} parent
   */
  constructor(obj, parent) {
    this.obj = obj;
    this.parent = parent;
  }
  /**
   * @param {object|any[]} obj
   * @returns {boolean}
   */
  includes(obj) {
    let p = this;
    do {
      if (p.obj === obj) {
        return true;
      }
    } while (p = p.parent);
    return false;
  }
  /**
   * @param {Reference|undefined} stack
   * @param {object|any[]} obj
   * @returns {Reference}
   */
  static createCheck(stack, obj) {
    if (stack && stack.includes(obj)) {
      throw new Error(`${encodeErrPrefix} object contains circular references`);
    }
    return new _Ref(obj, stack);
  }
};
var simpleTokens = {
  null: new Token(Type.null, null),
  undefined: new Token(Type.undefined, void 0),
  true: new Token(Type.true, true),
  false: new Token(Type.false, false),
  emptyArray: new Token(Type.array, 0),
  emptyMap: new Token(Type.map, 0)
};
var typeEncoders = {
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  number(obj, _typ, _options2, _refStack) {
    if (!Number.isInteger(obj) || !Number.isSafeInteger(obj)) {
      return new Token(Type.float, obj);
    } else if (obj >= 0) {
      return new Token(Type.uint, obj);
    } else {
      return new Token(Type.negint, obj);
    }
  },
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  bigint(obj, _typ, _options2, _refStack) {
    if (obj >= BigInt(0)) {
      return new Token(Type.uint, obj);
    } else {
      return new Token(Type.negint, obj);
    }
  },
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  Uint8Array(obj, _typ, _options2, _refStack) {
    return new Token(Type.bytes, obj);
  },
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  string(obj, _typ, _options2, _refStack) {
    return new Token(Type.string, obj);
  },
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  boolean(obj, _typ, _options2, _refStack) {
    return obj ? simpleTokens.true : simpleTokens.false;
  },
  /**
   * @param {any} _obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  null(_obj, _typ, _options2, _refStack) {
    return simpleTokens.null;
  },
  /**
   * @param {any} _obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  undefined(_obj, _typ, _options2, _refStack) {
    return simpleTokens.undefined;
  },
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  ArrayBuffer(obj, _typ, _options2, _refStack) {
    return new Token(Type.bytes, new Uint8Array(obj));
  },
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  DataView(obj, _typ, _options2, _refStack) {
    return new Token(Type.bytes, new Uint8Array(obj.buffer, obj.byteOffset, obj.byteLength));
  },
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} options
   * @param {Reference} [refStack]
   * @returns {TokenOrNestedTokens}
   */
  Array(obj, _typ, options, refStack) {
    if (!obj.length) {
      if (options.addBreakTokens === true) {
        return [simpleTokens.emptyArray, new Token(Type.break)];
      }
      return simpleTokens.emptyArray;
    }
    refStack = Ref.createCheck(refStack, obj);
    const entries3 = [];
    let i = 0;
    for (const e of obj) {
      entries3[i++] = objectToTokens(e, options, refStack);
    }
    if (options.addBreakTokens) {
      return [new Token(Type.array, obj.length), entries3, new Token(Type.break)];
    }
    return [new Token(Type.array, obj.length), entries3];
  },
  /**
   * @param {any} obj
   * @param {string} typ
   * @param {EncodeOptions} options
   * @param {Reference} [refStack]
   * @returns {TokenOrNestedTokens}
   */
  Object(obj, typ, options, refStack) {
    const isMap = typ !== "Object";
    const keys2 = isMap ? obj.keys() : Object.keys(obj);
    const length2 = isMap ? obj.size : keys2.length;
    if (!length2) {
      if (options.addBreakTokens === true) {
        return [simpleTokens.emptyMap, new Token(Type.break)];
      }
      return simpleTokens.emptyMap;
    }
    refStack = Ref.createCheck(refStack, obj);
    const entries3 = [];
    let i = 0;
    for (const key of keys2) {
      entries3[i++] = [
        objectToTokens(key, options, refStack),
        objectToTokens(isMap ? obj.get(key) : obj[key], options, refStack)
      ];
    }
    sortMapEntries(entries3, options);
    if (options.addBreakTokens) {
      return [new Token(Type.map, length2), entries3, new Token(Type.break)];
    }
    return [new Token(Type.map, length2), entries3];
  }
};
typeEncoders.Map = typeEncoders.Object;
typeEncoders.Buffer = typeEncoders.Uint8Array;
for (const typ of "Uint8Clamped Uint16 Uint32 Int8 Int16 Int32 BigUint64 BigInt64 Float32 Float64".split(" ")) {
  typeEncoders[`${typ}Array`] = typeEncoders.DataView;
}
function objectToTokens(obj, options = {}, refStack) {
  const typ = is(obj);
  const customTypeEncoder = options && options.typeEncoders && /** @type {OptionalTypeEncoder} */
  options.typeEncoders[typ] || typeEncoders[typ];
  if (typeof customTypeEncoder === "function") {
    const tokens = customTypeEncoder(obj, typ, options, refStack);
    if (tokens != null) {
      return tokens;
    }
  }
  const typeEncoder = typeEncoders[typ];
  if (!typeEncoder) {
    throw new Error(`${encodeErrPrefix} unsupported type: ${typ}`);
  }
  return typeEncoder(obj, typ, options, refStack);
}
function sortMapEntries(entries3, options) {
  if (options.mapSorter) {
    entries3.sort(options.mapSorter);
  }
}
function mapSorter(e1, e2) {
  const keyToken1 = Array.isArray(e1[0]) ? e1[0][0] : e1[0];
  const keyToken2 = Array.isArray(e2[0]) ? e2[0][0] : e2[0];
  if (keyToken1.type !== keyToken2.type) {
    return keyToken1.type.compare(keyToken2.type);
  }
  const major = keyToken1.type.major;
  const tcmp = cborEncoders[major].compareTokens(keyToken1, keyToken2);
  if (tcmp === 0) {
    console.warn("WARNING: complex key types used, CBOR key sorting guarantees are gone");
  }
  return tcmp;
}
function tokensToEncoded(buf2, tokens, encoders, options) {
  if (Array.isArray(tokens)) {
    for (const token of tokens) {
      tokensToEncoded(buf2, token, encoders, options);
    }
  } else {
    encoders[tokens.type.major](buf2, tokens, options);
  }
}
function encodeCustom(data, encoders, options) {
  const tokens = objectToTokens(data, options);
  if (!Array.isArray(tokens) && options.quickEncodeToken) {
    const quickBytes = options.quickEncodeToken(tokens);
    if (quickBytes) {
      return quickBytes;
    }
    const encoder3 = encoders[tokens.type.major];
    if (encoder3.encodedSize) {
      const size5 = encoder3.encodedSize(tokens, options);
      const buf2 = new Bl(size5);
      encoder3(buf2, tokens, options);
      if (buf2.chunks.length !== 1) {
        throw new Error(`Unexpected error: pre-calculated length for ${tokens} was wrong`);
      }
      return asU8A(buf2.chunks[0]);
    }
  }
  buf.reset();
  tokensToEncoded(buf, tokens, encoders, options);
  return buf.toBytes(true);
}
function encode(data, options) {
  options = Object.assign({}, defaultEncodeOptions, options);
  return encodeCustom(data, cborEncoders, options);
}

// node_modules/cborg/lib/decode.js
var defaultDecodeOptions = {
  strict: false,
  allowIndefinite: true,
  allowUndefined: true,
  allowBigInt: true
};
var Tokeniser = class {
  /**
   * @param {Uint8Array} data
   * @param {DecodeOptions} options
   */
  constructor(data, options = {}) {
    this._pos = 0;
    this.data = data;
    this.options = options;
  }
  pos() {
    return this._pos;
  }
  done() {
    return this._pos >= this.data.length;
  }
  next() {
    const byt = this.data[this._pos];
    let token = quick[byt];
    if (token === void 0) {
      const decoder3 = jump[byt];
      if (!decoder3) {
        throw new Error(`${decodeErrPrefix} no decoder for major type ${byt >>> 5} (byte 0x${byt.toString(16).padStart(2, "0")})`);
      }
      const minor = byt & 31;
      token = decoder3(this.data, this._pos, minor, this.options);
    }
    this._pos += token.encodedLength;
    return token;
  }
};
var DONE = Symbol.for("DONE");
var BREAK = Symbol.for("BREAK");
function tokenToArray(token, tokeniser, options) {
  const arr = [];
  for (let i = 0; i < token.value; i++) {
    const value = tokensToObject(tokeniser, options);
    if (value === BREAK) {
      if (token.value === Infinity) {
        break;
      }
      throw new Error(`${decodeErrPrefix} got unexpected break to lengthed array`);
    }
    if (value === DONE) {
      throw new Error(`${decodeErrPrefix} found array but not enough entries (got ${i}, expected ${token.value})`);
    }
    arr[i] = value;
  }
  return arr;
}
function tokenToMap(token, tokeniser, options) {
  const useMaps = options.useMaps === true;
  const obj = useMaps ? void 0 : {};
  const m = useMaps ? /* @__PURE__ */ new Map() : void 0;
  for (let i = 0; i < token.value; i++) {
    const key = tokensToObject(tokeniser, options);
    if (key === BREAK) {
      if (token.value === Infinity) {
        break;
      }
      throw new Error(`${decodeErrPrefix} got unexpected break to lengthed map`);
    }
    if (key === DONE) {
      throw new Error(`${decodeErrPrefix} found map but not enough entries (got ${i} [no key], expected ${token.value})`);
    }
    if (useMaps !== true && typeof key !== "string") {
      throw new Error(`${decodeErrPrefix} non-string keys not supported (got ${typeof key})`);
    }
    if (options.rejectDuplicateMapKeys === true) {
      if (useMaps && m.has(key) || !useMaps && key in obj) {
        throw new Error(`${decodeErrPrefix} found repeat map key "${key}"`);
      }
    }
    const value = tokensToObject(tokeniser, options);
    if (value === DONE) {
      throw new Error(`${decodeErrPrefix} found map but not enough entries (got ${i} [no value], expected ${token.value})`);
    }
    if (useMaps) {
      m.set(key, value);
    } else {
      obj[key] = value;
    }
  }
  return useMaps ? m : obj;
}
function tokensToObject(tokeniser, options) {
  if (tokeniser.done()) {
    return DONE;
  }
  const token = tokeniser.next();
  if (token.type === Type.break) {
    return BREAK;
  }
  if (token.type.terminal) {
    return token.value;
  }
  if (token.type === Type.array) {
    return tokenToArray(token, tokeniser, options);
  }
  if (token.type === Type.map) {
    return tokenToMap(token, tokeniser, options);
  }
  if (token.type === Type.tag) {
    if (options.tags && typeof options.tags[token.value] === "function") {
      const tagged = tokensToObject(tokeniser, options);
      return options.tags[token.value](tagged);
    }
    throw new Error(`${decodeErrPrefix} tag not supported (${token.value})`);
  }
  throw new Error("unsupported");
}
function decodeFirst(data, options) {
  if (!(data instanceof Uint8Array)) {
    throw new Error(`${decodeErrPrefix} data to decode must be a Uint8Array`);
  }
  options = Object.assign({}, defaultDecodeOptions, options);
  const tokeniser = options.tokenizer || new Tokeniser(data, options);
  const decoded = tokensToObject(tokeniser, options);
  if (decoded === DONE) {
    throw new Error(`${decodeErrPrefix} did not find any content to decode`);
  }
  if (decoded === BREAK) {
    throw new Error(`${decodeErrPrefix} got unexpected break`);
  }
  return [decoded, data.subarray(tokeniser.pos())];
}
function decode(data, options) {
  const [decoded, remainder] = decodeFirst(data, options);
  if (remainder.length > 0) {
    throw new Error(`${decodeErrPrefix} too many terminals, data makes no sense`);
  }
  return decoded;
}

// node_modules/multiformats/dist/src/bases/base32.js
var base32_exports = {};
__export(base32_exports, {
  base32: () => base32,
  base32hex: () => base32hex,
  base32hexpad: () => base32hexpad,
  base32hexpadupper: () => base32hexpadupper,
  base32hexupper: () => base32hexupper,
  base32pad: () => base32pad,
  base32padupper: () => base32padupper,
  base32upper: () => base32upper,
  base32z: () => base32z
});

// node_modules/multiformats/dist/src/bytes.js
var bytes_exports2 = {};
__export(bytes_exports2, {
  coerce: () => coerce,
  empty: () => empty,
  equals: () => equals,
  fromHex: () => fromHex,
  fromString: () => fromString2,
  isBinary: () => isBinary,
  toHex: () => toHex,
  toString: () => toString2
});
var empty = new Uint8Array(0);
function toHex(d) {
  return d.reduce((hex2, byte) => hex2 + byte.toString(16).padStart(2, "0"), "");
}
function fromHex(hex2) {
  const hexes2 = hex2.match(/../g);
  return hexes2 != null ? new Uint8Array(hexes2.map((b) => parseInt(b, 16))) : empty;
}
function equals(aa, bb) {
  if (aa === bb) {
    return true;
  }
  if (aa.byteLength !== bb.byteLength) {
    return false;
  }
  for (let ii = 0; ii < aa.byteLength; ii++) {
    if (aa[ii] !== bb[ii]) {
      return false;
    }
  }
  return true;
}
function coerce(o) {
  if (o instanceof Uint8Array && o.constructor.name === "Uint8Array") {
    return o;
  }
  if (o instanceof ArrayBuffer) {
    return new Uint8Array(o);
  }
  if (ArrayBuffer.isView(o)) {
    return new Uint8Array(o.buffer, o.byteOffset, o.byteLength);
  }
  throw new Error("Unknown type, must be binary type");
}
function isBinary(o) {
  return o instanceof ArrayBuffer || ArrayBuffer.isView(o);
}
function fromString2(str) {
  return new TextEncoder().encode(str);
}
function toString2(b) {
  return new TextDecoder().decode(b);
}

// node_modules/multiformats/dist/src/vendor/base-x.js
function base(ALPHABET, name15) {
  if (ALPHABET.length >= 255) {
    throw new TypeError("Alphabet too long");
  }
  var BASE_MAP = new Uint8Array(256);
  for (var j = 0; j < BASE_MAP.length; j++) {
    BASE_MAP[j] = 255;
  }
  for (var i = 0; i < ALPHABET.length; i++) {
    var x = ALPHABET.charAt(i);
    var xc = x.charCodeAt(0);
    if (BASE_MAP[xc] !== 255) {
      throw new TypeError(x + " is ambiguous");
    }
    BASE_MAP[xc] = i;
  }
  var BASE = ALPHABET.length;
  var LEADER = ALPHABET.charAt(0);
  var FACTOR = Math.log(BASE) / Math.log(256);
  var iFACTOR = Math.log(256) / Math.log(BASE);
  function encode32(source) {
    if (source instanceof Uint8Array)
      ;
    else if (ArrayBuffer.isView(source)) {
      source = new Uint8Array(source.buffer, source.byteOffset, source.byteLength);
    } else if (Array.isArray(source)) {
      source = Uint8Array.from(source);
    }
    if (!(source instanceof Uint8Array)) {
      throw new TypeError("Expected Uint8Array");
    }
    if (source.length === 0) {
      return "";
    }
    var zeroes = 0;
    var length2 = 0;
    var pbegin = 0;
    var pend = source.length;
    while (pbegin !== pend && source[pbegin] === 0) {
      pbegin++;
      zeroes++;
    }
    var size5 = (pend - pbegin) * iFACTOR + 1 >>> 0;
    var b58 = new Uint8Array(size5);
    while (pbegin !== pend) {
      var carry = source[pbegin];
      var i2 = 0;
      for (var it1 = size5 - 1; (carry !== 0 || i2 < length2) && it1 !== -1; it1--, i2++) {
        carry += 256 * b58[it1] >>> 0;
        b58[it1] = carry % BASE >>> 0;
        carry = carry / BASE >>> 0;
      }
      if (carry !== 0) {
        throw new Error("Non-zero carry");
      }
      length2 = i2;
      pbegin++;
    }
    var it2 = size5 - length2;
    while (it2 !== size5 && b58[it2] === 0) {
      it2++;
    }
    var str = LEADER.repeat(zeroes);
    for (; it2 < size5; ++it2) {
      str += ALPHABET.charAt(b58[it2]);
    }
    return str;
  }
  function decodeUnsafe(source) {
    if (typeof source !== "string") {
      throw new TypeError("Expected String");
    }
    if (source.length === 0) {
      return new Uint8Array();
    }
    var psz = 0;
    if (source[psz] === " ") {
      return;
    }
    var zeroes = 0;
    var length2 = 0;
    while (source[psz] === LEADER) {
      zeroes++;
      psz++;
    }
    var size5 = (source.length - psz) * FACTOR + 1 >>> 0;
    var b256 = new Uint8Array(size5);
    while (source[psz]) {
      var carry = BASE_MAP[source.charCodeAt(psz)];
      if (carry === 255) {
        return;
      }
      var i2 = 0;
      for (var it3 = size5 - 1; (carry !== 0 || i2 < length2) && it3 !== -1; it3--, i2++) {
        carry += BASE * b256[it3] >>> 0;
        b256[it3] = carry % 256 >>> 0;
        carry = carry / 256 >>> 0;
      }
      if (carry !== 0) {
        throw new Error("Non-zero carry");
      }
      length2 = i2;
      psz++;
    }
    if (source[psz] === " ") {
      return;
    }
    var it4 = size5 - length2;
    while (it4 !== size5 && b256[it4] === 0) {
      it4++;
    }
    var vch = new Uint8Array(zeroes + (size5 - it4));
    var j2 = zeroes;
    while (it4 !== size5) {
      vch[j2++] = b256[it4++];
    }
    return vch;
  }
  function decode36(string3) {
    var buffer2 = decodeUnsafe(string3);
    if (buffer2) {
      return buffer2;
    }
    throw new Error(`Non-${name15} character`);
  }
  return {
    encode: encode32,
    decodeUnsafe,
    decode: decode36
  };
}
var src = base;
var _brrp__multiformats_scope_baseX = src;
var base_x_default = _brrp__multiformats_scope_baseX;

// node_modules/multiformats/dist/src/bases/base.js
var Encoder = class {
  constructor(name15, prefix2, baseEncode) {
    __publicField(this, "name");
    __publicField(this, "prefix");
    __publicField(this, "baseEncode");
    this.name = name15;
    this.prefix = prefix2;
    this.baseEncode = baseEncode;
  }
  encode(bytes3) {
    if (bytes3 instanceof Uint8Array) {
      return `${this.prefix}${this.baseEncode(bytes3)}`;
    } else {
      throw Error("Unknown type, must be binary type");
    }
  }
};
var Decoder = class {
  constructor(name15, prefix2, baseDecode) {
    __publicField(this, "name");
    __publicField(this, "prefix");
    __publicField(this, "baseDecode");
    __publicField(this, "prefixCodePoint");
    this.name = name15;
    this.prefix = prefix2;
    const prefixCodePoint = prefix2.codePointAt(0);
    if (prefixCodePoint === void 0) {
      throw new Error("Invalid prefix character");
    }
    this.prefixCodePoint = prefixCodePoint;
    this.baseDecode = baseDecode;
  }
  decode(text2) {
    if (typeof text2 === "string") {
      if (text2.codePointAt(0) !== this.prefixCodePoint) {
        throw Error(`Unable to decode multibase string ${JSON.stringify(text2)}, ${this.name} decoder only supports inputs prefixed with ${this.prefix}`);
      }
      return this.baseDecode(text2.slice(this.prefix.length));
    } else {
      throw Error("Can only multibase decode strings");
    }
  }
  or(decoder3) {
    return or(this, decoder3);
  }
};
var ComposedDecoder = class {
  constructor(decoders) {
    __publicField(this, "decoders");
    this.decoders = decoders;
  }
  or(decoder3) {
    return or(this, decoder3);
  }
  decode(input11) {
    const prefix2 = input11[0];
    const decoder3 = this.decoders[prefix2];
    if (decoder3 != null) {
      return decoder3.decode(input11);
    } else {
      throw RangeError(`Unable to decode multibase string ${JSON.stringify(input11)}, only inputs prefixed with ${Object.keys(this.decoders)} are supported`);
    }
  }
};
function or(left, right) {
  return new ComposedDecoder({
    ...left.decoders ?? { [left.prefix]: left },
    ...right.decoders ?? { [right.prefix]: right }
  });
}
var Codec = class {
  constructor(name15, prefix2, baseEncode, baseDecode) {
    __publicField(this, "name");
    __publicField(this, "prefix");
    __publicField(this, "baseEncode");
    __publicField(this, "baseDecode");
    __publicField(this, "encoder");
    __publicField(this, "decoder");
    this.name = name15;
    this.prefix = prefix2;
    this.baseEncode = baseEncode;
    this.baseDecode = baseDecode;
    this.encoder = new Encoder(name15, prefix2, baseEncode);
    this.decoder = new Decoder(name15, prefix2, baseDecode);
  }
  encode(input11) {
    return this.encoder.encode(input11);
  }
  decode(input11) {
    return this.decoder.decode(input11);
  }
};
function from({ name: name15, prefix: prefix2, encode: encode32, decode: decode36 }) {
  return new Codec(name15, prefix2, encode32, decode36);
}
function baseX({ name: name15, prefix: prefix2, alphabet: alphabet3 }) {
  const { encode: encode32, decode: decode36 } = base_x_default(alphabet3, name15);
  return from({
    prefix: prefix2,
    name: name15,
    encode: encode32,
    decode: (text2) => coerce(decode36(text2))
  });
}
function decode2(string3, alphabetIdx, bitsPerChar, name15) {
  let end = string3.length;
  while (string3[end - 1] === "=") {
    --end;
  }
  const out = new Uint8Array(end * bitsPerChar / 8 | 0);
  let bits = 0;
  let buffer2 = 0;
  let written = 0;
  for (let i = 0; i < end; ++i) {
    const value = alphabetIdx[string3[i]];
    if (value === void 0) {
      throw new SyntaxError(`Non-${name15} character`);
    }
    buffer2 = buffer2 << bitsPerChar | value;
    bits += bitsPerChar;
    if (bits >= 8) {
      bits -= 8;
      out[written++] = 255 & buffer2 >> bits;
    }
  }
  if (bits >= bitsPerChar || (255 & buffer2 << 8 - bits) !== 0) {
    throw new SyntaxError("Unexpected end of data");
  }
  return out;
}
function encode2(data, alphabet3, bitsPerChar) {
  const pad2 = alphabet3[alphabet3.length - 1] === "=";
  const mask2 = (1 << bitsPerChar) - 1;
  let out = "";
  let bits = 0;
  let buffer2 = 0;
  for (let i = 0; i < data.length; ++i) {
    buffer2 = buffer2 << 8 | data[i];
    bits += 8;
    while (bits > bitsPerChar) {
      bits -= bitsPerChar;
      out += alphabet3[mask2 & buffer2 >> bits];
    }
  }
  if (bits !== 0) {
    out += alphabet3[mask2 & buffer2 << bitsPerChar - bits];
  }
  if (pad2) {
    while ((out.length * bitsPerChar & 7) !== 0) {
      out += "=";
    }
  }
  return out;
}
function createAlphabetIdx(alphabet3) {
  const alphabetIdx = {};
  for (let i = 0; i < alphabet3.length; ++i) {
    alphabetIdx[alphabet3[i]] = i;
  }
  return alphabetIdx;
}
function rfc4648({ name: name15, prefix: prefix2, bitsPerChar, alphabet: alphabet3 }) {
  const alphabetIdx = createAlphabetIdx(alphabet3);
  return from({
    prefix: prefix2,
    name: name15,
    encode(input11) {
      return encode2(input11, alphabet3, bitsPerChar);
    },
    decode(input11) {
      return decode2(input11, alphabetIdx, bitsPerChar, name15);
    }
  });
}

// node_modules/multiformats/dist/src/bases/base32.js
var base32 = rfc4648({
  prefix: "b",
  name: "base32",
  alphabet: "abcdefghijklmnopqrstuvwxyz234567",
  bitsPerChar: 5
});
var base32upper = rfc4648({
  prefix: "B",
  name: "base32upper",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567",
  bitsPerChar: 5
});
var base32pad = rfc4648({
  prefix: "c",
  name: "base32pad",
  alphabet: "abcdefghijklmnopqrstuvwxyz234567=",
  bitsPerChar: 5
});
var base32padupper = rfc4648({
  prefix: "C",
  name: "base32padupper",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567=",
  bitsPerChar: 5
});
var base32hex = rfc4648({
  prefix: "v",
  name: "base32hex",
  alphabet: "0123456789abcdefghijklmnopqrstuv",
  bitsPerChar: 5
});
var base32hexupper = rfc4648({
  prefix: "V",
  name: "base32hexupper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUV",
  bitsPerChar: 5
});
var base32hexpad = rfc4648({
  prefix: "t",
  name: "base32hexpad",
  alphabet: "0123456789abcdefghijklmnopqrstuv=",
  bitsPerChar: 5
});
var base32hexpadupper = rfc4648({
  prefix: "T",
  name: "base32hexpadupper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUV=",
  bitsPerChar: 5
});
var base32z = rfc4648({
  prefix: "h",
  name: "base32z",
  alphabet: "ybndrfg8ejkmcpqxot1uwisza345h769",
  bitsPerChar: 5
});

// node_modules/multiformats/dist/src/bases/base36.js
var base36_exports = {};
__export(base36_exports, {
  base36: () => base36,
  base36upper: () => base36upper
});
var base36 = baseX({
  prefix: "k",
  name: "base36",
  alphabet: "0123456789abcdefghijklmnopqrstuvwxyz"
});
var base36upper = baseX({
  prefix: "K",
  name: "base36upper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
});

// node_modules/multiformats/dist/src/bases/base58.js
var base58_exports = {};
__export(base58_exports, {
  base58btc: () => base58btc,
  base58flickr: () => base58flickr
});
var base58btc = baseX({
  name: "base58btc",
  prefix: "z",
  alphabet: "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz"
});
var base58flickr = baseX({
  name: "base58flickr",
  prefix: "Z",
  alphabet: "123456789abcdefghijkmnopqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ"
});

// node_modules/multiformats/dist/src/varint.js
var varint_exports = {};
__export(varint_exports, {
  decode: () => decode4,
  encodeTo: () => encodeTo,
  encodingLength: () => encodingLength
});

// node_modules/multiformats/dist/src/vendor/varint.js
var encode_1 = encode3;
var MSB = 128;
var REST = 127;
var MSBALL = ~REST;
var INT = Math.pow(2, 31);
function encode3(num, out, offset2) {
  out = out || [];
  offset2 = offset2 || 0;
  var oldOffset = offset2;
  while (num >= INT) {
    out[offset2++] = num & 255 | MSB;
    num /= 128;
  }
  while (num & MSBALL) {
    out[offset2++] = num & 255 | MSB;
    num >>>= 7;
  }
  out[offset2] = num | 0;
  encode3.bytes = offset2 - oldOffset + 1;
  return out;
}
var decode3 = read;
var MSB$1 = 128;
var REST$1 = 127;
function read(buf2, offset2) {
  var res = 0, offset2 = offset2 || 0, shift = 0, counter = offset2, b, l = buf2.length;
  do {
    if (counter >= l) {
      read.bytes = 0;
      throw new RangeError("Could not decode varint");
    }
    b = buf2[counter++];
    res += shift < 28 ? (b & REST$1) << shift : (b & REST$1) * Math.pow(2, shift);
    shift += 7;
  } while (b >= MSB$1);
  read.bytes = counter - offset2;
  return res;
}
var N1 = Math.pow(2, 7);
var N2 = Math.pow(2, 14);
var N3 = Math.pow(2, 21);
var N4 = Math.pow(2, 28);
var N5 = Math.pow(2, 35);
var N6 = Math.pow(2, 42);
var N7 = Math.pow(2, 49);
var N8 = Math.pow(2, 56);
var N9 = Math.pow(2, 63);
var length = function(value) {
  return value < N1 ? 1 : value < N2 ? 2 : value < N3 ? 3 : value < N4 ? 4 : value < N5 ? 5 : value < N6 ? 6 : value < N7 ? 7 : value < N8 ? 8 : value < N9 ? 9 : 10;
};
var varint = {
  encode: encode_1,
  decode: decode3,
  encodingLength: length
};
var _brrp_varint = varint;
var varint_default = _brrp_varint;

// node_modules/multiformats/dist/src/varint.js
function decode4(data, offset2 = 0) {
  const code19 = varint_default.decode(data, offset2);
  return [code19, varint_default.decode.bytes];
}
function encodeTo(int, target, offset2 = 0) {
  varint_default.encode(int, target, offset2);
  return target;
}
function encodingLength(int) {
  return varint_default.encodingLength(int);
}

// node_modules/multiformats/dist/src/hashes/digest.js
function create(code19, digest4) {
  const size5 = digest4.byteLength;
  const sizeOffset = encodingLength(code19);
  const digestOffset = sizeOffset + encodingLength(size5);
  const bytes3 = new Uint8Array(digestOffset + size5);
  encodeTo(code19, bytes3, 0);
  encodeTo(size5, bytes3, sizeOffset);
  bytes3.set(digest4, digestOffset);
  return new Digest(code19, size5, digest4, bytes3);
}
function decode5(multihash) {
  const bytes3 = coerce(multihash);
  const [code19, sizeOffset] = decode4(bytes3);
  const [size5, digestOffset] = decode4(bytes3.subarray(sizeOffset));
  const digest4 = bytes3.subarray(sizeOffset + digestOffset);
  if (digest4.byteLength !== size5) {
    throw new Error("Incorrect length");
  }
  return new Digest(code19, size5, digest4, bytes3);
}
function equals2(a, b) {
  if (a === b) {
    return true;
  } else {
    const data = b;
    return a.code === data.code && a.size === data.size && data.bytes instanceof Uint8Array && equals(a.bytes, data.bytes);
  }
}
var Digest = class {
  /**
   * Creates a multihash digest.
   */
  constructor(code19, size5, digest4, bytes3) {
    __publicField(this, "code");
    __publicField(this, "size");
    __publicField(this, "digest");
    __publicField(this, "bytes");
    this.code = code19;
    this.size = size5;
    this.digest = digest4;
    this.bytes = bytes3;
  }
};

// node_modules/multiformats/dist/src/cid.js
function format(link6, base3) {
  const { bytes: bytes3, version: version2 } = link6;
  switch (version2) {
    case 0:
      return toStringV0(bytes3, baseCache(link6), base3 ?? base58btc.encoder);
    default:
      return toStringV1(bytes3, baseCache(link6), base3 ?? base32.encoder);
  }
}
var cache = /* @__PURE__ */ new WeakMap();
function baseCache(cid) {
  const baseCache2 = cache.get(cid);
  if (baseCache2 == null) {
    const baseCache3 = /* @__PURE__ */ new Map();
    cache.set(cid, baseCache3);
    return baseCache3;
  }
  return baseCache2;
}
var _a;
var CID = class _CID {
  /**
   * @param version - Version of the CID
   * @param code - Code of the codec content is encoded in, see https://github.com/multiformats/multicodec/blob/master/table.csv
   * @param multihash - (Multi)hash of the of the content.
   */
  constructor(version2, code19, multihash, bytes3) {
    __publicField(this, "code");
    __publicField(this, "version");
    __publicField(this, "multihash");
    __publicField(this, "bytes");
    __publicField(this, "/");
    __publicField(this, _a, "CID");
    this.code = code19;
    this.version = version2;
    this.multihash = multihash;
    this.bytes = bytes3;
    this["/"] = bytes3;
  }
  /**
   * Signalling `cid.asCID === cid` has been replaced with `cid['/'] === cid.bytes`
   * please either use `CID.asCID(cid)` or switch to new signalling mechanism
   *
   * @deprecated
   */
  get asCID() {
    return this;
  }
  // ArrayBufferView
  get byteOffset() {
    return this.bytes.byteOffset;
  }
  // ArrayBufferView
  get byteLength() {
    return this.bytes.byteLength;
  }
  toV0() {
    switch (this.version) {
      case 0: {
        return this;
      }
      case 1: {
        const { code: code19, multihash } = this;
        if (code19 !== DAG_PB_CODE) {
          throw new Error("Cannot convert a non dag-pb CID to CIDv0");
        }
        if (multihash.code !== SHA_256_CODE) {
          throw new Error("Cannot convert non sha2-256 multihash CID to CIDv0");
        }
        return _CID.createV0(multihash);
      }
      default: {
        throw Error(`Can not convert CID version ${this.version} to version 0. This is a bug please report`);
      }
    }
  }
  toV1() {
    switch (this.version) {
      case 0: {
        const { code: code19, digest: digest4 } = this.multihash;
        const multihash = create(code19, digest4);
        return _CID.createV1(this.code, multihash);
      }
      case 1: {
        return this;
      }
      default: {
        throw Error(`Can not convert CID version ${this.version} to version 1. This is a bug please report`);
      }
    }
  }
  equals(other) {
    return _CID.equals(this, other);
  }
  static equals(self2, other) {
    const unknown2 = other;
    return unknown2 != null && self2.code === unknown2.code && self2.version === unknown2.version && equals2(self2.multihash, unknown2.multihash);
  }
  toString(base3) {
    return format(this, base3);
  }
  toJSON() {
    return { "/": format(this) };
  }
  link() {
    return this;
  }
  // Legacy
  [(_a = Symbol.toStringTag, Symbol.for("nodejs.util.inspect.custom"))]() {
    return `CID(${this.toString()})`;
  }
  /**
   * Takes any input `value` and returns a `CID` instance if it was
   * a `CID` otherwise returns `null`. If `value` is instanceof `CID`
   * it will return value back. If `value` is not instance of this CID
   * class, but is compatible CID it will return new instance of this
   * `CID` class. Otherwise returns null.
   *
   * This allows two different incompatible versions of CID library to
   * co-exist and interop as long as binary interface is compatible.
   */
  static asCID(input11) {
    if (input11 == null) {
      return null;
    }
    const value = input11;
    if (value instanceof _CID) {
      return value;
    } else if (value["/"] != null && value["/"] === value.bytes || value.asCID === value) {
      const { version: version2, code: code19, multihash, bytes: bytes3 } = value;
      return new _CID(version2, code19, multihash, bytes3 ?? encodeCID(version2, code19, multihash.bytes));
    } else if (value[cidSymbol] === true) {
      const { version: version2, multihash, code: code19 } = value;
      const digest4 = decode5(multihash);
      return _CID.create(version2, code19, digest4);
    } else {
      return null;
    }
  }
  /**
   * @param version - Version of the CID
   * @param code - Code of the codec content is encoded in, see https://github.com/multiformats/multicodec/blob/master/table.csv
   * @param digest - (Multi)hash of the of the content.
   */
  static create(version2, code19, digest4) {
    if (typeof code19 !== "number") {
      throw new Error("String codecs are no longer supported");
    }
    if (!(digest4.bytes instanceof Uint8Array)) {
      throw new Error("Invalid digest");
    }
    switch (version2) {
      case 0: {
        if (code19 !== DAG_PB_CODE) {
          throw new Error(`Version 0 CID must use dag-pb (code: ${DAG_PB_CODE}) block encoding`);
        } else {
          return new _CID(version2, code19, digest4, digest4.bytes);
        }
      }
      case 1: {
        const bytes3 = encodeCID(version2, code19, digest4.bytes);
        return new _CID(version2, code19, digest4, bytes3);
      }
      default: {
        throw new Error("Invalid version");
      }
    }
  }
  /**
   * Simplified version of `create` for CIDv0.
   */
  static createV0(digest4) {
    return _CID.create(0, DAG_PB_CODE, digest4);
  }
  /**
   * Simplified version of `create` for CIDv1.
   *
   * @param code - Content encoding format code.
   * @param digest - Multihash of the content.
   */
  static createV1(code19, digest4) {
    return _CID.create(1, code19, digest4);
  }
  /**
   * Decoded a CID from its binary representation. The byte array must contain
   * only the CID with no additional bytes.
   *
   * An error will be thrown if the bytes provided do not contain a valid
   * binary representation of a CID.
   */
  static decode(bytes3) {
    const [cid, remainder] = _CID.decodeFirst(bytes3);
    if (remainder.length !== 0) {
      throw new Error("Incorrect length");
    }
    return cid;
  }
  /**
   * Decoded a CID from its binary representation at the beginning of a byte
   * array.
   *
   * Returns an array with the first element containing the CID and the second
   * element containing the remainder of the original byte array. The remainder
   * will be a zero-length byte array if the provided bytes only contained a
   * binary CID representation.
   */
  static decodeFirst(bytes3) {
    const specs = _CID.inspectBytes(bytes3);
    const prefixSize = specs.size - specs.multihashSize;
    const multihashBytes = coerce(bytes3.subarray(prefixSize, prefixSize + specs.multihashSize));
    if (multihashBytes.byteLength !== specs.multihashSize) {
      throw new Error("Incorrect length");
    }
    const digestBytes = multihashBytes.subarray(specs.multihashSize - specs.digestSize);
    const digest4 = new Digest(specs.multihashCode, specs.digestSize, digestBytes, multihashBytes);
    const cid = specs.version === 0 ? _CID.createV0(digest4) : _CID.createV1(specs.codec, digest4);
    return [cid, bytes3.subarray(specs.size)];
  }
  /**
   * Inspect the initial bytes of a CID to determine its properties.
   *
   * Involves decoding up to 4 varints. Typically this will require only 4 to 6
   * bytes but for larger multicodec code values and larger multihash digest
   * lengths these varints can be quite large. It is recommended that at least
   * 10 bytes be made available in the `initialBytes` argument for a complete
   * inspection.
   */
  static inspectBytes(initialBytes) {
    let offset2 = 0;
    const next = () => {
      const [i, length2] = decode4(initialBytes.subarray(offset2));
      offset2 += length2;
      return i;
    };
    let version2 = next();
    let codec = DAG_PB_CODE;
    if (version2 === 18) {
      version2 = 0;
      offset2 = 0;
    } else {
      codec = next();
    }
    if (version2 !== 0 && version2 !== 1) {
      throw new RangeError(`Invalid CID version ${version2}`);
    }
    const prefixSize = offset2;
    const multihashCode = next();
    const digestSize = next();
    const size5 = offset2 + digestSize;
    const multihashSize = size5 - prefixSize;
    return { version: version2, codec, multihashCode, digestSize, multihashSize, size: size5 };
  }
  /**
   * Takes cid in a string representation and creates an instance. If `base`
   * decoder is not provided will use a default from the configuration. It will
   * throw an error if encoding of the CID is not compatible with supplied (or
   * a default decoder).
   */
  static parse(source, base3) {
    const [prefix2, bytes3] = parseCIDtoBytes(source, base3);
    const cid = _CID.decode(bytes3);
    if (cid.version === 0 && source[0] !== "Q") {
      throw Error("Version 0 CID string must not include multibase prefix");
    }
    baseCache(cid).set(prefix2, source);
    return cid;
  }
};
function parseCIDtoBytes(source, base3) {
  switch (source[0]) {
    case "Q": {
      const decoder3 = base3 ?? base58btc;
      return [
        base58btc.prefix,
        decoder3.decode(`${base58btc.prefix}${source}`)
      ];
    }
    case base58btc.prefix: {
      const decoder3 = base3 ?? base58btc;
      return [base58btc.prefix, decoder3.decode(source)];
    }
    case base32.prefix: {
      const decoder3 = base3 ?? base32;
      return [base32.prefix, decoder3.decode(source)];
    }
    case base36.prefix: {
      const decoder3 = base3 ?? base36;
      return [base36.prefix, decoder3.decode(source)];
    }
    default: {
      if (base3 == null) {
        throw Error("To parse non base32, base36 or base58btc encoded CID multibase decoder must be provided");
      }
      return [source[0], base3.decode(source)];
    }
  }
}
function toStringV0(bytes3, cache4, base3) {
  const { prefix: prefix2 } = base3;
  if (prefix2 !== base58btc.prefix) {
    throw Error(`Cannot string encode V0 in ${base3.name} encoding`);
  }
  const cid = cache4.get(prefix2);
  if (cid == null) {
    const cid2 = base3.encode(bytes3).slice(1);
    cache4.set(prefix2, cid2);
    return cid2;
  } else {
    return cid;
  }
}
function toStringV1(bytes3, cache4, base3) {
  const { prefix: prefix2 } = base3;
  const cid = cache4.get(prefix2);
  if (cid == null) {
    const cid2 = base3.encode(bytes3);
    cache4.set(prefix2, cid2);
    return cid2;
  } else {
    return cid;
  }
}
var DAG_PB_CODE = 112;
var SHA_256_CODE = 18;
function encodeCID(version2, code19, multihash) {
  const codeOffset = encodingLength(version2);
  const hashOffset = codeOffset + encodingLength(code19);
  const bytes3 = new Uint8Array(hashOffset + multihash.byteLength);
  encodeTo(version2, bytes3, 0);
  encodeTo(code19, bytes3, codeOffset);
  bytes3.set(multihash, hashOffset);
  return bytes3;
}
var cidSymbol = Symbol.for("@ipld/js-cid/CID");

// node_modules/@ipld/dag-cbor/src/index.js
var CID_CBOR_TAG = 42;
function toByteView(buf2) {
  if (buf2 instanceof ArrayBuffer) {
    return new Uint8Array(buf2, 0, buf2.byteLength);
  }
  return buf2;
}
function cidEncoder(obj) {
  if (obj.asCID !== obj && obj["/"] !== obj.bytes) {
    return null;
  }
  const cid = CID.asCID(obj);
  if (!cid) {
    return null;
  }
  const bytes3 = new Uint8Array(cid.bytes.byteLength + 1);
  bytes3.set(cid.bytes, 1);
  return [
    new Token(Type.tag, CID_CBOR_TAG),
    new Token(Type.bytes, bytes3)
  ];
}
function undefinedEncoder() {
  throw new Error("`undefined` is not supported by the IPLD Data Model and cannot be encoded");
}
function numberEncoder(num) {
  if (Number.isNaN(num)) {
    throw new Error("`NaN` is not supported by the IPLD Data Model and cannot be encoded");
  }
  if (num === Infinity || num === -Infinity) {
    throw new Error("`Infinity` and `-Infinity` is not supported by the IPLD Data Model and cannot be encoded");
  }
  return null;
}
function mapEncoder(map) {
  for (const key of map.keys()) {
    if (typeof key !== "string" || key.length === 0) {
      throw new Error("Non-string Map keys are not supported by the IPLD Data Model and cannot be encoded");
    }
  }
  return null;
}
var _encodeOptions = {
  float64: true,
  typeEncoders: {
    Map: mapEncoder,
    Object: cidEncoder,
    undefined: undefinedEncoder,
    number: numberEncoder
  }
};
var encodeOptions = {
  ..._encodeOptions,
  typeEncoders: {
    ..._encodeOptions.typeEncoders
  }
};
function cidDecoder(bytes3) {
  if (bytes3[0] !== 0) {
    throw new Error("Invalid CID for CBOR tag 42; expected leading 0x00");
  }
  return CID.decode(bytes3.subarray(1));
}
var _decodeOptions = {
  allowIndefinite: false,
  coerceUndefinedToNull: true,
  allowNaN: false,
  allowInfinity: false,
  allowBigInt: true,
  // this will lead to BigInt for ints outside of
  // safe-integer range, which may surprise users
  strict: true,
  useMaps: false,
  rejectDuplicateMapKeys: true,
  /** @type {import('cborg').TagDecoder[]} */
  tags: []
};
_decodeOptions.tags[CID_CBOR_TAG] = cidDecoder;
var decodeOptions = {
  ..._decodeOptions,
  tags: _decodeOptions.tags.slice()
};
var name = "dag-cbor";
var code = 113;
var encode4 = (node) => encode(node, _encodeOptions);
var decode6 = (data) => decode(toByteView(data), _decodeOptions);

// node_modules/@ipld/dag-ucan/src/utf8.js
var encoder = new TextEncoder();
var decoder = new TextDecoder();
var encode5 = (text2) => encoder.encode(text2);
var decode7 = (bytes3) => decoder.decode(bytes3);

// node_modules/multiformats/dist/src/link.js
var DAG_PB_CODE2 = 112;
function createLegacy(digest4) {
  return CID.create(0, DAG_PB_CODE2, digest4);
}
function create2(code19, digest4) {
  return CID.create(1, code19, digest4);
}
function isLink(value) {
  if (value == null) {
    return false;
  }
  const withSlash = value;
  if (withSlash["/"] != null && withSlash["/"] === withSlash.bytes) {
    return true;
  }
  const withAsCID = value;
  if (withAsCID.asCID === value) {
    return true;
  }
  return false;
}
function parse(source, base3) {
  return CID.parse(source, base3);
}

// node_modules/multiformats/dist/src/hashes/identity.js
var identity_exports = {};
__export(identity_exports, {
  identity: () => identity
});
var code2 = 0;
var name2 = "identity";
var encode6 = coerce;
function digest(input11, options) {
  if ((options == null ? void 0 : options.truncate) != null && options.truncate !== input11.byteLength) {
    if (options.truncate < 0 || options.truncate > input11.byteLength) {
      throw new Error(`Invalid truncate option, must be less than or equal to ${input11.byteLength}`);
    }
    input11 = input11.subarray(0, options.truncate);
  }
  return create(code2, encode6(input11));
}
var identity = { code: code2, name: name2, encode: encode6, digest };

// node_modules/@ipld/dag-ucan/src/did.js
var did_exports = {};
__export(did_exports, {
  BLS12381G1: () => BLS12381G1,
  BLS12381G2: () => BLS12381G2,
  DID_CORE: () => DID_CORE,
  ED25519: () => ED25519,
  P256: () => P256,
  P384: () => P384,
  P521: () => P521,
  RSA: () => RSA,
  SECP256K1: () => SECP256K1,
  decode: () => decode8,
  encode: () => encode7,
  format: () => format2,
  from: () => from3,
  parse: () => parse2
});

// node_modules/multiformats/dist/src/hashes/hasher.js
var DEFAULT_MIN_DIGEST_LENGTH = 20;
function from2({ name: name15, code: code19, encode: encode32, minDigestLength, maxDigestLength }) {
  return new Hasher(name15, code19, encode32, minDigestLength, maxDigestLength);
}
var Hasher = class {
  constructor(name15, code19, encode32, minDigestLength, maxDigestLength) {
    __publicField(this, "name");
    __publicField(this, "code");
    __publicField(this, "encode");
    __publicField(this, "minDigestLength");
    __publicField(this, "maxDigestLength");
    this.name = name15;
    this.code = code19;
    this.encode = encode32;
    this.minDigestLength = minDigestLength ?? DEFAULT_MIN_DIGEST_LENGTH;
    this.maxDigestLength = maxDigestLength;
  }
  digest(input11, options) {
    if ((options == null ? void 0 : options.truncate) != null) {
      if (options.truncate < this.minDigestLength) {
        throw new Error(`Invalid truncate option, must be greater than or equal to ${this.minDigestLength}`);
      }
      if (this.maxDigestLength != null && options.truncate > this.maxDigestLength) {
        throw new Error(`Invalid truncate option, must be less than or equal to ${this.maxDigestLength}`);
      }
    }
    if (input11 instanceof Uint8Array) {
      const result = this.encode(input11);
      if (result instanceof Uint8Array) {
        return createDigest(result, this.code, options == null ? void 0 : options.truncate);
      }
      return result.then((digest4) => createDigest(digest4, this.code, options == null ? void 0 : options.truncate));
    } else {
      throw Error("Unknown type, must be binary type");
    }
  }
};
function createDigest(digest4, code19, truncate2) {
  if (truncate2 != null && truncate2 !== digest4.byteLength) {
    if (truncate2 > digest4.byteLength) {
      throw new Error(`Invalid truncate option, must be less than or equal to ${digest4.byteLength}`);
    }
    digest4 = digest4.subarray(0, truncate2);
  }
  return create(code19, digest4);
}

// node_modules/@ipld/dag-ucan/src/did.js
var DID_PREFIX = "did:";
var DID_PREFIX_SIZE = DID_PREFIX.length;
var DID_KEY_PREFIX = `did:key:`;
var DID_KEY_PREFIX_SIZE = DID_KEY_PREFIX.length;
var ED25519 = 237;
var RSA = 4613;
var P256 = 4608;
var P384 = 4609;
var P521 = 4610;
var SECP256K1 = 231;
var BLS12381G1 = 234;
var BLS12381G2 = 235;
var DID_CORE = 3357;
var METHOD_OFFSET = varint_exports.encodingLength(DID_CORE);
var parse2 = (did2) => {
  if (!did2.startsWith(DID_PREFIX)) {
    throw new RangeError(`Invalid DID "${did2}", must start with 'did:'`);
  } else if (did2.startsWith(DID_KEY_PREFIX)) {
    const key = base58btc.decode(did2.slice(DID_KEY_PREFIX_SIZE));
    return decode8(key);
  } else {
    const suffix = encode5(did2.slice(DID_PREFIX_SIZE));
    const bytes3 = new Uint8Array(suffix.byteLength + METHOD_OFFSET);
    varint_exports.encodeTo(DID_CORE, bytes3);
    bytes3.set(suffix, METHOD_OFFSET);
    return new DID(bytes3);
  }
};
var format2 = (id) => id.did();
var from3 = (principal2) => {
  if (principal2 instanceof DID) {
    return principal2;
  } else if (principal2 instanceof Uint8Array) {
    return decode8(principal2);
  } else if (typeof principal2 === "string") {
    return parse2(principal2);
  } else {
    return parse2(principal2.did());
  }
};
var decode8 = (bytes3) => {
  const [code19] = varint_exports.decode(bytes3);
  const { buffer: buffer2, byteOffset, byteLength } = bytes3;
  switch (code19) {
    case P256:
      if (bytes3.length > 35) {
        throw new RangeError(`Only p256-pub compressed is supported.`);
      }
    case ED25519:
    case RSA:
    case P384:
    case P521:
    case BLS12381G1:
    case BLS12381G2:
    case SECP256K1:
      return (
        /** @type {UCAN.PrincipalView<any>} */
        new DIDKey(buffer2, byteOffset, byteLength)
      );
    case DID_CORE:
      return new DID(buffer2, byteOffset, byteLength);
    default:
      throw new RangeError(
        `Unsupported DID encoding, unknown multicode 0x${code19.toString(16)}.`
      );
  }
};
var encode7 = (principal2) => parse2(principal2.did());
var DID = class extends Uint8Array {
  /**
   * @returns {ID}
   */
  did() {
    const bytes3 = new Uint8Array(this.buffer, this.byteOffset + METHOD_OFFSET);
    return (
      /** @type {ID} */
      `did:${decode7(bytes3)}`
    );
  }
  toJSON() {
    return this.did();
  }
};
var DIDKey = class extends DID {
  /**
   * @return {`did:key:${string}`}
   */
  did() {
    return `did:key:${base58btc.encode(this)}`;
  }
};

// node_modules/multiformats/dist/src/codecs/raw.js
var raw_exports = {};
__export(raw_exports, {
  code: () => code3,
  decode: () => decode9,
  encode: () => encode8,
  name: () => name3
});
var name3 = "raw";
var code3 = 85;
function encode8(node) {
  return coerce(node);
}
function decode9(data) {
  return coerce(data);
}

// node_modules/@ipld/dag-ucan/src/signature.js
var signature_exports = {};
__export(signature_exports, {
  BLS12381G1: () => BLS12381G12,
  BLS12381G2: () => BLS12381G22,
  EIP191: () => EIP191,
  ES256: () => ES256,
  ES256K: () => ES256K,
  ES384: () => ES384,
  ES512: () => ES512,
  EdDSA: () => EdDSA,
  NON_STANDARD: () => NON_STANDARD,
  RS256: () => RS256,
  Signature: () => Signature,
  create: () => create3,
  createNamed: () => createNamed,
  createNonStandard: () => createNonStandard,
  decode: () => decode10,
  encode: () => encode9,
  format: () => format3,
  fromJSON: () => fromJSON2,
  nameCode: () => nameCode,
  parse: () => parse3,
  toJSON: () => toJSON2,
  view: () => view
});

// node_modules/multiformats/dist/src/bases/base64.js
var base64_exports = {};
__export(base64_exports, {
  base64: () => base64,
  base64pad: () => base64pad,
  base64url: () => base64url,
  base64urlpad: () => base64urlpad
});
var base64 = rfc4648({
  prefix: "m",
  name: "base64",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/",
  bitsPerChar: 6
});
var base64pad = rfc4648({
  prefix: "M",
  name: "base64pad",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=",
  bitsPerChar: 6
});
var base64url = rfc4648({
  prefix: "u",
  name: "base64url",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_",
  bitsPerChar: 6
});
var base64urlpad = rfc4648({
  prefix: "U",
  name: "base64urlpad",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_=",
  bitsPerChar: 6
});

// node_modules/@ipld/dag-ucan/src/signature.js
var NON_STANDARD = 53248;
var ES256K = 53479;
var BLS12381G12 = 53482;
var BLS12381G22 = 53483;
var EdDSA = 53485;
var ES256 = 13636096;
var ES384 = 13636097;
var ES512 = 13636098;
var RS256 = 13636101;
var EIP191 = 53649;
var codeName = (code19) => {
  switch (code19) {
    case ES256K:
      return "ES256K";
    case BLS12381G12:
      return "BLS12381G1";
    case BLS12381G22:
      return "BLS12381G2";
    case EdDSA:
      return "EdDSA";
    case ES256:
      return "ES256";
    case ES384:
      return "ES384";
    case ES512:
      return "ES512";
    case RS256:
      return "RS256";
    case EIP191:
      return "EIP191";
    default:
      throw new RangeError(
        `Unknown signature algorithm code 0x${code19.toString(16)}`
      );
  }
};
var nameCode = (name15) => {
  switch (name15) {
    case "ES256K":
      return ES256K;
    case "BLS12381G1":
      return BLS12381G12;
    case "BLS12381G2":
      return BLS12381G22;
    case "EdDSA":
      return EdDSA;
    case "ES256":
      return ES256;
    case "ES384":
      return ES384;
    case "ES512":
      return ES512;
    case "RS256":
      return RS256;
    case "EIP191":
      return EIP191;
    default:
      return NON_STANDARD;
  }
};
var Signature = class extends Uint8Array {
  get code() {
    const [code19] = varint_exports.decode(this);
    Object.defineProperties(this, { code: { value: code19 } });
    return (
      /** @type {A} */
      code19
    );
  }
  get size() {
    const value = size(this);
    Object.defineProperties(this, { size: { value } });
    return value;
  }
  get algorithm() {
    const value = algorithm(this);
    Object.defineProperties(this, { algorithm: { value } });
    return value;
  }
  get raw() {
    const { buffer: buffer2, byteOffset, size: size5, code: code19 } = this;
    const codeSize = varint_exports.encodingLength(code19);
    const rawSize = varint_exports.encodingLength(size5);
    const value = new Uint8Array(buffer2, byteOffset + codeSize + rawSize, size5);
    Object.defineProperties(this, { raw: { value } });
    return value;
  }
  /**
   * Verify that this signature was created by the given key.
   *
   * @param {UCAN.Crypto.Verifier<A>} signer
   * @param {UCAN.ByteView<T>} payload
   */
  async verify(signer, payload) {
    try {
      if (await signer.verify(payload, this) === true) {
        return { ok: {} };
      } else {
        throw new Error("Invalid signature");
      }
    } catch (cause) {
      return { error: (
        /** @type {Error} */
        cause
      ) };
    }
  }
  toJSON() {
    return toJSON2(this);
  }
};
var algorithm = (signature) => {
  const { code: code19, raw, buffer: buffer2, byteOffset } = signature;
  if (code19 === NON_STANDARD) {
    const offset2 = raw.byteLength + varint_exports.encodingLength(code19) + varint_exports.encodingLength(raw.byteLength);
    const bytes3 = new Uint8Array(buffer2, byteOffset + offset2);
    return decode7(bytes3);
  } else {
    return codeName(code19);
  }
};
var size = (signature) => {
  const offset2 = varint_exports.encodingLength(signature.code);
  const [size5] = varint_exports.decode(
    new Uint8Array(signature.buffer, signature.byteOffset + offset2)
  );
  return size5;
};
var create3 = (code19, raw) => {
  const _ = codeName(code19);
  const codeSize = varint_exports.encodingLength(code19);
  const rawSize = varint_exports.encodingLength(raw.byteLength);
  const signature = new Signature(codeSize + rawSize + raw.byteLength);
  varint_exports.encodeTo(code19, signature);
  varint_exports.encodeTo(raw.byteLength, signature, codeSize);
  signature.set(raw, codeSize + rawSize);
  Object.defineProperties(signature, {
    code: { value: code19 },
    size: { value: raw.byteLength }
  });
  return signature;
};
var createNamed = (name15, raw) => {
  const code19 = nameCode(name15);
  return code19 === NON_STANDARD ? createNonStandard(name15, raw) : create3(code19, raw);
};
var createNonStandard = (name15, raw) => {
  const code19 = NON_STANDARD;
  const codeSize = varint_exports.encodingLength(code19);
  const rawSize = varint_exports.encodingLength(raw.byteLength);
  const nameBytes = encode5(name15);
  const signature = new Signature(
    codeSize + rawSize + raw.byteLength + nameBytes.byteLength
  );
  varint_exports.encodeTo(code19, signature);
  varint_exports.encodeTo(raw.byteLength, signature, codeSize);
  signature.set(raw, codeSize + rawSize);
  signature.set(nameBytes, codeSize + rawSize + raw.byteLength);
  return signature;
};
var view = (bytes3) => new Signature(bytes3.buffer, bytes3.byteOffset, bytes3.byteLength);
var decode10 = (bytes3) => {
  if (!(bytes3 instanceof Uint8Array)) {
    throw new TypeError(
      `Can only decode Uint8Array into a Signature, instead got ${JSON.stringify(
        bytes3
      )}`
    );
  }
  const signature = view(bytes3);
  const { code: code19, algorithm: algorithm2, raw } = signature;
  return signature;
};
var encode9 = (signature) => decode10(signature);
var format3 = (signature, base3) => (base3 || base64url).encode(signature);
var parse3 = (signature, base3) => (
  /** @type {UCAN.SignatureView<T, A>} */
  decode10((base3 || base64url).decode(signature))
);
var toJSON2 = (signature) => ({
  "/": { bytes: base64.baseEncode(signature) }
});
var fromJSON2 = (json) => decode10(base64.baseDecode(json["/"].bytes));

// node_modules/@ipld/dag-ucan/src/schema.js
var readPayload = (data) => readPayloadWith(data, {
  readPrincipal,
  readProof
});
var readJWTPayload = (data) => readPayloadWith(data, {
  readPrincipal: readStringPrincipal,
  readProof: readStringProof
});
var readPayloadWith = (data, { readPrincipal: readPrincipal2, readProof: readProof2 }) => ({
  iss: readPrincipal2(data.iss, "iss"),
  aud: readPrincipal2(data.aud, "aud"),
  att: readCapabilities(data.att, "att"),
  prf: readOptionalArray(data.prf, readProof2, "prf") || [],
  exp: readNullable(data.exp === Infinity ? null : data.exp, readInt, "exp"),
  nbf: readOptional(data.nbf, readInt, "nbf"),
  fct: readOptionalArray(data.fct, readFact, "fct") || [],
  nnc: readOptional(data.nnc, readString, "nnc")
});
var readSignature = (source) => {
  if (source instanceof Uint8Array) {
    return decode10(source);
  } else {
    throw new TypeError(
      `Can only decode Uint8Array into a Signature, instead got ${JSON.stringify(
        source
      )}`
    );
  }
};
var readInt = (input11, name15) => Number.isInteger(input11) ? (
  /** @type {number} */
  input11
) : ParseError.throw(
  `Expected ${name15} to be integer, instead got ${JSON.stringify(input11)}`
);
var readCapability = (input11, context2) => readStruct(input11, asCapability, context2);
var readCapabilities = (input11, context2) => (
  /** @type {C} */
  readArray(input11, readCapability, context2)
);
var asCapability = (input11) => (
  /** @type {C} */
  {
    ...input11,
    can: readAbility(input11.can),
    with: readResource(input11.with)
  }
);
var readAbility = (input11) => typeof input11 !== "string" ? ParseError.throw(
  `Capability has invalid 'can: ${JSON.stringify(
    input11
  )}', value must be a string`
) : input11.slice(1, -1).includes("/") ? (
  /** @type {UCAN.Ability} */
  input11.toLocaleLowerCase()
) : input11 === "*" ? input11 : ParseError.throw(
  `Capability has invalid 'can: "${input11}"', value must have at least one path segment`
);
var readResource = (input11) => typeof input11 !== "string" ? ParseError.throw(
  `Capability has invalid 'with: ${JSON.stringify(
    input11
  )}', value must be a string`
) : parseURL(input11) || ParseError.throw(
  `Capability has invalid 'with: "${input11}"', value must be a valid URI string`
);
var parseURL = (input11) => {
  try {
    new URL(input11);
    return input11;
  } catch (_) {
    return null;
  }
};
var readArray = (input11, read8, context2) => Array.isArray(input11) ? input11.map((element, n) => read8(element, `${context2}[${n}]`)) : ParseError.throw(`${context2} must be an array`);
var readOptionalArray = (input11, reader, context2) => input11 === void 0 ? input11 : readArray(input11, reader, context2);
var readStruct = (input11, reader, context2) => input11 != null && typeof input11 === "object" ? reader(input11) : ParseError.throw(
  `${context2} must be of type object, instead got ${input11}`
);
var readFact = (input11, context2) => readStruct(input11, Object, context2);
var readProof = (source, context2) => isLink(source) ? (
  /** @type {UCAN.Link} */
  source
) : fail(
  `Expected ${context2} to be IPLD link, instead got ${JSON.stringify(
    source
  )}`
);
var readStringProof = (source, context2) => parseProof(readString(source, context2));
var parseProof = (source) => {
  try {
    return parse(source);
  } catch (error4) {
    return create2(code3, identity.digest(encode5(source)));
  }
};
var readPrincipal = (input11, context2) => decode8(readBytes(input11, context2));
var readStringPrincipal = (source, context2) => parse2(readString(source, context2));
var readOptional = (source, read8, context2 = "Field") => source !== void 0 ? read8(source, context2) : void 0;
var readNullable = (source, read8, context2) => source === null ? null : read8(source, context2);
var readString = (source, context2 = "Field") => typeof source === "string" ? source : fail(`${context2} has invalid value ${source}`);
var readBytes = (source, context2) => source instanceof Uint8Array ? source : fail(
  `Expected ${context2} to be Uint8Array, instead got ${JSON.stringify(
    source
  )}`
);
var readVersion = (input11, context2) => /\d+\.\d+\.\d+/.test(
  /** @type {string} */
  input11
) ? (
  /** @type {UCAN.Version} */
  input11
) : ParseError.throw(`Invalid version '${context2}: ${JSON.stringify(input11)}'`);
var readLiteral = (input11, literal2, context2) => input11 === literal2 ? literal2 : ParseError.throw(
  `Expected ${context2} to be a ${JSON.stringify(
    literal2
  )} instead got ${JSON.stringify(input11)}`
);
var ParseError = class extends TypeError {
  get name() {
    return "ParseError";
  }
  /**
   * @param {string} message
   * @returns {never}
   */
  static throw(message) {
    throw new this(message);
  }
};
var fail = (reason) => ParseError.throw(reason);

// node_modules/cborg/lib/json/encode.js
var JSONEncoder = class extends Array {
  constructor() {
    super();
    this.inRecursive = [];
  }
  /**
   * @param {Bl} buf
   */
  prefix(buf2) {
    const recurs = this.inRecursive[this.inRecursive.length - 1];
    if (recurs) {
      if (recurs.type === Type.array) {
        recurs.elements++;
        if (recurs.elements !== 1) {
          buf2.push([44]);
        }
      }
      if (recurs.type === Type.map) {
        recurs.elements++;
        if (recurs.elements !== 1) {
          if (recurs.elements % 2 === 1) {
            buf2.push([44]);
          } else {
            buf2.push([58]);
          }
        }
      }
    }
  }
  /**
   * @param {Bl} buf
   * @param {Token} token
   */
  [Type.uint.major](buf2, token) {
    this.prefix(buf2);
    const is2 = String(token.value);
    const isa = [];
    for (let i = 0; i < is2.length; i++) {
      isa[i] = is2.charCodeAt(i);
    }
    buf2.push(isa);
  }
  /**
   * @param {Bl} buf
   * @param {Token} token
   */
  [Type.negint.major](buf2, token) {
    this[Type.uint.major](buf2, token);
  }
  /**
   * @param {Bl} _buf
   * @param {Token} _token
   */
  [Type.bytes.major](_buf, _token) {
    throw new Error(`${encodeErrPrefix} unsupported type: Uint8Array`);
  }
  /**
   * @param {Bl} buf
   * @param {Token} token
   */
  [Type.string.major](buf2, token) {
    this.prefix(buf2);
    const byts = fromString(JSON.stringify(token.value));
    buf2.push(byts.length > 32 ? asU8A(byts) : byts);
  }
  /**
   * @param {Bl} buf
   * @param {Token} _token
   */
  [Type.array.major](buf2, _token) {
    this.prefix(buf2);
    this.inRecursive.push({ type: Type.array, elements: 0 });
    buf2.push([91]);
  }
  /**
   * @param {Bl} buf
   * @param {Token} _token
   */
  [Type.map.major](buf2, _token) {
    this.prefix(buf2);
    this.inRecursive.push({ type: Type.map, elements: 0 });
    buf2.push([123]);
  }
  /**
   * @param {Bl} _buf
   * @param {Token} _token
   */
  [Type.tag.major](_buf, _token) {
  }
  /**
   * @param {Bl} buf
   * @param {Token} token
   */
  [Type.float.major](buf2, token) {
    if (token.type.name === "break") {
      const recurs = this.inRecursive.pop();
      if (recurs) {
        if (recurs.type === Type.array) {
          buf2.push([93]);
        } else if (recurs.type === Type.map) {
          buf2.push([125]);
        } else {
          throw new Error("Unexpected recursive type; this should not happen!");
        }
        return;
      }
      throw new Error("Unexpected break; this should not happen!");
    }
    if (token.value === void 0) {
      throw new Error(`${encodeErrPrefix} unsupported type: undefined`);
    }
    this.prefix(buf2);
    if (token.type.name === "true") {
      buf2.push([116, 114, 117, 101]);
      return;
    } else if (token.type.name === "false") {
      buf2.push([102, 97, 108, 115, 101]);
      return;
    } else if (token.type.name === "null") {
      buf2.push([110, 117, 108, 108]);
      return;
    }
    const is2 = String(token.value);
    const isa = [];
    let dp = false;
    for (let i = 0; i < is2.length; i++) {
      isa[i] = is2.charCodeAt(i);
      if (!dp && (isa[i] === 46 || isa[i] === 101 || isa[i] === 69)) {
        dp = true;
      }
    }
    if (!dp) {
      isa.push(46);
      isa.push(48);
    }
    buf2.push(isa);
  }
};
function mapSorter2(e1, e2) {
  if (Array.isArray(e1[0]) || Array.isArray(e2[0])) {
    throw new Error(`${encodeErrPrefix} complex map keys are not supported`);
  }
  const keyToken1 = e1[0];
  const keyToken2 = e2[0];
  if (keyToken1.type !== Type.string || keyToken2.type !== Type.string) {
    throw new Error(`${encodeErrPrefix} non-string map keys are not supported`);
  }
  if (keyToken1 < keyToken2) {
    return -1;
  }
  if (keyToken1 > keyToken2) {
    return 1;
  }
  throw new Error(`${encodeErrPrefix} unexpected duplicate map keys, this is not supported`);
}
var defaultEncodeOptions2 = { addBreakTokens: true, mapSorter: mapSorter2 };
function encode10(data, options) {
  options = Object.assign({}, defaultEncodeOptions2, options);
  return encodeCustom(data, new JSONEncoder(), options);
}

// node_modules/cborg/lib/json/decode.js
var Tokenizer = class {
  /**
   * @param {Uint8Array} data
   * @param {DecodeOptions} options
   */
  constructor(data, options = {}) {
    this._pos = 0;
    this.data = data;
    this.options = options;
    this.modeStack = ["value"];
    this.lastToken = "";
  }
  pos() {
    return this._pos;
  }
  /**
   * @returns {boolean}
   */
  done() {
    return this._pos >= this.data.length;
  }
  /**
   * @returns {number}
   */
  ch() {
    return this.data[this._pos];
  }
  /**
   * @returns {string}
   */
  currentMode() {
    return this.modeStack[this.modeStack.length - 1];
  }
  skipWhitespace() {
    let c = this.ch();
    while (c === 32 || c === 9 || c === 13 || c === 10) {
      c = this.data[++this._pos];
    }
  }
  /**
   * @param {number[]} str
   */
  expect(str) {
    if (this.data.length - this._pos < str.length) {
      throw new Error(`${decodeErrPrefix} unexpected end of input at position ${this._pos}`);
    }
    for (let i = 0; i < str.length; i++) {
      if (this.data[this._pos++] !== str[i]) {
        throw new Error(`${decodeErrPrefix} unexpected token at position ${this._pos}, expected to find '${String.fromCharCode(...str)}'`);
      }
    }
  }
  parseNumber() {
    const startPos = this._pos;
    let negative = false;
    let float2 = false;
    const swallow = (chars) => {
      while (!this.done()) {
        const ch = this.ch();
        if (chars.includes(ch)) {
          this._pos++;
        } else {
          break;
        }
      }
    };
    if (this.ch() === 45) {
      negative = true;
      this._pos++;
    }
    if (this.ch() === 48) {
      this._pos++;
      if (this.ch() === 46) {
        this._pos++;
        float2 = true;
      } else {
        return new Token(Type.uint, 0, this._pos - startPos);
      }
    }
    swallow([48, 49, 50, 51, 52, 53, 54, 55, 56, 57]);
    if (negative && this._pos === startPos + 1) {
      throw new Error(`${decodeErrPrefix} unexpected token at position ${this._pos}`);
    }
    if (!this.done() && this.ch() === 46) {
      if (float2) {
        throw new Error(`${decodeErrPrefix} unexpected token at position ${this._pos}`);
      }
      float2 = true;
      this._pos++;
      swallow([48, 49, 50, 51, 52, 53, 54, 55, 56, 57]);
    }
    if (!this.done() && (this.ch() === 101 || this.ch() === 69)) {
      float2 = true;
      this._pos++;
      if (!this.done() && (this.ch() === 43 || this.ch() === 45)) {
        this._pos++;
      }
      swallow([48, 49, 50, 51, 52, 53, 54, 55, 56, 57]);
    }
    const numStr = String.fromCharCode.apply(null, this.data.subarray(startPos, this._pos));
    const num = parseFloat(numStr);
    if (float2) {
      return new Token(Type.float, num, this._pos - startPos);
    }
    if (this.options.allowBigInt !== true || Number.isSafeInteger(num)) {
      return new Token(num >= 0 ? Type.uint : Type.negint, num, this._pos - startPos);
    }
    return new Token(num >= 0 ? Type.uint : Type.negint, BigInt(numStr), this._pos - startPos);
  }
  /**
   * @returns {Token}
   */
  parseString() {
    if (this.ch() !== 34) {
      throw new Error(`${decodeErrPrefix} unexpected character at position ${this._pos}; this shouldn't happen`);
    }
    this._pos++;
    for (let i = this._pos, l = 0; i < this.data.length && l < 65536; i++, l++) {
      const ch = this.data[i];
      if (ch === 92 || ch < 32 || ch >= 128) {
        break;
      }
      if (ch === 34) {
        const str = String.fromCharCode.apply(null, this.data.subarray(this._pos, i));
        this._pos = i + 1;
        return new Token(Type.string, str, l);
      }
    }
    const startPos = this._pos;
    const chars = [];
    const readu4 = () => {
      if (this._pos + 4 >= this.data.length) {
        throw new Error(`${decodeErrPrefix} unexpected end of unicode escape sequence at position ${this._pos}`);
      }
      let u4 = 0;
      for (let i = 0; i < 4; i++) {
        let ch = this.ch();
        if (ch >= 48 && ch <= 57) {
          ch -= 48;
        } else if (ch >= 97 && ch <= 102) {
          ch = ch - 97 + 10;
        } else if (ch >= 65 && ch <= 70) {
          ch = ch - 65 + 10;
        } else {
          throw new Error(`${decodeErrPrefix} unexpected unicode escape character at position ${this._pos}`);
        }
        u4 = u4 * 16 + ch;
        this._pos++;
      }
      return u4;
    };
    const readUtf8Char = () => {
      const firstByte = this.ch();
      let codePoint = null;
      let bytesPerSequence = firstByte > 239 ? 4 : firstByte > 223 ? 3 : firstByte > 191 ? 2 : 1;
      if (this._pos + bytesPerSequence > this.data.length) {
        throw new Error(`${decodeErrPrefix} unexpected unicode sequence at position ${this._pos}`);
      }
      let secondByte, thirdByte, fourthByte, tempCodePoint;
      switch (bytesPerSequence) {
        case 1:
          if (firstByte < 128) {
            codePoint = firstByte;
          }
          break;
        case 2:
          secondByte = this.data[this._pos + 1];
          if ((secondByte & 192) === 128) {
            tempCodePoint = (firstByte & 31) << 6 | secondByte & 63;
            if (tempCodePoint > 127) {
              codePoint = tempCodePoint;
            }
          }
          break;
        case 3:
          secondByte = this.data[this._pos + 1];
          thirdByte = this.data[this._pos + 2];
          if ((secondByte & 192) === 128 && (thirdByte & 192) === 128) {
            tempCodePoint = (firstByte & 15) << 12 | (secondByte & 63) << 6 | thirdByte & 63;
            if (tempCodePoint > 2047 && (tempCodePoint < 55296 || tempCodePoint > 57343)) {
              codePoint = tempCodePoint;
            }
          }
          break;
        case 4:
          secondByte = this.data[this._pos + 1];
          thirdByte = this.data[this._pos + 2];
          fourthByte = this.data[this._pos + 3];
          if ((secondByte & 192) === 128 && (thirdByte & 192) === 128 && (fourthByte & 192) === 128) {
            tempCodePoint = (firstByte & 15) << 18 | (secondByte & 63) << 12 | (thirdByte & 63) << 6 | fourthByte & 63;
            if (tempCodePoint > 65535 && tempCodePoint < 1114112) {
              codePoint = tempCodePoint;
            }
          }
      }
      if (codePoint === null) {
        codePoint = 65533;
        bytesPerSequence = 1;
      } else if (codePoint > 65535) {
        codePoint -= 65536;
        chars.push(codePoint >>> 10 & 1023 | 55296);
        codePoint = 56320 | codePoint & 1023;
      }
      chars.push(codePoint);
      this._pos += bytesPerSequence;
    };
    while (!this.done()) {
      const ch = this.ch();
      let ch1;
      switch (ch) {
        case 92:
          this._pos++;
          if (this.done()) {
            throw new Error(`${decodeErrPrefix} unexpected string termination at position ${this._pos}`);
          }
          ch1 = this.ch();
          this._pos++;
          switch (ch1) {
            case 34:
            case 39:
            case 92:
            case 47:
              chars.push(ch1);
              break;
            case 98:
              chars.push(8);
              break;
            case 116:
              chars.push(9);
              break;
            case 110:
              chars.push(10);
              break;
            case 102:
              chars.push(12);
              break;
            case 114:
              chars.push(13);
              break;
            case 117:
              chars.push(readu4());
              break;
            default:
              throw new Error(`${decodeErrPrefix} unexpected string escape character at position ${this._pos}`);
          }
          break;
        case 34:
          this._pos++;
          return new Token(Type.string, decodeCodePointsArray(chars), this._pos - startPos);
        default:
          if (ch < 32) {
            throw new Error(`${decodeErrPrefix} invalid control character at position ${this._pos}`);
          } else if (ch < 128) {
            chars.push(ch);
            this._pos++;
          } else {
            readUtf8Char();
          }
      }
    }
    throw new Error(`${decodeErrPrefix} unexpected end of string at position ${this._pos}`);
  }
  /**
   * @returns {Token}
   */
  parseValue() {
    switch (this.ch()) {
      case 123:
        this.modeStack.push("obj-start");
        this._pos++;
        return new Token(Type.map, Infinity, 1);
      case 91:
        this.modeStack.push("array-start");
        this._pos++;
        return new Token(Type.array, Infinity, 1);
      case 34: {
        return this.parseString();
      }
      case 110:
        this.expect([110, 117, 108, 108]);
        return new Token(Type.null, null, 4);
      case 102:
        this.expect([102, 97, 108, 115, 101]);
        return new Token(Type.false, false, 5);
      case 116:
        this.expect([116, 114, 117, 101]);
        return new Token(Type.true, true, 4);
      case 45:
      case 48:
      case 49:
      case 50:
      case 51:
      case 52:
      case 53:
      case 54:
      case 55:
      case 56:
      case 57:
        return this.parseNumber();
      default:
        throw new Error(`${decodeErrPrefix} unexpected character at position ${this._pos}`);
    }
  }
  /**
   * @returns {Token}
   */
  next() {
    this.skipWhitespace();
    switch (this.currentMode()) {
      case "value":
        this.modeStack.pop();
        return this.parseValue();
      case "array-value": {
        this.modeStack.pop();
        if (this.ch() === 93) {
          this._pos++;
          this.skipWhitespace();
          return new Token(Type.break, void 0, 1);
        }
        if (this.ch() !== 44) {
          throw new Error(`${decodeErrPrefix} unexpected character at position ${this._pos}, was expecting array delimiter but found '${String.fromCharCode(this.ch())}'`);
        }
        this._pos++;
        this.modeStack.push("array-value");
        this.skipWhitespace();
        return this.parseValue();
      }
      case "array-start": {
        this.modeStack.pop();
        if (this.ch() === 93) {
          this._pos++;
          this.skipWhitespace();
          return new Token(Type.break, void 0, 1);
        }
        this.modeStack.push("array-value");
        this.skipWhitespace();
        return this.parseValue();
      }
      case "obj-key":
        if (this.ch() === 125) {
          this.modeStack.pop();
          this._pos++;
          this.skipWhitespace();
          return new Token(Type.break, void 0, 1);
        }
        if (this.ch() !== 44) {
          throw new Error(`${decodeErrPrefix} unexpected character at position ${this._pos}, was expecting object delimiter but found '${String.fromCharCode(this.ch())}'`);
        }
        this._pos++;
        this.skipWhitespace();
      case "obj-start": {
        this.modeStack.pop();
        if (this.ch() === 125) {
          this._pos++;
          this.skipWhitespace();
          return new Token(Type.break, void 0, 1);
        }
        const token = this.parseString();
        this.skipWhitespace();
        if (this.ch() !== 58) {
          throw new Error(`${decodeErrPrefix} unexpected character at position ${this._pos}, was expecting key/value delimiter ':' but found '${String.fromCharCode(this.ch())}'`);
        }
        this._pos++;
        this.modeStack.push("obj-value");
        return token;
      }
      case "obj-value": {
        this.modeStack.pop();
        this.modeStack.push("obj-key");
        this.skipWhitespace();
        return this.parseValue();
      }
      default:
        throw new Error(`${decodeErrPrefix} unexpected parse state at position ${this._pos}; this shouldn't happen`);
    }
  }
};
function decode11(data, options) {
  options = Object.assign({ tokenizer: new Tokenizer(data, options) }, options);
  return decode(data, options);
}

// node_modules/@ipld/dag-json/src/index.js
function toByteView2(buf2) {
  if (buf2 instanceof ArrayBuffer) {
    return new Uint8Array(buf2, 0, buf2.byteLength);
  }
  return buf2;
}
function cidEncoder2(obj) {
  if (obj.asCID !== obj && obj["/"] !== obj.bytes) {
    return null;
  }
  const cid = CID.asCID(obj);
  if (!cid) {
    return null;
  }
  const cidString = cid.toString();
  return [
    new Token(Type.map, Infinity, 1),
    new Token(Type.string, "/", 1),
    // key
    new Token(Type.string, cidString, cidString.length),
    // value
    new Token(Type.break, void 0, 1)
  ];
}
function bytesEncoder(bytes3) {
  const bytesString = base64.encode(bytes3).slice(1);
  return [
    new Token(Type.map, Infinity, 1),
    new Token(Type.string, "/", 1),
    // key
    new Token(Type.map, Infinity, 1),
    // value
    new Token(Type.string, "bytes", 5),
    // inner key
    new Token(Type.string, bytesString, bytesString.length),
    // inner value
    new Token(Type.break, void 0, 1),
    new Token(Type.break, void 0, 1)
  ];
}
function taBytesEncoder(obj) {
  return bytesEncoder(new Uint8Array(obj.buffer, obj.byteOffset, obj.byteLength));
}
function abBytesEncoder(ab) {
  return bytesEncoder(new Uint8Array(ab));
}
function undefinedEncoder2() {
  throw new Error("`undefined` is not supported by the IPLD Data Model and cannot be encoded");
}
function numberEncoder2(num) {
  if (Number.isNaN(num)) {
    throw new Error("`NaN` is not supported by the IPLD Data Model and cannot be encoded");
  }
  if (num === Infinity || num === -Infinity) {
    throw new Error("`Infinity` and `-Infinity` is not supported by the IPLD Data Model and cannot be encoded");
  }
  return null;
}
var encodeOptions2 = {
  typeEncoders: {
    Object: cidEncoder2,
    Buffer: bytesEncoder,
    Uint8Array: bytesEncoder,
    Int8Array: taBytesEncoder,
    Uint16Array: taBytesEncoder,
    Int16Array: taBytesEncoder,
    Uint32Array: taBytesEncoder,
    Int32Array: taBytesEncoder,
    Float32Array: taBytesEncoder,
    Float64Array: taBytesEncoder,
    Uint8ClampedArray: taBytesEncoder,
    BigInt64Array: taBytesEncoder,
    BigUint64Array: taBytesEncoder,
    DataView: taBytesEncoder,
    ArrayBuffer: abBytesEncoder,
    undefined: undefinedEncoder2,
    number: numberEncoder2
  }
};
var DagJsonTokenizer = class extends Tokenizer {
  /**
   * @param {Uint8Array} data
   * @param {object} [options]
   */
  constructor(data, options) {
    super(data, options);
    this.tokenBuffer = [];
  }
  /**
   * @returns {boolean}
   */
  done() {
    return this.tokenBuffer.length === 0 && super.done();
  }
  /**
   * @returns {Token}
   */
  _next() {
    if (this.tokenBuffer.length > 0) {
      return this.tokenBuffer.pop();
    }
    return super.next();
  }
  /**
   * Implements rules outlined in https://github.com/ipld/specs/pull/356
   *
   * @returns {Token}
   */
  next() {
    const token = this._next();
    if (token.type === Type.map) {
      const keyToken = this._next();
      if (keyToken.type === Type.string && keyToken.value === "/") {
        const valueToken = this._next();
        if (valueToken.type === Type.string) {
          const breakToken = this._next();
          if (breakToken.type !== Type.break) {
            throw new Error("Invalid encoded CID form");
          }
          this.tokenBuffer.push(valueToken);
          return new Token(Type.tag, 42, 0);
        }
        if (valueToken.type === Type.map) {
          const innerKeyToken = this._next();
          if (innerKeyToken.type === Type.string && innerKeyToken.value === "bytes") {
            const innerValueToken = this._next();
            if (innerValueToken.type === Type.string) {
              for (let i = 0; i < 2; i++) {
                const breakToken = this._next();
                if (breakToken.type !== Type.break) {
                  throw new Error("Invalid encoded Bytes form");
                }
              }
              const bytes3 = base64.decode(`m${innerValueToken.value}`);
              return new Token(Type.bytes, bytes3, innerValueToken.value.length);
            }
            this.tokenBuffer.push(innerValueToken);
          }
          this.tokenBuffer.push(innerKeyToken);
        }
        this.tokenBuffer.push(valueToken);
      }
      this.tokenBuffer.push(keyToken);
    }
    return token;
  }
};
var decodeOptions2 = {
  allowIndefinite: false,
  allowUndefined: false,
  allowNaN: false,
  allowInfinity: false,
  allowBigInt: true,
  // this will lead to BigInt for ints outside of
  // safe-integer range, which may surprise users
  strict: true,
  useMaps: false,
  rejectDuplicateMapKeys: true,
  /** @type {import('cborg').TagDecoder[]} */
  tags: []
};
decodeOptions2.tags[42] = CID.parse;
var encode11 = (node) => encode10(node, encodeOptions2);
var decode12 = (data) => {
  const buf2 = toByteView2(data);
  const options = Object.assign(decodeOptions2, { tokenizer: new DagJsonTokenizer(buf2, decodeOptions2) });
  return decode11(buf2, options);
};
var utf8Decoder = new TextDecoder();
var utf8Encoder = new TextEncoder();

// node_modules/@ipld/dag-ucan/src/formatter.js
var format4 = (model) => {
  const header = formatHeader(model.v, model.s.algorithm);
  const payload = formatPayload(model);
  const signature = formatSignature(model.s);
  return (
    /** @type {UCAN.JWT<C>} */
    `${header}.${payload}.${signature}`
  );
};
var formatSignPayload = (payload, version2, alg) => `${formatHeader(version2, alg)}.${formatPayload(payload)}`;
var formatHeader = (version2, alg) => base64url.baseEncode(encodeHeader(version2, alg));
var formatPayload = (data) => base64url.baseEncode(encodePayload(data));
var formatSignature = (signature) => base64url.baseEncode(signature.raw);
var encodeHeader = (v, alg) => encode11({
  alg,
  ucv: v,
  typ: "JWT"
});
var encodePayload = (data) => encode11({
  iss: format2(data.iss),
  aud: format2(data.aud),
  att: data.att,
  exp: data.exp,
  prf: data.prf.map(encodeProof),
  // leave out optionals and empty fields
  ...data.fct.length > 0 && { fct: data.fct },
  ...data.nnc && { nnc: data.nnc },
  ...data.nbf && { nbf: data.nbf }
});
var encodeProof = (proof) => (
  /** @type {UCAN.ToString<UCAN.Link>} */
  proof.toString()
);

// node_modules/@ipld/dag-ucan/src/view.js
var toJSON3 = (data) => JSON.parse(decode7(encode11(data)));
var View = class {
  /**
   * @param {UCAN.UCAN<C>} model
   */
  constructor(model) {
    this.model = model;
  }
  get version() {
    return this.model.v;
  }
  get issuer() {
    return from3(this.model.iss);
  }
  get audience() {
    return from3(this.model.aud);
  }
  /**
   * @returns {C}
   */
  get capabilities() {
    return this.model.att;
  }
  /**
   * @returns {number}
   */
  get expiration() {
    const { exp } = this.model;
    return exp === null ? Infinity : exp;
  }
  /**
   * @returns {undefined|number}
   */
  get notBefore() {
    return this.model.nbf;
  }
  /**
   * @returns {undefined|string}
   */
  get nonce() {
    return this.model.nnc;
  }
  /**
   * @returns {UCAN.Fact[]}
   */
  get facts() {
    return this.model.fct;
  }
  /**
   * @returns {UCAN.Link[]}
   */
  get proofs() {
    return this.model.prf;
  }
  get signature() {
    return this.model.s;
  }
  // compatibility with UCAN.UCAN
  get jwt() {
    return this.model.jwt;
  }
  get s() {
    return this.model.s;
  }
  get v() {
    return this.model.v;
  }
  get iss() {
    return this.model.iss;
  }
  get aud() {
    return this.model.aud;
  }
  get att() {
    return this.model.att;
  }
  get exp() {
    return this.model.exp;
  }
  get nbf() {
    return this.model.nbf;
  }
  get nnc() {
    return this.model.nnc;
  }
  get fct() {
    return this.model.fct;
  }
  get prf() {
    return this.model.prf;
  }
  /**
   * @returns {UCAN.ToJSON<UCAN.UCAN<C>, UCAN.UCANJSON<this>>}
   */
  toJSON() {
    const { v, iss, aud, s, att, prf, exp, fct, nnc, nbf } = this.model;
    return {
      iss,
      aud,
      v,
      s,
      exp,
      ...toJSON3({
        att,
        prf,
        ...fct.length > 0 && { fct }
      }),
      ...nnc != null && { nnc },
      ...nbf && { nbf }
    };
  }
};

// node_modules/@ipld/dag-ucan/src/codec/cbor.js
var code4 = code;
var from4 = (model) => new CBORView(model);
var encode12 = (model) => {
  const { fct, nnc, nbf, ...payload } = readPayload(model);
  return (
    /** @type {Uint8Array} */
    encode4({
      // leave out optionals unless they are set
      ...fct.length > 0 && { fct },
      ...nnc != null && { nnc },
      ...nbf && { nbf },
      ...payload,
      // add version and signature
      v: readVersion(model.v, "v"),
      s: encodeSignature(model.s, "s")
    })
  );
};
var encodeSignature = (signature, context2) => {
  try {
    return encode9(signature);
  } catch (cause) {
    throw new Error(
      `Expected signature ${context2}, instead got ${JSON.stringify(signature)}`,
      // @ts-expect-error - types don't know about second arg
      { cause }
    );
  }
};
var decode13 = (bytes3) => {
  const model = decode6(bytes3);
  return new CBORView({
    ...readPayload(model),
    v: readVersion(model.v, "v"),
    s: readSignature(model.s)
  });
};
var CBORView = class extends View {
  /** @type {UCAN.MulticodecCode<typeof code, "CBOR">} */
  get code() {
    return code4;
  }
  format() {
    return format4(this.model);
  }
  encode() {
    return encode12(this.model);
  }
};

// node_modules/@ipld/dag-ucan/src/parser.js
var parse4 = (jwt) => {
  const segments = jwt.split(".");
  const [header, payload, signature] = segments.length === 3 ? segments : fail(
    `Can't parse UCAN: ${jwt}: Expected JWT format: 3 dot-separated base64url-encoded values.`
  );
  const { ucv, alg } = parseHeader(header);
  return {
    ...parsePayload(payload),
    v: ucv,
    s: createNamed(alg, base64url.baseDecode(signature))
  };
};
var parseHeader = (header) => {
  const { ucv, alg, typ } = decode12(base64url.baseDecode(header));
  return {
    typ: readLiteral(typ, "JWT", "typ"),
    ucv: readVersion(ucv, "ucv"),
    alg: readString(alg, "alg")
  };
};
var parsePayload = (source) => {
  const payload = decode12(base64url.baseDecode(source));
  return readJWTPayload(payload);
};

// node_modules/@ipld/dag-ucan/src/codec/jwt.js
var from5 = (model) => new JWTView(model);
var decode14 = (bytes3) => {
  const jwt = (
    /** @type {UCAN.JWT<C>} */
    decode7(bytes3)
  );
  return new JWTView({ ...parse4(jwt), jwt });
};
var encode13 = ({ jwt }) => encode5(jwt);
var format5 = ({ jwt }) => jwt;
var JWTView = class extends View {
  /**
   * @param {UCAN.FromJWT<C>} model
   */
  constructor(model) {
    super(model);
    this.model = model;
  }
  /** @type {UCAN.MulticodecCode<typeof code, "Raw">} */
  get code() {
    return code3;
  }
  format() {
    return format5(this.model);
  }
  encode() {
    return encode13(this.model);
  }
};

// node_modules/multiformats/dist/src/hashes/sha2-browser.js
var sha2_browser_exports = {};
__export(sha2_browser_exports, {
  sha256: () => sha2562,
  sha512: () => sha512
});
function sha(name15) {
  return async (data) => new Uint8Array(await crypto.subtle.digest(name15, data));
}
var sha2562 = from2({
  name: "sha2-256",
  code: 18,
  encode: sha("SHA-256")
});
var sha512 = from2({
  name: "sha2-512",
  code: 19,
  encode: sha("SHA-512")
});

// node_modules/@ipld/dag-ucan/src/lib.js
var VERSION = "0.9.1";
var name4 = "dag-ucan";
var code5 = code4;
var defaultHasher = sha2562;
var encode14 = (ucan2) => ucan2.jwt ? encode13(ucan2) : encode12(ucan2);
var decode15 = (bytes3) => {
  try {
    return decode13(bytes3);
  } catch (_) {
    return decode14(
      /** @type {UCAN.ByteView<UCAN.FromJWT<C>>} */
      bytes3
    );
  }
};
var link = async (ucan2, options) => {
  const { cid } = await write(ucan2, options);
  return cid;
};
var write = async (ucan2, { hasher = defaultHasher } = {}) => {
  const [code19, bytes3] = ucan2.jwt ? [code3, encode13(ucan2)] : [code4, encode12(ucan2)];
  const digest4 = await hasher.digest(bytes3);
  return {
    bytes: bytes3,
    cid: create2(code19, digest4),
    data: ucan2
  };
};
var parse5 = (jwt) => {
  const model = parse4(jwt);
  return format4(model) === jwt ? from4(model) : from5({ ...model, jwt: (
    /** @type {UCAN.JWT<C>} */
    jwt
  ) });
};
var format6 = (ucan2) => ucan2.jwt ? format5(ucan2) : format4(ucan2);
var issue = async ({
  issuer,
  audience,
  capabilities,
  lifetimeInSeconds = 30,
  expiration = now() + lifetimeInSeconds,
  notBefore,
  facts = [],
  proofs: proofs2 = [],
  nonce
}) => {
  const v = VERSION;
  const data = readPayload({
    iss: parse2(issuer.did()),
    aud: parse2(audience.did()),
    att: capabilities,
    fct: facts,
    exp: expiration,
    nbf: notBefore,
    prf: proofs2,
    nnc: nonce
  });
  const payload = encodeSignaturePayload(data, v, issuer.signatureAlgorithm);
  return from4({
    ...data,
    v,
    s: await issuer.sign(payload)
  });
};
var encodeSignaturePayload = (payload, version2, algorithm2) => encode5(formatSignPayload(payload, version2, algorithm2));
var verifySignature = (ucan2, verifier) => format2(ucan2.issuer) === verifier.did() && verifier.verify(
  encodeSignaturePayload(ucan2.model, ucan2.model.v, ucan2.signature.algorithm),
  ucan2.signature
);
var isExpired = (ucan2) => ucan2.expiration <= now();
var isTooEarly = (ucan2) => ucan2.notBefore != null && now() <= ucan2.notBefore;
var now = () => Math.floor(Date.now() / 1e3);

// node_modules/@ucanto/core/src/delegation.js
var API10 = __toESM(require_lib());

// node_modules/@ucanto/core/src/dag.js
var dag_exports = {};
__export(dag_exports, {
  CBOR: () => cbor_exports2,
  addEveryInto: () => addEveryInto,
  addInto: () => addInto,
  createStore: () => createStore,
  embed: () => embed,
  get: () => get,
  identity: () => identity,
  iterate: () => iterate,
  notFound: () => notFound,
  sha256: () => sha2562,
  writeInto: () => writeInto
});
var API2 = __toESM(require_lib(), 1);

// node_modules/@ucanto/core/src/cbor.js
var cbor_exports2 = {};
__export(cbor_exports2, {
  code: () => code,
  contentType: () => contentType,
  decode: () => decode6,
  encode: () => encode15,
  link: () => link2,
  name: () => name,
  write: () => write2
});
var API = __toESM(require_lib(), 1);
var contentType = "application/vnd.ipld.dag-cbor";
var prepare = (data, seen) => {
  if (seen.has(data)) {
    throw new TypeError("Can not encode circular structure");
  }
  if (data === void 0 && seen.size === 0) {
    return null;
  }
  if (data === null) {
    return null;
  }
  if (typeof data === "symbol" && seen.size === 0) {
    return null;
  }
  if (isLink(data)) {
    return data;
  }
  if (ArrayBuffer.isView(data)) {
    return data;
  }
  if (Array.isArray(data)) {
    seen.add(data);
    const items = [];
    for (const item of data) {
      items.push(
        item === void 0 || typeof item === "symbol" ? null : prepare(item, seen)
      );
    }
    return items;
  }
  if (typeof /** @type {{toJSON?:unknown}} */
  data.toJSON === "function") {
    seen.add(data);
    const json = (
      /** @type {{toJSON():unknown}} */
      data.toJSON()
    );
    return prepare(json, seen);
  }
  if (typeof data === "object") {
    seen.add(data);
    const object = {};
    for (const [key, value] of Object.entries(data)) {
      if (value !== void 0 && typeof value !== "symbol") {
        object[key] = prepare(value, new Set(seen));
      }
    }
    return object;
  }
  return data;
};
var encode15 = (data) => (
  /** @type {CBOR.ByteView<T>} */
  encode4(prepare(data, /* @__PURE__ */ new Set()))
);
var link2 = async (bytes3, { hasher = sha2562 } = {}) => {
  return (
    /** @type {API.Link<T, typeof CBOR.code>} */
    create2(code, await hasher.digest(bytes3))
  );
};
var write2 = async (data, options) => {
  const bytes3 = encode15(data);
  const cid = await link2(bytes3, options);
  return { cid, bytes: bytes3 };
};

// node_modules/@ucanto/core/src/dag.js
var iterate = function* (value) {
  if (value && typeof value === "object" && "iterateIPLDBlocks" in value && typeof value.iterateIPLDBlocks === "function") {
    yield* value.iterateIPLDBlocks();
  }
};
var createStore = (blocks = []) => {
  const store3 = /* @__PURE__ */ new Map();
  addEveryInto(blocks, store3);
  return store3;
};
var EMBED_CODE = identity.code;
var get = (cid, store3, fallback) => {
  if (cid.multihash.code === EMBED_CODE) {
    return { cid, bytes: cid.multihash.digest };
  }
  const block = (
    /** @type {API.Block<U, Format, Alg, V>|undefined} */
    store3.get(`${cid}`)
  );
  return block ? block : fallback === void 0 ? notFound(cid) : fallback;
};
var embed = (source, { codec } = {}) => {
  const encoder3 = (
    /** @type {MF.BlockEncoder<C, U>}  */
    codec || cbor_exports2
  );
  const bytes3 = encoder3.encode(source);
  const digest4 = identity.digest(bytes3);
  return {
    cid: create2(encoder3.code, digest4),
    bytes: bytes3,
    data: source
  };
};
var notFound = (link6) => {
  throw new Error(`Block for the ${link6} is not found`);
};
var writeInto = async (source, store3, options = {}) => {
  const codec = (
    /** @type {MF.BlockEncoder<C, U>} */
    options.codec || cbor_exports2
  );
  const hasher = (
    /** @type {MF.MultihashHasher<A>} */
    options.hasher || sha2562
  );
  const bytes3 = codec.encode(source);
  const digest4 = await hasher.digest(bytes3);
  const link6 = create2(codec.code, digest4);
  store3.set(
    /** @type {API.ToString<typeof link>} */
    link6.toString(),
    {
      bytes: bytes3,
      cid: link6
    }
  );
  return { bytes: bytes3, cid: link6, data: source };
};
var addInto = ({ cid, bytes: bytes3 }, store3) => {
  store3.set(
    /** @type {API.ToString<typeof cid>} */
    cid.toString(),
    {
      bytes: bytes3,
      cid
    }
  );
  return { bytes: bytes3, cid };
};
var addEveryInto = (source, store3) => {
  for (const block of source) {
    addInto(block, store3);
  }
};

// node_modules/@ucanto/core/src/car.js
var car_exports = {};
__export(car_exports, {
  code: () => code6,
  contentType: () => contentType2,
  createWriter: () => createWriter2,
  decode: () => decode16,
  encode: () => encode16,
  link: () => link3,
  name: () => name5,
  write: () => write3
});
var API3 = __toESM(require_lib(), 1);

// node_modules/@ipld/car/src/decoder-common.js
var import_varint2 = __toESM(require_varint(), 1);
var CIDV0_BYTES = {
  SHA2_256: 18,
  LENGTH: 32,
  DAG_PB: 112
};
var V2_HEADER_LENGTH = (
  /* characteristics */
  16 + 8 + 8 + 8
);
function decodeVarint(bytes3, seeker) {
  if (!bytes3.length) {
    throw new Error("Unexpected end of data");
  }
  const i = import_varint2.default.decode(bytes3);
  seeker.seek(
    /** @type {number} */
    import_varint2.default.decode.bytes
  );
  return i;
}
function decodeV2Header(bytes3) {
  const dv = new DataView(bytes3.buffer, bytes3.byteOffset, bytes3.byteLength);
  let offset2 = 0;
  const header = {
    version: 2,
    /** @type {[bigint, bigint]} */
    characteristics: [
      dv.getBigUint64(offset2, true),
      dv.getBigUint64(offset2 += 8, true)
    ],
    dataOffset: Number(dv.getBigUint64(offset2 += 8, true)),
    dataSize: Number(dv.getBigUint64(offset2 += 8, true)),
    indexOffset: Number(dv.getBigUint64(offset2 += 8, true))
  };
  return header;
}
function getMultihashLength(bytes3) {
  import_varint2.default.decode(bytes3);
  const codeLength = (
    /** @type {number} */
    import_varint2.default.decode.bytes
  );
  const length2 = import_varint2.default.decode(bytes3.subarray(import_varint2.default.decode.bytes));
  const lengthLength = (
    /** @type {number} */
    import_varint2.default.decode.bytes
  );
  const mhLength = codeLength + lengthLength + length2;
  return mhLength;
}

// node_modules/@ipld/car/src/header-validator.js
var Kinds = {
  Null: (
    /**
     * @param obj
     * @returns {undefined|null}
     */
    (obj) => obj === null ? obj : void 0
  ),
  Int: (
    /**
     * @param obj
     * @returns {undefined|number}
     */
    (obj) => Number.isInteger(obj) ? obj : void 0
  ),
  Float: (
    /**
     * @param obj
     * @returns {undefined|number}
     */
    (obj) => typeof obj === "number" && Number.isFinite(obj) ? obj : void 0
  ),
  String: (
    /**
     * @param obj
     * @returns {undefined|string}
     */
    (obj) => typeof obj === "string" ? obj : void 0
  ),
  Bool: (
    /**
     * @param obj
     * @returns {undefined|boolean}
     */
    (obj) => typeof obj === "boolean" ? obj : void 0
  ),
  Bytes: (
    /**
     * @param obj
     * @returns {undefined|Uint8Array}
     */
    (obj) => obj instanceof Uint8Array ? obj : void 0
  ),
  Link: (
    /**
     * @param obj
     * @returns {undefined|object}
     */
    (obj) => obj !== null && typeof obj === "object" && obj.asCID === obj ? obj : void 0
  ),
  List: (
    /**
     * @param obj
     * @returns {undefined|Array<any>}
     */
    (obj) => Array.isArray(obj) ? obj : void 0
  ),
  Map: (
    /**
     * @param obj
     * @returns {undefined|object}
     */
    (obj) => obj !== null && typeof obj === "object" && obj.asCID !== obj && !Array.isArray(obj) && !(obj instanceof Uint8Array) ? obj : void 0
  )
};
var Types = {
  "CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)": Kinds.Link,
  "CarV1HeaderOrV2Pragma > roots (anon)": (
    /**
     * @param obj
     * @returns {undefined|any}
     */
    (obj) => {
      if (Kinds.List(obj) === void 0) {
        return void 0;
      }
      for (let i = 0; i < obj.length; i++) {
        let v = obj[i];
        v = Types["CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)"](v);
        if (v === void 0) {
          return void 0;
        }
        if (v !== obj[i]) {
          const ret = obj.slice(0, i);
          for (let j = i; j < obj.length; j++) {
            let v2 = obj[j];
            v2 = Types["CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)"](v2);
            if (v2 === void 0) {
              return void 0;
            }
            ret.push(v2);
          }
          return ret;
        }
      }
      return obj;
    }
  ),
  Int: Kinds.Int,
  CarV1HeaderOrV2Pragma: (
    /**
     * @param obj
     * @returns {undefined|any}
     */
    (obj) => {
      if (Kinds.Map(obj) === void 0) {
        return void 0;
      }
      const entries3 = Object.entries(obj);
      let ret = obj;
      let requiredCount = 1;
      for (let i = 0; i < entries3.length; i++) {
        const [key, value] = entries3[i];
        switch (key) {
          case "roots":
            {
              const v = Types["CarV1HeaderOrV2Pragma > roots (anon)"](obj[key]);
              if (v === void 0) {
                return void 0;
              }
              if (v !== value || ret !== obj) {
                if (ret === obj) {
                  ret = {};
                  for (let j = 0; j < i; j++) {
                    ret[entries3[j][0]] = entries3[j][1];
                  }
                }
                ret.roots = v;
              }
            }
            break;
          case "version":
            {
              requiredCount--;
              const v = Types.Int(obj[key]);
              if (v === void 0) {
                return void 0;
              }
              if (v !== value || ret !== obj) {
                if (ret === obj) {
                  ret = {};
                  for (let j = 0; j < i; j++) {
                    ret[entries3[j][0]] = entries3[j][1];
                  }
                }
                ret.version = v;
              }
            }
            break;
          default:
            return void 0;
        }
      }
      if (requiredCount > 0) {
        return void 0;
      }
      return ret;
    }
  )
};
var Reprs = {
  "CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)": Kinds.Link,
  "CarV1HeaderOrV2Pragma > roots (anon)": (
    /**
     * @param obj
     * @returns {undefined|any}
     */
    (obj) => {
      if (Kinds.List(obj) === void 0) {
        return void 0;
      }
      for (let i = 0; i < obj.length; i++) {
        let v = obj[i];
        v = Reprs["CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)"](v);
        if (v === void 0) {
          return void 0;
        }
        if (v !== obj[i]) {
          const ret = obj.slice(0, i);
          for (let j = i; j < obj.length; j++) {
            let v2 = obj[j];
            v2 = Reprs["CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)"](v2);
            if (v2 === void 0) {
              return void 0;
            }
            ret.push(v2);
          }
          return ret;
        }
      }
      return obj;
    }
  ),
  Int: Kinds.Int,
  CarV1HeaderOrV2Pragma: (
    /**
     * @param obj
     * @returns {undefined|any}
     */
    (obj) => {
      if (Kinds.Map(obj) === void 0) {
        return void 0;
      }
      const entries3 = Object.entries(obj);
      let ret = obj;
      let requiredCount = 1;
      for (let i = 0; i < entries3.length; i++) {
        const [key, value] = entries3[i];
        switch (key) {
          case "roots":
            {
              const v = Reprs["CarV1HeaderOrV2Pragma > roots (anon)"](value);
              if (v === void 0) {
                return void 0;
              }
              if (v !== value || ret !== obj) {
                if (ret === obj) {
                  ret = {};
                  for (let j = 0; j < i; j++) {
                    ret[entries3[j][0]] = entries3[j][1];
                  }
                }
                ret.roots = v;
              }
            }
            break;
          case "version":
            {
              requiredCount--;
              const v = Reprs.Int(value);
              if (v === void 0) {
                return void 0;
              }
              if (v !== value || ret !== obj) {
                if (ret === obj) {
                  ret = {};
                  for (let j = 0; j < i; j++) {
                    ret[entries3[j][0]] = entries3[j][1];
                  }
                }
                ret.version = v;
              }
            }
            break;
          default:
            return void 0;
        }
      }
      if (requiredCount > 0) {
        return void 0;
      }
      return ret;
    }
  )
};
var CarV1HeaderOrV2Pragma = {
  toTyped: Types.CarV1HeaderOrV2Pragma,
  toRepresentation: Reprs.CarV1HeaderOrV2Pragma
};

// node_modules/@ipld/car/src/buffer-decoder.js
function readHeader(reader, strictVersion) {
  const length2 = decodeVarint(reader.upTo(8), reader);
  if (length2 === 0) {
    throw new Error("Invalid CAR header (zero length)");
  }
  const header = reader.exactly(length2, true);
  const block = decode6(header);
  if (CarV1HeaderOrV2Pragma.toTyped(block) === void 0) {
    throw new Error("Invalid CAR header format");
  }
  if (block.version !== 1 && block.version !== 2 || strictVersion !== void 0 && block.version !== strictVersion) {
    throw new Error(`Invalid CAR version: ${block.version}${strictVersion !== void 0 ? ` (expected ${strictVersion})` : ""}`);
  }
  if (block.version === 1) {
    if (!Array.isArray(block.roots)) {
      throw new Error("Invalid CAR header format");
    }
    return block;
  }
  if (block.roots !== void 0) {
    throw new Error("Invalid CAR header format");
  }
  const v2Header = decodeV2Header(reader.exactly(V2_HEADER_LENGTH, true));
  reader.seek(v2Header.dataOffset - reader.pos);
  const v1Header = readHeader(reader, 1);
  return Object.assign(v1Header, v2Header);
}
function readCid(reader) {
  const first = reader.exactly(2, false);
  if (first[0] === CIDV0_BYTES.SHA2_256 && first[1] === CIDV0_BYTES.LENGTH) {
    const bytes4 = reader.exactly(34, true);
    const multihash2 = decode5(bytes4);
    return CID.create(0, CIDV0_BYTES.DAG_PB, multihash2);
  }
  const version2 = decodeVarint(reader.upTo(8), reader);
  if (version2 !== 1) {
    throw new Error(`Unexpected CID version (${version2})`);
  }
  const codec = decodeVarint(reader.upTo(8), reader);
  const bytes3 = reader.exactly(getMultihashLength(reader.upTo(8)), true);
  const multihash = decode5(bytes3);
  return CID.create(version2, codec, multihash);
}
function readBlockHead(reader) {
  const start = reader.pos;
  let length2 = decodeVarint(reader.upTo(8), reader);
  if (length2 === 0) {
    throw new Error("Invalid CAR section (zero length)");
  }
  length2 += reader.pos - start;
  const cid = readCid(reader);
  const blockLength2 = length2 - Number(reader.pos - start);
  return { cid, length: length2, blockLength: blockLength2 };
}
function fromBytes(bytes3) {
  let reader = bytesReader(bytes3);
  const header = readHeader(reader);
  if (header.version === 2) {
    const v1length = reader.pos - header.dataOffset;
    reader = limitReader(reader, header.dataSize - v1length);
  }
  const blocks = [];
  while (reader.upTo(8).length > 0) {
    const { cid, blockLength: blockLength2 } = readBlockHead(reader);
    blocks.push({ cid, bytes: reader.exactly(blockLength2, true) });
  }
  return {
    header,
    blocks
  };
}
function bytesReader(bytes3) {
  let pos = 0;
  return {
    upTo(length2) {
      return bytes3.subarray(pos, pos + Math.min(length2, bytes3.length - pos));
    },
    exactly(length2, seek = false) {
      if (length2 > bytes3.length - pos) {
        throw new Error("Unexpected end of data");
      }
      const out = bytes3.subarray(pos, pos + length2);
      if (seek) {
        pos += length2;
      }
      return out;
    },
    seek(length2) {
      pos += length2;
    },
    get pos() {
      return pos;
    }
  };
}
function limitReader(reader, byteLimit) {
  let bytesRead = 0;
  return {
    upTo(length2) {
      let bytes3 = reader.upTo(length2);
      if (bytes3.length + bytesRead > byteLimit) {
        bytes3 = bytes3.subarray(0, byteLimit - bytesRead);
      }
      return bytes3;
    },
    exactly(length2, seek = false) {
      const bytes3 = reader.exactly(length2, seek);
      if (bytes3.length + bytesRead > byteLimit) {
        throw new Error("Unexpected end of data");
      }
      if (seek) {
        bytesRead += length2;
      }
      return bytes3;
    },
    seek(length2) {
      bytesRead += length2;
      reader.seek(length2);
    },
    get pos() {
      return reader.pos;
    }
  };
}

// node_modules/@ipld/car/src/buffer-reader-browser.js
var CarBufferReader = class _CarBufferReader {
  /**
   * @constructs CarBufferReader
   * @param {CarHeader|CarV2Header} header
   * @param {Block[]} blocks
   */
  constructor(header, blocks) {
    this._header = header;
    this._blocks = blocks;
    this._cids = void 0;
  }
  /**
   * @property {number} version of the CAR
   * @memberof CarBufferReader
   * @instance
   */
  get version() {
    return this._header.version;
  }
  /**
   * Get the list of roots defined by the CAR referenced by this reader. May be
   * zero or more `CID`s.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @returns {CID[]}
   */
  getRoots() {
    return this._header.roots;
  }
  /**
   * Check whether a given `CID` exists within the CAR referenced by this
   * reader.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @param {CID} key
   * @returns {boolean}
   */
  has(key) {
    return this._blocks.some((b) => b.cid.equals(key));
  }
  /**
   * Fetch a `Block` (a `{ cid:CID, bytes:Uint8Array }` pair) from the CAR
   * referenced by this reader matching the provided `CID`. In the case where
   * the provided `CID` doesn't exist within the CAR, `undefined` will be
   * returned.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @param {CID} key
   * @returns {Block | undefined}
   */
  get(key) {
    return this._blocks.find((b) => b.cid.equals(key));
  }
  /**
   * Returns a `Block[]` of the `Block`s (`{ cid:CID, bytes:Uint8Array }` pairs) contained within
   * the CAR referenced by this reader.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @returns {Block[]}
   */
  blocks() {
    return this._blocks;
  }
  /**
   * Returns a `CID[]` of the `CID`s contained within the CAR referenced by this reader.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @returns {CID[]}
   */
  cids() {
    if (!this._cids) {
      this._cids = this._blocks.map((b) => b.cid);
    }
    return this._cids;
  }
  /**
   * Instantiate a {@link CarBufferReader} from a `Uint8Array` blob. This performs a
   * decode fully in memory and maintains the decoded state in memory for full
   * access to the data via the `CarReader` API.
   *
   * @static
   * @memberof CarBufferReader
   * @param {Uint8Array} bytes
   * @returns {CarBufferReader}
   */
  static fromBytes(bytes3) {
    if (!(bytes3 instanceof Uint8Array)) {
      throw new TypeError("fromBytes() requires a Uint8Array");
    }
    const { header, blocks } = fromBytes(bytes3);
    return new _CarBufferReader(header, blocks);
  }
};

// node_modules/cborg/lib/length.js
var cborEncoders2 = makeCborEncoders();
var defaultEncodeOptions3 = {
  float64: false,
  quickEncodeToken
};
function tokensToLength(tokens, encoders = cborEncoders2, options = defaultEncodeOptions3) {
  if (Array.isArray(tokens)) {
    let len = 0;
    for (const token of tokens) {
      len += tokensToLength(token, encoders, options);
    }
    return len;
  } else {
    const encoder3 = encoders[tokens.type.major];
    if (encoder3.encodedSize === void 0 || typeof encoder3.encodedSize !== "function") {
      throw new Error(`Encoder for ${tokens.type.name} does not have an encodedSize()`);
    }
    return encoder3.encodedSize(tokens, options);
  }
}

// node_modules/@ipld/car/src/buffer-writer.js
var import_varint3 = __toESM(require_varint());
var CarBufferWriter = class {
  /**
   * @param {Uint8Array} bytes
   * @param {number} headerSize
   */
  constructor(bytes3, headerSize) {
    this.bytes = bytes3;
    this.byteOffset = headerSize;
    this.roots = [];
    this.headerSize = headerSize;
  }
  /**
   * Add a root to this writer, to be used to create a header when the CAR is
   * finalized with {@link CarBufferWriter.close `close()`}
   *
   * @param {CID} root
   * @param {{resize?:boolean}} [options]
   * @returns {CarBufferWriter}
   */
  addRoot(root2, options) {
    addRoot(this, root2, options);
    return this;
  }
  /**
   * Write a `Block` (a `{ cid:CID, bytes:Uint8Array }` pair) to the archive.
   * Throws if there is not enough capacity.
   *
   * @param {Block} block - A `{ cid:CID, bytes:Uint8Array }` pair.
   * @returns {CarBufferWriter}
   */
  write(block) {
    addBlock(this, block);
    return this;
  }
  /**
   * Finalize the CAR and return it as a `Uint8Array`.
   *
   * @param {object} [options]
   * @param {boolean} [options.resize]
   * @returns {Uint8Array}
   */
  close(options) {
    return close(this, options);
  }
};
var addRoot = (writer, root2, options = {}) => {
  const { resize = false } = options;
  const { bytes: bytes3, headerSize, byteOffset, roots } = writer;
  writer.roots.push(root2);
  const size5 = headerLength(writer);
  if (size5 > headerSize) {
    if (size5 - headerSize + byteOffset < bytes3.byteLength) {
      if (resize) {
        resizeHeader(writer, size5);
      } else {
        roots.pop();
        throw new RangeError(`Header of size ${headerSize} has no capacity for new root ${root2}.
  However there is a space in the buffer and you could call addRoot(root, { resize: root }) to resize header to make a space for this root.`);
      }
    } else {
      roots.pop();
      throw new RangeError(`Buffer has no capacity for a new root ${root2}`);
    }
  }
};
var blockLength = ({ cid, bytes: bytes3 }) => {
  const size5 = cid.bytes.byteLength + bytes3.byteLength;
  return import_varint3.default.encodingLength(size5) + size5;
};
var addBlock = (writer, { cid, bytes: bytes3 }) => {
  const byteLength = cid.bytes.byteLength + bytes3.byteLength;
  const size5 = import_varint3.default.encode(byteLength);
  if (writer.byteOffset + size5.length + byteLength > writer.bytes.byteLength) {
    throw new RangeError("Buffer has no capacity for this block");
  } else {
    writeBytes(writer, size5);
    writeBytes(writer, cid.bytes);
    writeBytes(writer, bytes3);
  }
};
var close = (writer, options = {}) => {
  const { resize = false } = options;
  const { roots, bytes: bytes3, byteOffset, headerSize } = writer;
  const headerBytes = encode4({ version: 1, roots });
  const varintBytes = import_varint3.default.encode(headerBytes.length);
  const size5 = varintBytes.length + headerBytes.byteLength;
  const offset2 = headerSize - size5;
  if (offset2 === 0) {
    writeHeader(writer, varintBytes, headerBytes);
    return bytes3.subarray(0, byteOffset);
  } else if (resize) {
    resizeHeader(writer, size5);
    writeHeader(writer, varintBytes, headerBytes);
    return bytes3.subarray(0, writer.byteOffset);
  } else {
    throw new RangeError(`Header size was overestimated.
You can use close({ resize: true }) to resize header`);
  }
};
var resizeHeader = (writer, byteLength) => {
  const { bytes: bytes3, headerSize } = writer;
  bytes3.set(bytes3.subarray(headerSize, writer.byteOffset), byteLength);
  writer.byteOffset += byteLength - headerSize;
  writer.headerSize = byteLength;
};
var writeBytes = (writer, bytes3) => {
  writer.bytes.set(bytes3, writer.byteOffset);
  writer.byteOffset += bytes3.length;
};
var writeHeader = ({ bytes: bytes3 }, varint6, header) => {
  bytes3.set(varint6);
  bytes3.set(header, varint6.length);
};
var headerPreludeTokens = [
  new Token(Type.map, 2),
  new Token(Type.string, "version"),
  new Token(Type.uint, 1),
  new Token(Type.string, "roots")
];
var CID_TAG = new Token(Type.tag, 42);
var calculateHeaderLength = (rootLengths) => {
  const tokens = [...headerPreludeTokens];
  tokens.push(new Token(Type.array, rootLengths.length));
  for (const rootLength of rootLengths) {
    tokens.push(CID_TAG);
    tokens.push(new Token(Type.bytes, { length: rootLength + 1 }));
  }
  const length2 = tokensToLength(tokens);
  return import_varint3.default.encodingLength(length2) + length2;
};
var headerLength = ({ roots }) => calculateHeaderLength(roots.map((cid) => cid.bytes.byteLength));
var createWriter = (buffer2, options = {}) => {
  const {
    roots = [],
    byteOffset = 0,
    byteLength = buffer2.byteLength,
    headerSize = headerLength({ roots })
  } = options;
  const bytes3 = new Uint8Array(buffer2, byteOffset, byteLength);
  const writer = new CarBufferWriter(bytes3, headerSize);
  for (const root2 of roots) {
    writer.addRoot(root2);
  }
  return writer;
};

// node_modules/@ucanto/core/src/car.js
var contentType2 = "application/vnd.ipld.car";
var name5 = "CAR";
var code6 = 514;
var Writer = class {
  /**
   * @param {API.IPLDBlock[]} blocks
   * @param {number} byteLength
   */
  constructor(blocks = [], byteLength = 0) {
    this.written = /* @__PURE__ */ new Set();
    this.blocks = blocks;
    this.byteLength = byteLength;
  }
  /**
   * @param {API.IPLDBlock[]} blocks
   */
  write(...blocks) {
    for (const block of blocks) {
      const id = block.cid.toString(base32);
      if (!this.written.has(id)) {
        this.blocks.push(block);
        this.byteLength += blockLength(
          /** @type {any} */
          block
        );
        this.written.add(id);
      }
    }
    return this;
  }
  /**
   * @param {API.IPLDBlock[]} rootBlocks
   */
  flush(...rootBlocks) {
    const roots = [];
    for (const block of rootBlocks.reverse()) {
      const id = block.cid.toString(base32);
      if (!this.written.has(id)) {
        this.blocks.unshift(block);
        this.byteLength += blockLength({
          cid: (
            /** @type {CarBufferWriter.CID} */
            block.cid
          ),
          bytes: block.bytes
        });
        this.written.add(id);
      }
      roots.unshift(
        /** @type {CarBufferWriter.CID} */
        block.cid
      );
    }
    this.byteLength += headerLength({ roots });
    const buffer2 = new ArrayBuffer(this.byteLength);
    const writer = createWriter(buffer2, { roots });
    for (
      const block of
      /** @type {CarBufferWriter.Block[]} */
      this.blocks
    ) {
      writer.write(block);
    }
    return writer.close();
  }
};
var createWriter2 = () => new Writer();
var encode16 = ({ roots = [], blocks }) => {
  const writer = new Writer();
  if (blocks) {
    writer.write(...blocks.values());
  }
  return writer.flush(...roots);
};
var decode16 = (bytes3) => {
  const reader = CarBufferReader.fromBytes(bytes3);
  const roots = [];
  const blocks = /* @__PURE__ */ new Map();
  for (const root2 of reader.getRoots()) {
    const block = (
      /** @type {API.IPLDBlock} */
      reader.get(root2)
    );
    if (block) {
      roots.push(block);
    }
  }
  for (const block of reader.blocks()) {
    blocks.set(block.cid.toString(), block);
  }
  return { roots, blocks };
};
var link3 = async (bytes3, { hasher = sha2562 } = {}) => {
  return (
    /** @type {API.Link<T, typeof code, typeof hasher.code>} */
    create2(code6, await hasher.digest(bytes3))
  );
};
var write3 = async (data, options) => {
  const bytes3 = encode16(data);
  const cid = await link3(bytes3, options);
  return { bytes: bytes3, cid };
};

// node_modules/@ucanto/core/src/schema.js
var schema_exports3 = {};
__export(schema_exports3, {
  API: () => API5,
  Bytes: () => Bytes,
  DID: () => did_exports2,
  Link: () => link_exports2,
  Principal: () => principal_exports,
  Text: () => text_exports,
  URI: () => uri_exports,
  and: () => and,
  array: () => array,
  boolean: () => boolean,
  bytes: () => bytes2,
  dictionary: () => dictionary,
  did: () => match4,
  didBytes: () => matchBytes,
  endsWith: () => endsWith,
  enum: () => createEnum,
  error: () => error2,
  float: () => float,
  greaterThan: () => greaterThan,
  integer: () => integer,
  intersection: () => intersection,
  lessThan: () => lessThan,
  link: () => match2,
  literal: () => literal,
  memberError: () => memberError,
  never: () => never,
  nullable: () => nullable,
  number: () => number,
  ok: () => ok,
  optional: () => optional,
  or: () => or2,
  principal: () => match3,
  refine: () => refine,
  startsWith: () => startsWith,
  string: () => string,
  struct: () => struct,
  text: () => match5,
  toString: () => toString3,
  tuple: () => tuple,
  typeError: () => typeError,
  uint64: () => uint64,
  union: () => union,
  unknown: () => unknown,
  uri: () => match,
  variant: () => variant
});

// node_modules/@ucanto/core/src/schema/uri.js
var uri_exports = {};
__export(uri_exports, {
  from: () => from6,
  match: () => match,
  read: () => read2,
  uri: () => uri
});
var API6 = __toESM(require_lib(), 1);

// node_modules/@ucanto/core/src/result.js
var API4 = __toESM(require_lib());
var ok = (value) => {
  if (value == null) {
    throw new TypeError(`ok(${value}) is not allowed, consider ok({}) instead`);
  } else {
    return { ok: value };
  }
};
var error = (cause) => {
  if (cause == null) {
    throw new TypeError(
      `error(${cause}) is not allowed, consider passing an error instead`
    );
  } else {
    return { error: cause };
  }
};
var panic = (message) => {
  throw new Failure(message);
};
var fail2 = (message) => ({ error: new Failure(message) });
var Failure = class extends Error {
  describe() {
    return this.toString();
  }
  get message() {
    return this.describe();
  }
  toJSON() {
    const { name: name15, message, stack } = this;
    return { name: name15, message, stack };
  }
};

// node_modules/@ucanto/core/src/schema/schema.js
var API5 = class {
  /**
   * @param {Settings} settings
   */
  constructor(settings) {
    this.settings = settings;
  }
  toString() {
    return `new ${this.constructor.name}()`;
  }
  /**
   * @abstract
   * @param {I} input
   * @param {Settings} settings
   * @returns {Schema.ReadResult<T>}
   */
  /* c8 ignore next 3 */
  readWith(input11, settings) {
    throw new Error(`Abstract method readWith must be implemented by subclass`);
  }
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<T>}
   */
  read(input11) {
    return this.readWith(input11, this.settings);
  }
  /**
   * @param {unknown} value
   * @returns {value is T}
   */
  is(value) {
    var _a15;
    return !((_a15 = this.read(
      /** @type {I} */
      value
    )) == null ? void 0 : _a15.error);
  }
  /**
   * @param {unknown} value
   * @return {T}
   */
  from(value) {
    const result = this.read(
      /** @type {I} */
      value
    );
    if (result.error) {
      throw result.error;
    } else {
      return result.ok;
    }
  }
  /**
   * @returns {Schema.Schema<T|undefined, I>}
   */
  optional() {
    return optional(this);
  }
  /**
   * @returns {Schema.Schema<T|null, I>}
   */
  nullable() {
    return nullable(this);
  }
  /**
   * @returns {Schema.Schema<T[], I>}
   */
  array() {
    return array(this);
  }
  /**
   * @template U
   * @param {Schema.Reader<U, I>} schema
   * @returns {Schema.Schema<T | U, I>}
   */
  or(schema6) {
    return or2(this, schema6);
  }
  /**
   * @template U
   * @param {Schema.Reader<U, I>} schema
   * @returns {Schema.Schema<T & U, I>}
   */
  and(schema6) {
    return and(this, schema6);
  }
  /**
   * @template {T} U
   * @param {Schema.Reader<U, T>} schema
   * @returns {Schema.Schema<U, I>}
   */
  refine(schema6) {
    return refine(this, schema6);
  }
  /**
   * @template {string} Kind
   * @param {Kind} [kind]
   * @returns {Schema.Schema<Schema.Branded<T, Kind>, I>}
   */
  brand(kind) {
    return (
      /** @type {Schema.Schema<Schema.Branded<T, Kind>, I>} */
      this
    );
  }
  /**
   * @param {Schema.NotUndefined<T>} value
   * @returns {Schema.DefaultSchema<Schema.NotUndefined<T>, I>}
   */
  default(value) {
    const fallback = this.from(value);
    if (fallback === void 0) {
      throw new Error(`Value of type undefined is not a valid default`);
    }
    const schema6 = new Default({
      reader: (
        /** @type {Schema.Reader<T, I>} */
        this
      ),
      value: (
        /** @type {Schema.NotUndefined<T>} */
        fallback
      )
    });
    return (
      /** @type {Schema.DefaultSchema<Schema.NotUndefined<T>, I>} */
      schema6
    );
  }
};
var Never = class extends API5 {
  toString() {
    return "never()";
  }
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<never>}
   */
  read(input11) {
    return typeError({ expect: "never", actual: input11 });
  }
};
var never = () => new Never();
var Unknown = class extends API5 {
  /**
   * @param {I} input
   */
  read(input11) {
    return (
      /** @type {Schema.ReadResult<unknown>}*/
      { ok: input11 }
    );
  }
  toString() {
    return "unknown()";
  }
};
var unknown = () => new Unknown();
var Nullable = class extends API5 {
  /**
   * @param {I} input
   * @param {Schema.Reader<O, I>} reader
   */
  readWith(input11, reader) {
    const result = reader.read(input11);
    if (result.error) {
      return input11 === null ? { ok: null } : {
        error: new UnionError({
          causes: [
            result.error,
            typeError({ expect: "null", actual: input11 }).error
          ]
        })
      };
    } else {
      return result;
    }
  }
  toString() {
    return `${this.settings}.nullable()`;
  }
};
var nullable = (schema6) => new Nullable(schema6);
var Optional = class extends API5 {
  optional() {
    return this;
  }
  /**
   * @param {I} input
   * @param {Schema.Reader<O, I>} reader
   * @returns {Schema.ReadResult<O|undefined>}
   */
  readWith(input11, reader) {
    const result = reader.read(input11);
    return result.error && input11 === void 0 ? { ok: void 0 } : result;
  }
  toString() {
    return `${this.settings}.optional()`;
  }
};
var Default = class extends API5 {
  /**
   * @returns {Schema.DefaultSchema<O & Schema.NotUndefined<O>, I>}
   */
  optional() {
    return (
      /** @type {Schema.DefaultSchema<O & Schema.NotUndefined<O>, I>} */
      this
    );
  }
  /**
   * @param {I} input
   * @param {object} options
   * @param {Schema.Reader<O|undefined, I>} options.reader
   * @param {O} options.value
   * @returns {Schema.ReadResult<O>}
   */
  readWith(input11, { reader, value }) {
    if (input11 === void 0) {
      return (
        /** @type {Schema.ReadResult<O>} */
        { ok: value }
      );
    } else {
      const result = reader.read(input11);
      return result.error ? result : result.ok !== void 0 ? (
        // We just checked that result.ok is not undefined but still needs
        // reassurance
        /** @type {Schema.ReadResult<O>} */
        result
      ) : { ok: value };
    }
  }
  toString() {
    return `${this.settings.reader}.default(${JSON.stringify(
      this.settings.value
    )})`;
  }
  get value() {
    return this.settings.value;
  }
};
var optional = (schema6) => new Optional(schema6);
var ArrayOf = class extends API5 {
  /**
   * @param {I} input
   * @param {Schema.Reader<O, I>} schema
   */
  readWith(input11, schema6) {
    if (!Array.isArray(input11)) {
      return typeError({ expect: "array", actual: input11 });
    }
    const results = [];
    for (const [index3, value] of input11.entries()) {
      const result = schema6.read(value);
      if (result.error) {
        return memberError({ at: index3, cause: result.error });
      } else {
        results.push(result.ok);
      }
    }
    return { ok: results };
  }
  get element() {
    return this.settings;
  }
  toString() {
    return `array(${this.element})`;
  }
};
var array = (schema6) => new ArrayOf(schema6);
var Tuple = class extends API5 {
  /**
   * @param {I} input
   * @param {U} shape
   * @returns {Schema.ReadResult<Schema.InferTuple<U>>}
   */
  readWith(input11, shape) {
    if (!Array.isArray(input11)) {
      return typeError({ expect: "array", actual: input11 });
    }
    if (input11.length !== this.shape.length) {
      return error2(`Array must contain exactly ${this.shape.length} elements`);
    }
    const results = [];
    for (const [index3, reader] of shape.entries()) {
      const result = reader.read(input11[index3]);
      if (result.error) {
        return memberError({ at: index3, cause: result.error });
      } else {
        results[index3] = result.ok;
      }
    }
    return { ok: (
      /** @type {Schema.InferTuple<U>} */
      results
    ) };
  }
  /** @type {U} */
  get shape() {
    return this.settings;
  }
  toString() {
    return `tuple([${this.shape.map((reader) => reader.toString()).join(", ")}])`;
  }
};
var tuple = (shape) => new Tuple(shape);
var Dictionary = class _Dictionary extends API5 {
  /**
   * @param {I} input
   * @param {object} schema
   * @param {Schema.Reader<K, string>} schema.key
   * @param {Schema.Reader<V, I>} schema.value
   */
  readWith(input11, { key, value }) {
    if (typeof input11 != "object" || input11 === null || Array.isArray(input11)) {
      return typeError({
        expect: "dictionary",
        actual: input11
      });
    }
    const dict = (
      /** @type {Schema.Dictionary<K, V>} */
      {}
    );
    for (const [k, v] of Object.entries(input11)) {
      const keyResult = key.read(k);
      if (keyResult.error) {
        return memberError({ at: k, cause: keyResult.error });
      }
      const valueResult = value.read(v);
      if (valueResult.error) {
        return memberError({ at: k, cause: valueResult.error });
      }
      if (valueResult.ok !== void 0) {
        dict[keyResult.ok] = valueResult.ok;
      }
    }
    return { ok: dict };
  }
  get key() {
    return this.settings.key;
  }
  get value() {
    return this.settings.value;
  }
  partial() {
    const { key, value } = this.settings;
    return new _Dictionary({
      key,
      value: optional(value)
    });
  }
  toString() {
    return `dictionary(${this.settings})`;
  }
};
var dictionary = ({ value, key }) => new Dictionary({
  value,
  key: key || /** @type {Schema.Reader<K, string>} */
  string()
});
var Enum = class extends API5 {
  /**
   * @param {I} input
   * @param {{type:string, variants:Set<T[number]>}} settings
   * @returns {Schema.ReadResult<T[number]>}
   */
  readWith(input11, { variants, type: type2 }) {
    if (variants.has(input11)) {
      return (
        /** @type {Schema.ReadResult<T[number]>} */
        { ok: input11 }
      );
    } else {
      return typeError({ expect: type2, actual: input11 });
    }
  }
  toString() {
    return this.settings.type;
  }
};
var createEnum = (variants) => new Enum({
  type: variants.join("|"),
  variants: new Set(variants)
});
var Union = class extends API5 {
  /**
   * @param {I} input
   * @param {U} variants
   */
  readWith(input11, variants) {
    const causes = [];
    for (const reader of variants) {
      const result = reader.read(input11);
      if (result.error) {
        causes.push(result.error);
      } else {
        return (
          /** @type {Schema.ReadResult<Schema.InferUnion<U>>} */
          result
        );
      }
    }
    return { error: new UnionError({ causes }) };
  }
  get variants() {
    return this.settings;
  }
  toString() {
    return `union([${this.variants.map((type2) => type2.toString()).join(", ")}])`;
  }
};
var union = (variants) => new Union(variants);
var or2 = (left, right) => union([left, right]);
var Intersection = class extends API5 {
  /**
   * @param {I} input
   * @param {U} schemas
   * @returns {Schema.ReadResult<Schema.InferIntersection<U>>}
   */
  readWith(input11, schemas) {
    const causes = [];
    for (const schema6 of schemas) {
      const result = schema6.read(input11);
      if (result.error) {
        causes.push(result.error);
      }
    }
    return causes.length > 0 ? { error: new IntersectionError({ causes }) } : (
      /** @type {Schema.ReadResult<Schema.InferIntersection<U>>} */
      {
        ok: input11
      }
    );
  }
  toString() {
    return `intersection([${this.settings.map((type2) => type2.toString()).join(",")}])`;
  }
};
var intersection = (variants) => new Intersection(variants);
var and = (left, right) => intersection([left, right]);
var Boolean2 = class extends API5 {
  /**
   * @param {I} input
   */
  readWith(input11) {
    switch (input11) {
      case true:
      case false:
        return { ok: (
          /** @type {boolean} */
          input11
        ) };
      default:
        return typeError({
          expect: "boolean",
          actual: input11
        });
    }
  }
  toString() {
    return `boolean()`;
  }
};
var anyBoolean = new Boolean2();
var boolean = () => anyBoolean;
var UnknownNumber = class extends API5 {
  /**
   * @param {number} n
   */
  greaterThan(n) {
    return this.refine(greaterThan(n));
  }
  /**
   * @param {number} n
   */
  lessThan(n) {
    return this.refine(lessThan(n));
  }
  /**
   * @template {O} U
   * @param {Schema.Reader<U, O>} schema
   * @returns {Schema.NumberSchema<U, I>}
   */
  refine(schema6) {
    return new RefinedNumber({ base: this, schema: schema6 });
  }
};
var AnyNumber = class extends UnknownNumber {
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<number>}
   */
  readWith(input11) {
    return typeof input11 === "number" ? { ok: input11 } : typeError({ expect: "number", actual: input11 });
  }
  toString() {
    return `number()`;
  }
};
var anyNumber = new AnyNumber();
var number = () => anyNumber;
var RefinedNumber = class extends UnknownNumber {
  /**
   * @param {I} input
   * @param {{base:Schema.Reader<T, I>, schema:Schema.Reader<O, T>}} settings
   * @returns {Schema.ReadResult<O>}
   */
  readWith(input11, { base: base3, schema: schema6 }) {
    const result = base3.read(input11);
    return result.error ? result : schema6.read(result.ok);
  }
  toString() {
    return `${this.settings.base}.refine(${this.settings.schema})`;
  }
};
var LessThan = class extends API5 {
  /**
   * @param {T} input
   * @param {number} number
   * @returns {Schema.ReadResult<T>}
   */
  readWith(input11, number3) {
    if (input11 < number3) {
      return { ok: input11 };
    } else {
      return error2(`Expected ${input11} < ${number3}`);
    }
  }
  toString() {
    return `lessThan(${this.settings})`;
  }
};
var lessThan = (n) => new LessThan(n);
var GreaterThan = class extends API5 {
  /**
   * @param {T} input
   * @param {number} number
   * @returns {Schema.ReadResult<T>}
   */
  readWith(input11, number3) {
    if (input11 > number3) {
      return { ok: input11 };
    } else {
      return error2(`Expected ${input11} > ${number3}`);
    }
  }
  toString() {
    return `greaterThan(${this.settings})`;
  }
};
var greaterThan = (n) => new GreaterThan(n);
var Integer = {
  /**
   * @param {number} input
   * @returns {Schema.ReadResult<Schema.Integer>}
   */
  read(input11) {
    return Number.isInteger(input11) ? { ok: (
      /** @type {Schema.Integer} */
      input11
    ) } : typeError({
      expect: "integer",
      actual: input11
    });
  },
  toString() {
    return `Integer`;
  }
};
var anyInteger = anyNumber.refine(Integer);
var integer = () => anyInteger;
var MAX_UINT64 = 2n ** 64n - 1n;
var Uint64Schema = class extends API5 {
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<O>}
   */
  read(input11) {
    switch (typeof input11) {
      case "bigint":
        return input11 > MAX_UINT64 ? error2(`Integer is too big for uint64, ${input11} > ${MAX_UINT64}`) : input11 < 0 ? error2(
          `Negative integer can not be represented as uint64, ${input11} < ${0}`
        ) : { ok: (
          /** @type {I & O} */
          input11
        ) };
      case "number":
        return !Number.isInteger(input11) ? typeError({
          expect: "uint64",
          actual: input11
        }) : input11 < 0 ? error2(
          `Negative integer can not be represented as uint64, ${input11} < ${0}`
        ) : { ok: (
          /** @type {O} */
          BigInt(input11)
        ) };
      default:
        return typeError({
          expect: "uint64",
          actual: input11
        });
    }
  }
  toString() {
    return `uint64`;
  }
};
var Uint64 = new Uint64Schema();
var uint64 = () => Uint64;
var Float = {
  /**
   * @param {number} number
   * @returns {Schema.ReadResult<Schema.Float>}
   */
  read(number3) {
    return Number.isFinite(number3) ? { ok: (
      /** @type {Schema.Float} */
      number3
    ) } : typeError({
      expect: "Float",
      actual: number3
    });
  },
  toString() {
    return "Float";
  }
};
var anyFloat = anyNumber.refine(Float);
var float = () => anyFloat;
var UnknownString = class extends API5 {
  /**
   * @template {O|unknown} U
   * @param {Schema.Reader<U, O>} schema
   * @returns {Schema.StringSchema<O & U, I>}
   */
  refine(schema6) {
    const other = (
      /** @type {Schema.Reader<U, O>} */
      schema6
    );
    const rest = new RefinedString({
      base: this,
      schema: other
    });
    return (
      /** @type {Schema.StringSchema<O & U, I>} */
      rest
    );
  }
  /**
   * @template {string} Prefix
   * @param {Prefix} prefix
   */
  startsWith(prefix2) {
    return this.refine(startsWith(prefix2));
  }
  /**
   * @template {string} Suffix
   * @param {Suffix} suffix
   */
  endsWith(suffix) {
    return this.refine(endsWith(suffix));
  }
  toString() {
    return `string()`;
  }
};
var RefinedString = class extends UnknownString {
  /**
   * @param {I} input
   * @param {{base:Schema.Reader<T, I>, schema:Schema.Reader<O, T>}} settings
   * @returns {Schema.ReadResult<T & O>}
   */
  readWith(input11, { base: base3, schema: schema6 }) {
    const result = base3.read(input11);
    return result.error ? result : (
      /** @type {Schema.ReadResult<T & O>} */
      schema6.read(result.ok)
    );
  }
  toString() {
    return `${this.settings.base}.refine(${this.settings.schema})`;
  }
};
var AnyString = class extends UnknownString {
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<string>}
   */
  readWith(input11) {
    return typeof input11 === "string" ? { ok: input11 } : typeError({ expect: "string", actual: input11 });
  }
};
var anyString = new AnyString();
var string = () => anyString;
var BytesSchema = class extends API5 {
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<Uint8Array>}
   */
  readWith(input11) {
    if (input11 instanceof Uint8Array) {
      return { ok: input11 };
    } else {
      return typeError({ expect: "Uint8Array", actual: input11 });
    }
  }
};
var Bytes = new BytesSchema();
var bytes2 = () => Bytes;
var StartsWith = class extends API5 {
  /**
   * @param {Body} input
   * @param {Prefix} prefix
   */
  readWith(input11, prefix2) {
    const result = input11.startsWith(prefix2) ? (
      /** @type {Schema.ReadResult<Body & `${Prefix}${Body}`>} */
      {
        ok: input11
      }
    ) : error2(`Expect string to start with "${prefix2}" instead got "${input11}"`);
    return result;
  }
  get prefix() {
    return this.settings;
  }
  toString() {
    return `startsWith("${this.prefix}")`;
  }
};
var startsWith = (prefix2) => new StartsWith(prefix2);
var EndsWith = class extends API5 {
  /**
   * @param {Body} input
   * @param {Suffix} suffix
   */
  readWith(input11, suffix) {
    return input11.endsWith(suffix) ? (
      /** @type {Schema.ReadResult<Body & `${Body}${Suffix}`>} */
      {
        ok: input11
      }
    ) : error2(`Expect string to end with "${suffix}" instead got "${input11}"`);
  }
  get suffix() {
    return this.settings;
  }
  toString() {
    return `endsWith("${this.suffix}")`;
  }
};
var endsWith = (suffix) => new EndsWith(suffix);
var Refine = class extends API5 {
  /**
   * @param {I} input
   * @param {{ base: Schema.Reader<T, I>, schema: Schema.Reader<U, T> }} settings
   */
  readWith(input11, { base: base3, schema: schema6 }) {
    const result = base3.read(input11);
    return result.error ? result : schema6.read(result.ok);
  }
  toString() {
    return `${this.settings.base}.refine(${this.settings.schema})`;
  }
};
var refine = (base3, schema6) => new Refine({ base: base3, schema: schema6 });
var Literal = class extends API5 {
  /**
   * @param {I} input
   * @param {T} expect
   * @returns {Schema.ReadResult<T>}
   */
  readWith(input11, expect) {
    return input11 !== /** @type {unknown} */
    expect ? { error: new LiteralError({ expect, actual: input11 }) } : { ok: expect };
  }
  get value() {
    return (
      /** @type {Exclude<T, undefined>} */
      this.settings
    );
  }
  /**
   * @template {Schema.NotUndefined<T>} U
   * @param {U} value
   */
  default(value = (
    /** @type {U} */
    this.value
  )) {
    return super.default(value);
  }
  toString() {
    return `literal(${toString3(this.value)})`;
  }
};
var literal = (value) => new Literal(value);
var Struct = class _Struct extends API5 {
  /**
   * @param {I} input
   * @param {U} shape
   * @returns {Schema.ReadResult<Schema.InferStruct<U>>}
   */
  readWith(input11, shape) {
    if (typeof input11 != "object" || input11 === null || Array.isArray(input11)) {
      return typeError({
        expect: "object",
        actual: input11
      });
    }
    const source = (
      /** @type {{[K in keyof U]: unknown}} */
      input11
    );
    const struct2 = (
      /** @type {{[K in keyof U]: Schema.Infer<U[K]>}} */
      {}
    );
    const entries3 = (
      /** @type {{[K in keyof U]: [K & string, U[K]]}[keyof U][]} */
      Object.entries(shape)
    );
    for (const [at2, reader] of entries3) {
      const result = reader.read(source[at2]);
      if (result.error) {
        return memberError({ at: at2, cause: result.error });
      } else if (result.ok !== void 0) {
        struct2[at2] = /** @type {Schema.Infer<U[typeof at]>} */
        result.ok;
      }
    }
    return { ok: struct2 };
  }
  /**
   * @returns {Schema.MapRepresentation<Partial<Schema.InferStruct<U>>> & Schema.StructSchema}
   */
  partial() {
    return new _Struct(
      Object.fromEntries(
        Object.entries(this.shape).map(([key, value]) => [key, optional(value)])
      )
    );
  }
  /** @type {U} */
  get shape() {
    return this.settings;
  }
  toString() {
    return [
      `struct({ `,
      ...Object.entries(this.shape).map(([key, schema6]) => `${key}: ${schema6}`).join(", "),
      ` })`
    ].join("");
  }
  /**
   * @param {Schema.InferStructSource<U>} data
   */
  create(data) {
    return this.from(data || {});
  }
  /**
   * @template {{[key:string]: Schema.Reader}} E
   * @param {E} extension
   * @returns {Schema.StructSchema<U & E, I>}
   */
  extend(extension) {
    return new _Struct({ ...this.shape, ...extension });
  }
};
var struct = (fields) => {
  const shape = (
    /** @type {{[K in keyof U]: Schema.Reader<unknown, unknown>}} */
    {}
  );
  const entries3 = Object.entries(fields);
  for (const [key, field] of entries3) {
    switch (typeof field) {
      case "number":
      case "string":
      case "boolean":
        shape[key] = literal(field);
        break;
      case "object":
        shape[key] = field === null ? literal(null) : field;
        break;
      default:
        throw new Error(
          `Invalid struct field "${key}", expected schema or literal, instead got ${typeof field}`
        );
    }
  }
  return new Struct(
    /** @type {V} */
    shape
  );
};
var Variant = class extends API5 {
  /**
   * @param {I} input
   * @param {U} variants
   * @returns {Schema.ReadResult<Schema.InferVariant<U>>}
   */
  readWith(input11, variants) {
    if (typeof input11 != "object" || input11 === null || Array.isArray(input11)) {
      return typeError({
        expect: "object",
        actual: input11
      });
    }
    const keys2 = (
      /** @type {Array<keyof input & keyof variants & string>} */
      Object.keys(input11)
    );
    const [key] = keys2.length === 1 ? keys2 : [];
    const reader = key ? variants[key] : void 0;
    if (reader) {
      const result = reader.read(input11[key]);
      return result.error ? memberError({ at: key, cause: result.error }) : { ok: (
        /** @type {Schema.InferVariant<U>} */
        { [key]: result.ok }
      ) };
    } else if (variants._) {
      const result = variants._.read(input11);
      return result.error ? result : { ok: (
        /** @type {Schema.InferVariant<U>} */
        { _: result.ok }
      ) };
    } else if (key) {
      return error2(
        `Expected an object with one of the these keys: ${Object.keys(variants).sort().join(", ")} instead got object with key ${key}`
      );
    } else {
      return error2(
        "Expected an object with a single key instead got object with keys " + keys2.sort().join(", ")
      );
    }
  }
  /**
   * @template [E=never]
   * @param {I} input
   * @param {E} [fallback]
   */
  match(input11, fallback) {
    const result = this.read(input11);
    if (result.error) {
      if (fallback !== void 0) {
        return [null, fallback];
      } else {
        throw result.error;
      }
    } else {
      const [key] = Object.keys(result.ok);
      const value = result.ok[key];
      return (
        /** @type {any} */
        [key, value]
      );
    }
  }
  /**
   * @template {Schema.InferVariant<U>} O
   * @param {O} source
   * @returns {O}
   */
  create(source) {
    return (
      /** @type {O} */
      this.from(source)
    );
  }
};
var variant = (variants) => new Variant(variants);
var error2 = (message) => ({ error: new SchemaError(message) });
var SchemaError = class extends Failure {
  get name() {
    return "SchemaError";
  }
  /* c8 ignore next 3 */
  describe() {
    return this.name;
  }
};
var TypeError2 = class extends SchemaError {
  /**
   * @param {{expect:string, actual:unknown}} data
   */
  constructor({ expect, actual }) {
    super();
    this.expect = expect;
    this.actual = actual;
  }
  get name() {
    return "TypeError";
  }
  describe() {
    return `Expected value of type ${this.expect} instead got ${toString3(
      this.actual
    )}`;
  }
};
var typeError = (data) => ({ error: new TypeError2(data) });
var toString3 = (value) => {
  const type2 = typeof value;
  switch (type2) {
    case "boolean":
    case "string":
      return JSON.stringify(value);
    case "bigint":
      return `${value}n`;
    case "number":
    case "symbol":
    case "undefined":
      return String(value);
    case "object":
      return value === null ? "null" : Array.isArray(value) ? "array" : Symbol.toStringTag in /** @type {object} */
      value ? value[Symbol.toStringTag] : "object";
    default:
      return type2;
  }
};
var LiteralError = class extends SchemaError {
  /**
   * @param {{
   * expect:string|number|boolean|null
   * actual:unknown
   * }} data
   */
  constructor({ expect, actual }) {
    super();
    this.expect = expect;
    this.actual = actual;
  }
  get name() {
    return "LiteralError";
  }
  describe() {
    return `Expected literal ${toString3(this.expect)} instead got ${toString3(
      this.actual
    )}`;
  }
};
var ElementError = class extends SchemaError {
  /**
   * @param {{at:number, cause:Schema.Error}} data
   */
  constructor({ at: at2, cause }) {
    super();
    this.at = at2;
    this.cause = cause;
  }
  get name() {
    return "ElementError";
  }
  describe() {
    return [
      `Array contains invalid element at ${this.at}:`,
      li(this.cause.message)
    ].join("\n");
  }
};
var FieldError = class extends SchemaError {
  /**
   * @param {{at:string, cause:Schema.Error}} data
   */
  constructor({ at: at2, cause }) {
    super();
    this.at = at2;
    this.cause = cause;
  }
  get name() {
    return "FieldError";
  }
  describe() {
    return [
      `Object contains invalid field "${this.at}":`,
      li(this.cause.message)
    ].join("\n");
  }
};
var memberError = ({ at: at2, cause }) => typeof at2 === "string" ? { error: new FieldError({ at: at2, cause }) } : { error: new ElementError({ at: at2, cause }) };
var UnionError = class extends SchemaError {
  /**
   * @param {{causes: Schema.Error[]}} data
   */
  constructor({ causes }) {
    super();
    this.causes = causes;
  }
  get name() {
    return "UnionError";
  }
  describe() {
    const { causes } = this;
    return [
      `Value does not match any type of the union:`,
      ...causes.map((cause) => li(cause.message))
    ].join("\n");
  }
};
var IntersectionError = class extends SchemaError {
  /**
   * @param {{causes: Schema.Error[]}} data
   */
  constructor({ causes }) {
    super();
    this.causes = causes;
  }
  get name() {
    return "IntersectionError";
  }
  describe() {
    const { causes } = this;
    return [
      `Value does not match following types of the intersection:`,
      ...causes.map((cause) => li(cause.message))
    ].join("\n");
  }
};
var indent = (message, indent3 = "  ") => `${indent3}${message.split("\n").join(`
${indent3}`)}`;
var li = (message) => indent(`- ${message}`);

// node_modules/@ucanto/core/src/schema/uri.js
var URISchema = class extends API5 {
  /**
   * @param {unknown} input
   * @param {Partial<O>} options
   * @returns {Schema.ReadResult<API.URI<O['protocol']>>}
   */
  readWith(input11, { protocol } = {}) {
    if (typeof input11 !== "string" && !(input11 instanceof URL)) {
      return error2(
        `Expected URI but got ${input11 === null ? "null" : typeof input11}`
      );
    }
    try {
      const url = new URL(String(input11));
      if (protocol != null && url.protocol !== protocol) {
        return error2(`Expected ${protocol} URI instead got ${url.href}`);
      } else {
        return { ok: (
          /** @type {API.URI<O['protocol']>} */
          url.href
        ) };
      }
    } catch (_) {
      return error2(`Invalid URI`);
    }
  }
};
var schema = new URISchema({});
var uri = () => schema;
var read2 = (input11) => schema.read(input11);
var match = (options) => new URISchema(options);
var from6 = (input11) => (
  /** @type {API.URI<`${Scheme}:`>} */
  schema.from(input11)
);

// node_modules/@ucanto/core/src/schema/link.js
var link_exports2 = {};
__export(link_exports2, {
  create: () => create2,
  createLegacy: () => createLegacy,
  isLink: () => isLink,
  link: () => link4,
  match: () => match2,
  optional: () => optional2,
  parse: () => parse,
  read: () => read3,
  schema: () => schema2
});
var API7 = __toESM(require_lib(), 1);
var LinkSchema = class extends API5 {
  /**
   *
   * @param {unknown} cid
   * @param {Settings<Code, Alg, Version>} settings
   * @returns {Schema.ReadResult<API.Link<unknown, Code, Alg, Version>>}
   */
  readWith(cid, { code: code19, multihash = {}, version: version2 }) {
    if (cid == null) {
      return error2(`Expected link but got ${cid} instead`);
    } else {
      if (!isLink(cid)) {
        return error2(`Expected link to be a CID instead of ${cid}`);
      } else {
        if (code19 != null && cid.code !== code19) {
          return error2(
            `Expected link to be CID with 0x${code19.toString(16)} codec`
          );
        }
        if (multihash.code != null && cid.multihash.code !== multihash.code)
          return error2(
            `Expected link to be CID with 0x${multihash.code.toString(
              16
            )} hashing algorithm`
          );
        if (version2 != null && cid.version !== version2) {
          return error2(
            `Expected link to be CID version ${version2} instead of ${cid.version}`
          );
        }
        const [expectDigest, actualDigest] = multihash.digest != null ? [
          base32.baseEncode(multihash.digest),
          base32.baseEncode(cid.multihash.digest)
        ] : ["", ""];
        if (expectDigest !== actualDigest) {
          return error2(
            `Expected link with "${expectDigest}" hash digest instead of "${actualDigest}"`
          );
        }
        return {
          ok: (
            /** @type {API.Link<unknown, any, any, any>} */
            cid
          )
        };
      }
    }
  }
};
var schema2 = new LinkSchema({});
var link4 = () => schema2;
var match2 = (options = {}) => new LinkSchema(options);
var read3 = (input11) => schema2.read(input11);
var optional2 = () => schema2.optional();

// node_modules/@ucanto/core/src/schema/principal.js
var principal_exports = {};
__export(principal_exports, {
  from: () => from7,
  match: () => match3,
  principal: () => principal,
  read: () => read4
});
var API8 = __toESM(require_lib(), 1);
var PrincipalSchema = class extends API5 {
  /**
   * @param {unknown} source
   * @param {void|Method} method
   */
  readWith(source, method) {
    if (!(source instanceof Uint8Array)) {
      return typeError({ expect: "Uint8Array", actual: source });
    }
    let principal2;
    try {
      principal2 = decode8(source);
    } catch (err) {
      return error2(`Unable to decode bytes as DID: ${err}`);
    }
    const prefix2 = method ? `did:${method}:` : `did:`;
    if (!principal2.did().startsWith(prefix2)) {
      return error2(
        `Expected a ${prefix2} but got "${principal2.did()}" instead`
      );
    }
    return { ok: (
      /** @type {API.PrincipalView<API.DID<Method>>} */
      principal2
    ) };
  }
};
var schema3 = new PrincipalSchema();
var principal = () => schema3;
var read4 = (input11) => schema3.read(input11);
var match3 = (options = {}) => (
  /** @type {Schema.Schema<API.PrincipalView<API.DID<Method> & API.URI<"did:">>>} */
  new PrincipalSchema(options.method)
);
var from7 = (input11) => match3({}).from(input11);

// node_modules/@ucanto/core/src/schema/did.js
var did_exports2 = {};
__export(did_exports2, {
  did: () => did,
  didBytes: () => didBytes,
  from: () => from8,
  fromBytes: () => fromBytes2,
  match: () => match4,
  matchBytes: () => matchBytes,
  read: () => read5,
  readBytes: () => readBytes2
});
var API9 = __toESM(require_lib(), 1);
var DIDSchema = class extends API5 {
  /**
   * @param {string} source
   * @param {void|Method} method
   */
  readWith(source, method) {
    const prefix2 = method ? `did:${method}:` : `did:`;
    if (!source.startsWith(prefix2)) {
      return error2(`Expected a ${prefix2} but got "${source}" instead`);
    } else {
      return { ok: (
        /** @type {API.DID<Method>} */
        source
      ) };
    }
  }
};
var schema4 = string().refine(new DIDSchema());
var did = () => schema4;
var read5 = (input11) => schema4.read(input11);
var match4 = (options = {}) => (
  /** @type {Schema.Schema<API.DID<Method> & API.URI<"did:">>} */
  string().refine(new DIDSchema(options.method))
);
var from8 = (input11) => match4({}).from(input11);
var DIDBytesSchema = class extends API5 {
  /**
   * @param {unknown} source
   * @param {void|Method} method
   */
  readWith(source, method) {
    if (!(source instanceof Uint8Array)) {
      return typeError({ expect: "Uint8Array", actual: source });
    }
    let did2;
    try {
      did2 = decode8(source).did();
    } catch (err) {
      return error2(`Unable to parse bytes as did: ${err}`);
    }
    const prefix2 = method ? `did:${method}:` : `did:`;
    if (!did2.startsWith(prefix2)) {
      return error2(`Expected a ${prefix2} but got "${did2}" instead`);
    } else {
      return { ok: (
        /** @type {API.DID<Method>} */
        did2
      ) };
    }
  }
};
var schemaBytes = new DIDBytesSchema();
var didBytes = () => schemaBytes;
var readBytes2 = (input11) => schemaBytes.read(input11);
var matchBytes = (options = {}) => (
  /** @type {Schema.Schema<API.DID<Method> & API.URI<"did:">>} */
  new DIDBytesSchema(options.method)
);
var fromBytes2 = (input11) => matchBytes({}).from(input11);

// node_modules/@ucanto/core/src/schema/text.js
var text_exports = {};
__export(text_exports, {
  match: () => match5,
  read: () => read6,
  text: () => text
});
var schema5 = string();
var match5 = (options) => options ? schema5.refine(new Match(options.pattern)) : schema5;
var text = match5;
var read6 = (input11) => schema5.read(input11);
var Match = class extends API5 {
  /**
   * @param {string} source
   * @param {RegExp} pattern
   */
  readWith(source, pattern) {
    if (!pattern.test(source)) {
      return error2(
        `Expected to match ${pattern} but got "${source}" instead`
      );
    } else {
      return { ok: source };
    }
  }
};

// node_modules/@ucanto/core/src/delegation.js
var isLink2 = isLink;
var isDelegation = (proof) => !isLink(proof);
var allows = (...delegations) => {
  let allow = {};
  for (const delegation of delegations) {
    for (const { with: uri2, can, nb } of iterateCapabilities(delegation)) {
      const resource = allow[uri2] || (allow[uri2] = {});
      const abilities = resource[can] || (resource[can] = []);
      abilities.push({ ...nb });
    }
  }
  return (
    /** @type {API.InferAllowedFromDelegations<T>} */
    allow
  );
};
var iterateCapabilities = function* ({ issuer, capabilities, proofs: proofs2 }) {
  for (const own of capabilities) {
    if (own.with === "ucan:*") {
      yield {
        ...own,
        with: issuer.did()
      };
      for (const proof of proofs2) {
        if (isDelegation(proof)) {
          for (const capability2 of iterateCapabilities(proof)) {
            const can = matchAbility(capability2.can, own.can);
            if (can) {
              yield {
                ...capability2,
                can,
                // We do not know capability semantics so it is impossible
                // for us to eliminate capabilities that do not satisfy imposed
                // caveats (`own.nb`). Therefore we optimistically assume that
                // `own.nb` further constraints `capability.nb` and do a shallow
                // merge of the two. As a result we may include capabilities
                // that during validation will be considered invalid due to
                // constraint violations. While that is not ideal validator
                // will treat them as if they were omitted and therefore it
                // is a reasonable compromise.
                nb: { ...capability2.nb, ...Object(own.nb) }
              };
            }
          }
        }
      }
    } else {
      yield own;
    }
  }
};
var matchAbility = (provided, claimed) => {
  if (provided === "*") {
    return claimed;
  }
  if (claimed === "*") {
    return provided;
  }
  if (claimed.endsWith("/*") && provided.startsWith(claimed.slice(0, -1))) {
    return provided;
  }
  if (provided.endsWith("/*") && claimed.startsWith(provided.slice(0, -1))) {
    return claimed;
  }
  if (provided === claimed) {
    return provided;
  }
  return null;
};
var Delegation = class {
  /**
   * @param {API.UCANBlock<C>} root
   * @param {DAG.BlockStore} [blocks]
   */
  constructor(root2, blocks = /* @__PURE__ */ new Map()) {
    this.root = root2;
    this.blocks = blocks;
    Object.defineProperties(this, {
      blocks: {
        enumerable: false
      }
    });
  }
  /**
   * @returns {API.AttachedLinkSet}
   */
  get attachedLinks() {
    const _attachedLinks = /* @__PURE__ */ new Set();
    const ucanView = this.data;
    for (const capability2 of ucanView.capabilities) {
      const links3 = getLinksFromObject(capability2);
      for (const link6 of links3) {
        _attachedLinks.add(`${link6}`);
      }
    }
    for (const fact of ucanView.facts) {
      if (isLink(fact)) {
        _attachedLinks.add(`${fact}`);
      } else {
        const links3 = Object.values(fact).filter((e) => isLink(e));
        for (const link6 of links3) {
          _attachedLinks.add(`${link6}`);
        }
      }
    }
    return _attachedLinks;
  }
  get version() {
    return this.data.version;
  }
  get signature() {
    return this.data.signature;
  }
  get cid() {
    return this.root.cid;
  }
  link() {
    return this.root.cid;
  }
  get asCID() {
    return this.cid;
  }
  get bytes() {
    return this.root.bytes;
  }
  get data() {
    const data = decode17(this.root);
    Object.defineProperties(this, { data: { value: data, enumerable: false } });
    return data;
  }
  /**
   * Attach a block to the delegation DAG so it would be included in the
   * block iterator.
   * ⚠️ You can only attach blocks that are referenced from the `capabilities`
   * or `facts`.
   *
   * @param {API.Block} block
   */
  attach(block) {
    if (!this.attachedLinks.has(`${block.cid.link()}`)) {
      throw new Error(`given block with ${block.cid} is not an attached link`);
    }
    this.blocks.set(`${block.cid}`, block);
  }
  export() {
    return exportDAG(this.root, this.blocks, this.attachedLinks);
  }
  /**
   * @returns {API.Await<API.Result<Uint8Array, Error>>}
   */
  archive() {
    return archive(this);
  }
  iterateIPLDBlocks() {
    return exportDAG(this.root, this.blocks, this.attachedLinks);
  }
  /**
   * @type {API.Proof[]}
   */
  get proofs() {
    return proofs(this);
  }
  /**
   * @type {API.Principal}
   */
  get issuer() {
    return this.data.issuer;
  }
  /**
   * @type {API.Principal}
   */
  get audience() {
    return this.data.audience;
  }
  /**
   * @returns {C}
   */
  get capabilities() {
    return (
      /** @type {C} */
      this.data.capabilities
    );
  }
  /**
   * @returns {number}
   */
  get expiration() {
    return this.data.expiration;
  }
  /**
   * @returns {undefined|number}
   */
  get notBefore() {
    return this.data.notBefore;
  }
  /**
   * @returns {undefined|string}
   */
  get nonce() {
    return this.data.nonce;
  }
  /**
   * @returns {API.Fact[]}
   */
  get facts() {
    return this.data.facts;
  }
  /**
   * Iterate over the proofs
   *
   * @returns {IterableIterator<API.Delegation>}
   */
  iterate() {
    return it(this);
  }
  delegate() {
    return this;
  }
  buildIPLDView() {
    return this;
  }
  /**
   * @returns {API.DelegationJSON<this>}
   */
  toJSON() {
    return (
      /** @type {any} */
      {
        ...this.data.toJSON(),
        "/": this.cid.toString(),
        prf: this.proofs.map(
          (proof) => isDelegation(proof) ? proof : { "/": proof.toString() }
        )
      }
    );
  }
};
var archive = async (delegation) => {
  try {
    const store3 = /* @__PURE__ */ new Map();
    for (const block of delegation.iterateIPLDBlocks()) {
      store3.set(`${block.cid}`, block);
    }
    const variant2 = await write2({
      [`ucan@${delegation.version}`]: delegation.root.cid
    });
    store3.set(`${variant2.cid}`, variant2);
    const bytes3 = encode16({
      roots: [variant2],
      blocks: store3
    });
    return ok(bytes3);
  } catch (cause) {
    return error(
      /** @type {Error} */
      cause
    );
  }
};
var ArchiveSchema = variant({
  "ucan@0.9.1": (
    /** @type {Schema.Schema<API.UCANLink>} */
    match2({ version: 1 })
  )
});
var extract = async (archive4) => {
  try {
    const { roots, blocks } = decode16(archive4);
    const [root2] = roots;
    if (root2 == null) {
      return error2("CAR archive does not contain a root block");
    }
    const { bytes: bytes3 } = root2;
    const variant2 = decode6(bytes3);
    const [, link6] = ArchiveSchema.match(variant2);
    return ok(view2({ root: link6, blocks }));
  } catch (cause) {
    return error(
      /** @type {Error} */
      cause
    );
  }
};
var it = function* (delegation) {
  for (const proof of delegation.proofs) {
    if (isDelegation(proof)) {
      yield* it(proof);
      yield proof;
    }
  }
};
var decodeCache = /* @__PURE__ */ new WeakMap();
var decode17 = ({ bytes: bytes3 }) => {
  const data = decodeCache.get(bytes3);
  if (!data) {
    const data2 = decode15(bytes3);
    decodeCache.set(bytes3, data2);
    return data2;
  }
  return data;
};
var delegate = async ({ issuer, audience, proofs: proofs2 = [], attachedBlocks = /* @__PURE__ */ new Map(), ...input11 }, options) => {
  const links3 = [];
  const blocks = /* @__PURE__ */ new Map();
  for (const proof of proofs2) {
    if (!isDelegation(proof)) {
      links3.push(proof);
    } else {
      links3.push(proof.cid);
      for (const block of proof.export()) {
        blocks.set(block.cid.toString(), block);
      }
    }
  }
  const data = await issue({
    ...input11,
    issuer,
    audience,
    proofs: links3
  });
  const { cid, bytes: bytes3 } = await write(data, options);
  decodeCache.set(cid, data);
  const delegation = new Delegation({ cid, bytes: bytes3 }, blocks);
  Object.defineProperties(delegation, { proofs: { value: proofs2 } });
  for (const block of attachedBlocks.values()) {
    delegation.attach(block);
  }
  return delegation;
};
var exportDAG = function* (root2, blocks, attachedLinks) {
  for (const link6 of decode17(root2).proofs) {
    const root3 = (
      /** @type {UCAN.Block} */
      blocks.get(`${link6}`)
    );
    if (root3) {
      yield* exportSubDAG(root3, blocks);
    }
  }
  for (const link6 of attachedLinks.values()) {
    const block = blocks.get(link6);
    if (block) {
      yield block;
    }
  }
  yield root2;
};
var exportSubDAG = function* (root2, blocks) {
  for (const link6 of decode17(root2).proofs) {
    const root3 = (
      /** @type {UCAN.Block} */
      blocks.get(`${link6}`)
    );
    if (root3) {
      yield* exportSubDAG(root3, blocks);
    }
  }
  yield root2;
};
var importDAG = (dag) => {
  let entries3 = [];
  for (const block of dag) {
    entries3.push([block.cid.toString(), block]);
  }
  const last = entries3.pop();
  if (!last) {
    throw new RangeError("Empty DAG can not be turned into a delegation");
  } else {
    const [, root2] = last;
    return new Delegation(
      /** @type {API.UCANBlock<C>} */
      root2,
      new Map(entries3)
    );
  }
};
var create4 = ({ root: root2, blocks }) => new Delegation(root2, blocks);
var view2 = ({ root: root2, blocks }, fallback) => {
  const block = get(root2, blocks, null);
  if (block == null) {
    return fallback !== void 0 ? fallback : notFound(root2);
  }
  return create4({ root: block, blocks });
};
var proofs = (delegation) => {
  const proofs2 = [];
  const { root: root2, blocks } = delegation;
  for (const link6 of decode17(root2).proofs) {
    const root3 = (
      /** @type {UCAN.Block} */
      blocks.get(link6.toString())
    );
    proofs2.push(root3 ? create4({ root: root3, blocks }) : link6);
  }
  Object.defineProperty(delegation, "proofs", { value: proofs2 });
  return proofs2;
};
function getLinksFromObject(obj) {
  const links3 = [];
  function recurse(obj2) {
    for (const key in obj2) {
      const value = obj2[key];
      if (isLink(value)) {
        links3.push(value);
      } else if (value && typeof value === "object") {
        recurse(value);
      }
    }
  }
  recurse(obj);
  return links3;
}

// node_modules/@ucanto/core/src/invocation.js
var invocation_exports = {};
__export(invocation_exports, {
  Invocation: () => Invocation,
  create: () => create5,
  invoke: () => invoke,
  isInvocation: () => isInvocation,
  view: () => view3
});
var API11 = __toESM(require_lib(), 1);
var isInvocation = (value) => isDelegation(value);
var invoke = (options) => new IssuedInvocation(options);
var create5 = ({ root: root2, blocks }) => new Invocation(root2, blocks);
var view3 = ({ root: root2, blocks }, fallback) => {
  const block = get(root2, blocks, null);
  if (block == null) {
    return fallback !== void 0 ? fallback : notFound(root2);
  }
  return (
    /** @type {API.Invocation<C>} */
    create5({ root: block, blocks })
  );
};
var IssuedInvocation = class {
  /**
   * @param {API.InvocationOptions<Capability>} data
   */
  constructor({
    issuer,
    audience,
    capability: capability2,
    proofs: proofs2 = [],
    expiration,
    lifetimeInSeconds,
    notBefore,
    nonce,
    facts = []
  }) {
    this.issuer = issuer;
    this.audience = audience;
    this.proofs = proofs2;
    this.capabilities = [capability2];
    this.expiration = expiration;
    this.lifetimeInSeconds = lifetimeInSeconds;
    this.notBefore = notBefore;
    this.nonce = nonce;
    this.facts = facts;
    this.attachedBlocks = /* @__PURE__ */ new Map();
  }
  /**
   * @param {API.Block} block
   */
  attach(block) {
    this.attachedBlocks.set(`${block.cid}`, block);
  }
  delegate() {
    return delegate(this);
  }
  buildIPLDView() {
    return delegate(this);
  }
  /**
   * @template {API.InvocationService<Capability>} Service
   * @param {API.ConnectionView<Service>} connection
   * @returns {Promise<API.InferReceipt<Capability, Service>>}
   */
  async execute(connection7) {
    const invocation = this;
    const [result] = await connection7.execute(invocation);
    return result;
  }
};
var Invocation = class extends Delegation {
};

// node_modules/@ucanto/core/src/message.js
var message_exports = {};
__export(message_exports, {
  MessageSchema: () => MessageSchema,
  build: () => build,
  view: () => view5
});
var API13 = __toESM(require_lib(), 1);

// node_modules/@ucanto/core/src/receipt.js
var receipt_exports = {};
__export(receipt_exports, {
  issue: () => issue2,
  view: () => view4
});
var API12 = __toESM(require_lib(), 1);
var view4 = ({ root: root2, blocks }, fallback) => {
  const block = get(root2, blocks, null);
  if (block == null) {
    return fallback !== void 0 ? fallback : notFound(root2);
  }
  const data = decode6(block.bytes);
  return new Receipt({ root: { ...block, data }, store: blocks });
};
var Receipt = class {
  /**
   * @param {object} input
   * @param {Required<API.Block<API.ReceiptModel<Ok, Error, Ran>>>} input.root
   * @param {DAG.BlockStore} input.store
   * @param {API.Meta} [input.meta]
   * @param {Ran|ReturnType<Ran['link']>} [input.ran]
   * @param {API.EffectsModel} [input.fx]
   * @param {API.SignatureView<API.OutcomeModel<Ok, Error, Ran>, SigAlg>} [input.signature]
   * @param {API.UCAN.Principal} [input.issuer]
   * @param {API.Proof[]} [input.proofs]
   */
  constructor({ root: root2, store: store3, ran, issuer, signature, proofs: proofs2 }) {
    this.store = store3;
    this.root = root2;
    this._ran = ran;
    this._fx = void 0;
    this._signature = signature;
    this._proofs = proofs2;
    this._issuer = issuer;
  }
  /**
   * @returns {Ran|ReturnType<Ran['link']>}
   */
  get ran() {
    const ran = this._ran;
    if (!ran) {
      const ran2 = (
        /** @type {Ran} */
        view3(
          {
            root: this.root.data.ocm.ran,
            blocks: this.store
          },
          this.root.data.ocm.ran
        )
      );
      this._ran = ran2;
      return ran2;
    } else {
      return ran;
    }
  }
  get proofs() {
    const proofs2 = this._proofs;
    if (proofs2) {
      return proofs2;
    } else {
      const { store: store3, root: root2 } = this;
      const { prf } = root2.data.ocm;
      const proofs3 = [];
      if (prf) {
        for (const link6 of prf) {
          const proof = view2({ root: link6, blocks: store3 }, link6);
          proofs3.push(proof);
        }
      }
      this._proofs = proofs3;
      return proofs3;
    }
  }
  link() {
    return this.root.cid;
  }
  get meta() {
    return this.root.data.ocm.meta;
  }
  get issuer() {
    const issuer = this._issuer;
    if (issuer) {
      return issuer;
    } else {
      const { iss } = this.root.data.ocm;
      if (iss) {
        const issuer2 = parse2(iss);
        this._issuer = issuer2;
        return issuer2;
      }
    }
  }
  get out() {
    return this.root.data.ocm.out;
  }
  get fx() {
    let fx = this._fx;
    if (!fx) {
      const { store: blocks } = this;
      const { fork: fork5, join: join3 } = this.root.data.ocm.fx;
      fx = {
        fork: fork5.map((root2) => view3({ root: root2, blocks }, root2))
      };
      if (join3) {
        fx.join = view3({ root: join3, blocks }, join3);
      }
      this._fx = fx;
    }
    return fx;
  }
  get signature() {
    const signature = this._signature;
    if (signature) {
      return signature;
    } else {
      const signature2 = (
        /** @type {API.SignatureView<API.OutcomeModel<Ok, Error, Ran>, SigAlg>} */
        view(this.root.data.sig)
      );
      this._signature = signature2;
      return signature2;
    }
  }
  /**
   * @param {API.Crypto.Verifier} signingPrincipal
   */
  verifySignature(signingPrincipal) {
    return this.signature.verify(
      signingPrincipal,
      encode15(this.root.data.ocm)
    );
  }
  buildIPLDView() {
    return this;
  }
  *iterateIPLDBlocks() {
    const { ran, fx, proofs: proofs2, root: root2 } = this;
    yield* iterate(ran);
    for (const fork5 of fx.fork) {
      yield* iterate(fork5);
    }
    if (fx.join) {
      yield* iterate(fx.join);
    }
    for (const proof of proofs2) {
      yield* iterate(proof);
    }
    yield root2;
  }
};
var ReceptBuilder = class {
  /**
   * @param {object} options
   * @param {API.Signer<API.DID, SigAlg>} options.issuer
   * @param {Ran|ReturnType<Ran['link']>} options.ran
   * @param {API.Result<Ok, Error>} options.result
   * @param {API.Effects} [options.fx]
   * @param {API.Proof[]} [options.proofs]
   * @param {Record<string, unknown>} [options.meta]
   */
  constructor({ issuer, result, ran, fx = NOFX, proofs: proofs2 = [], meta = {} }) {
    this.issuer = issuer;
    this.result = result;
    this.ran = ran;
    this.fx = fx;
    this.proofs = proofs2;
    this.meta = meta;
  }
  async buildIPLDView({ hasher = sha2562, codec = cbor_exports2 } = {}) {
    const store3 = createStore();
    addEveryInto(iterate(this.ran), store3);
    const prf = [];
    for (const proof of this.proofs) {
      addEveryInto(iterate(proof), store3);
      prf.push(proof.link());
    }
    const fx = { fork: [] };
    for (const fork5 of this.fx.fork) {
      addEveryInto(iterate(fork5), store3);
      fx.fork.push(fork5.link());
    }
    if (this.fx.join) {
      addEveryInto(iterate(this.fx.join), store3);
      fx.join = this.fx.join.link();
    }
    const outcome = {
      ran: (
        /** @type {ReturnType<Ran['link']>} */
        this.ran.link()
      ),
      out: this.result,
      fx,
      meta: this.meta,
      iss: this.issuer.did(),
      prf
    };
    const signature = await this.issuer.sign(encode15(outcome));
    const model = {
      ocm: outcome,
      sig: signature
    };
    const root2 = await writeInto(model, store3, {
      hasher,
      codec
    });
    return new Receipt({
      root: root2,
      store: store3,
      signature,
      proofs: this.proofs,
      ran: this.ran
    });
  }
};
var NOFX = Object.freeze({ fork: Object.freeze([]) });
var issue2 = (options) => new ReceptBuilder(options).buildIPLDView();

// node_modules/@ucanto/core/src/message.js
var MessageSchema = variant({
  "ucanto/message@7.0.0": struct({
    execute: match2().array().optional(),
    delegate: dictionary({
      key: string(),
      value: (
        /** @type {API.Reader<API.Link<API.ReceiptModel>>} */
        match2()
      )
    }).array().optional()
  })
});
var build = ({ invocations, receipts }) => new MessageBuilder({ invocations, receipts }).buildIPLDView();
var view5 = ({ root: root2, store: store3 }, fallback) => {
  const block = get(root2, store3, null);
  if (block === null) {
    return fallback !== void 0 ? fallback : notFound(root2);
  }
  const data = cbor_exports2.decode(block.bytes);
  const [branch, value] = MessageSchema.match(data, fallback);
  switch (branch) {
    case "ucanto/message@7.0.0":
      return new Message({ root: { ...block, data }, store: store3 });
    default:
      return value;
  }
};
var MessageBuilder = class {
  /**
   * @param {object} source
   * @param {I} [source.invocations]
   * @param {R} [source.receipts]
   */
  constructor({ invocations, receipts }) {
    this.invocations = invocations;
    this.receipts = receipts;
  }
  /**
   *
   * @param {API.BuildOptions} [options]
   * @returns {Promise<Message<{ In: API.InferInvocations<I>, Out: R }>>}
   */
  async buildIPLDView(options) {
    const store3 = /* @__PURE__ */ new Map();
    const { invocations, ...executeField } = await writeInvocations(
      this.invocations || [],
      store3
    );
    const { receipts, ...receiptsField } = await writeReceipts(
      this.receipts || [],
      store3
    );
    const root2 = await writeInto(
      /** @type {API.AgentMessageModel<{ In: API.InferInvocations<I>, Out: R }>} */
      {
        "ucanto/message@7.0.0": {
          ...executeField,
          ...receiptsField
        }
      },
      store3,
      options
    );
    return new Message({ root: root2, store: store3 }, { receipts, invocations });
  }
};
var writeInvocations = async (run, store3) => {
  const invocations = [];
  const execute2 = [];
  for (const invocation of run) {
    const view7 = await invocation.buildIPLDView();
    execute2.push(view7.link());
    invocations.push(view7);
    for (const block of view7.iterateIPLDBlocks()) {
      store3.set(`${block.cid}`, block);
    }
  }
  return { invocations, ...execute2.length > 0 ? { execute: execute2 } : {} };
};
var writeReceipts = async (source, store3) => {
  if (source.length === 0) {
    return {};
  }
  const receipts = /* @__PURE__ */ new Map();
  const report3 = {};
  for (const [n, receipt] of source.entries()) {
    const view7 = await receipt.buildIPLDView();
    for (const block of view7.iterateIPLDBlocks()) {
      store3.set(`${block.cid}`, block);
    }
    const key = `${view7.ran.link()}`;
    if (!(key in report3)) {
      report3[key] = view7.root.cid;
      receipts.set(key, view7);
    } else {
      receipts.set(`${key}@${n}`, view7);
    }
  }
  return { receipts, report: report3 };
};
var Message = class {
  /**
   * @param {object} source
   * @param {Required<API.Block<API.AgentMessageModel<T>>>} source.root
   * @param {DAG.BlockStore} source.store
   * @param {object} build
   * @param {API.Invocation[]} [build.invocations]
   * @param {Map<string, API.Receipt>} [build.receipts]
   */
  constructor({ root: root2, store: store3 }, { invocations, receipts } = {}) {
    this.root = root2;
    this.store = store3;
    this._invocations = invocations;
    this._receipts = receipts;
  }
  *iterateIPLDBlocks() {
    for (const invocation of this.invocations) {
      yield* invocation.iterateIPLDBlocks();
    }
    for (const receipt of this.receipts.values()) {
      yield* receipt.iterateIPLDBlocks();
    }
    yield this.root;
  }
  /**
   * @template [E=never]
   * @param {API.Link} link
   * @param {E} [fallback]
   * @returns {API.Receipt|E}
   */
  get(link6, fallback) {
    const receipts = this.root.data["ucanto/message@7.0.0"].report || {};
    const receipt = receipts[`${link6}`];
    if (receipt) {
      return view4({ root: receipt, blocks: this.store });
    } else {
      return fallback !== void 0 ? fallback : panic(`Message does not include receipt for ${link6}`);
    }
  }
  get invocationLinks() {
    return this.root.data["ucanto/message@7.0.0"].execute || [];
  }
  get invocations() {
    let invocations = this._invocations;
    if (!invocations) {
      invocations = this.invocationLinks.map((link6) => {
        return invocation_exports.view({ root: link6, blocks: this.store });
      });
    }
    return invocations;
  }
  get receipts() {
    let receipts = this._receipts;
    if (!receipts) {
      receipts = /* @__PURE__ */ new Map();
      const report3 = this.root.data["ucanto/message@7.0.0"].report || {};
      for (const [key, link6] of Object.entries(report3)) {
        const receipt = view4({ root: link6, blocks: this.store });
        receipts.set(`${receipt.ran.link()}`, receipt);
      }
    }
    return receipts;
  }
};

// node_modules/@ucanto/client/src/connection.js
var connect = (options) => new Connection(options);
var Connection = class {
  /**
   * @param {API.ConnectionOptions<T>} options
   */
  constructor(options) {
    this.id = options.id;
    this.options = options;
    this.codec = options.codec;
    this.channel = options.channel;
    this.hasher = options.hasher || sha2562;
  }
  /**
   * Execute invocations.
   *
   * @template {API.Capability} C
   * @template {API.Tuple<API.ServiceInvocation<C, T>>} I
   * @param {I} invocations
   * @returns {Promise<API.InferReceipts<I, T>>}
   */
  async execute(...invocations) {
    return execute(invocations, this);
  }
};
var execute = async (invocations, connection7) => {
  const input11 = await message_exports.build({ invocations });
  const request3 = await connection7.codec.encode(input11, connection7);
  const response = await connection7.channel.request(request3);
  try {
    const output = await connection7.codec.decode(response);
    const receipts = input11.invocationLinks.map((link6) => output.get(link6));
    return (
      /** @type {API.InferReceipts<I, T>} */
      receipts
    );
  } catch (error4) {
    const { message, name: name15 = "Error", ...cause } = (
      /** @type {Error} */
      error4
    );
    const receipts = [];
    for await (const ran of input11.invocationLinks) {
      const receipt = await receipt_exports.issue({
        ran,
        result: { error: { ...cause, name: name15, message } },
        // @ts-expect-error - we can not really sign a receipt without having
        // an access to a signer which client does not have. In the future
        // we will change client API requiring a signer to be passed in but
        // for now we just use a dummy signer.
        issuer: {
          did() {
            return connection7.id.did();
          },
          sign() {
            return signature_exports.createNonStandard("", new Uint8Array());
          }
        }
      });
      receipts.push(receipt);
    }
    return (
      /** @type {API.InferReceipts<I, T>} */
      receipts
    );
  }
};

// node_modules/@ucanto/client/src/lib.js
__reExport(lib_exports2, __toESM(require_lib()));
var delegate2 = delegation_exports.delegate;

// node_modules/@ucanto/transport/src/car.js
var car_exports2 = {};
__export(car_exports2, {
  codec: () => car_exports,
  contentType: () => contentType5,
  inbound: () => inbound2,
  outbound: () => outbound2,
  request: () => request_exports,
  response: () => response_exports
});

// node_modules/@ucanto/transport/src/car/request.js
var request_exports = {};
__export(request_exports, {
  codec: () => car_exports,
  contentType: () => contentType3,
  decode: () => decode19,
  encode: () => encode17
});
var API16 = __toESM(require_lib(), 1);
var contentType3 = car_exports.contentType;
var HEADERS = Object.freeze({
  "content-type": contentType3,
  // We will signal that we want to receive a CAR file in the response
  accept: contentType3
});
var encode17 = (message, options) => {
  const blocks = /* @__PURE__ */ new Map();
  for (const block of message.iterateIPLDBlocks()) {
    blocks.set(`${block.cid}`, block);
  }
  const body = car_exports.encode({
    roots: [message.root],
    blocks
  });
  return {
    headers: (options == null ? void 0 : options.headers) || { ...HEADERS },
    body
  };
};
var decode19 = async ({ headers, body }) => {
  const { roots, blocks } = car_exports.decode(
    /** @type {Uint8Array} */
    body
  );
  const message = message_exports.view({ root: roots[0].cid, store: blocks });
  return (
    /** @type {Message} */
    message
  );
};

// node_modules/@ucanto/transport/src/car/response.js
var response_exports = {};
__export(response_exports, {
  codec: () => car_exports,
  contentType: () => contentType4,
  decode: () => decode20,
  encode: () => encode18
});
var API17 = __toESM(require_lib(), 1);
var contentType4 = car_exports.contentType;
var HEADERS2 = Object.freeze({
  "content-type": contentType4
});
var encode18 = (message, options) => {
  const blocks = /* @__PURE__ */ new Map();
  for (const block of message.iterateIPLDBlocks()) {
    blocks.set(`${block.cid}`, block);
  }
  const body = car_exports.encode({
    roots: [message.root],
    blocks
  });
  return {
    headers: { ...HEADERS2 },
    body
  };
};
var decode20 = async ({ headers, body }) => {
  const { roots, blocks } = car_exports.decode(
    /** @type {Uint8Array} */
    body
  );
  const message = message_exports.view({ root: roots[0].cid, store: blocks });
  return (
    /** @type {Message} */
    message
  );
};

// node_modules/@ucanto/transport/src/codec.js
var codec_exports = {};
__export(codec_exports, {
  formatAcceptHeader: () => formatAcceptHeader,
  formatMediaType: () => formatMediaType,
  inbound: () => inbound,
  outbound: () => outbound,
  parseAcceptHeader: () => parseAcceptHeader,
  parseMediaType: () => parseMediaType
});
var API18 = __toESM(require_lib(), 1);
var inbound = (source) => new Inbound(source);
var Inbound = class {
  /**
   * @param {API.HTTPRequest} request
   * @returns {API.Result<API.InboundAcceptCodec, API.HTTPError>} transport
   */
  accept({ headers }) {
    const contentType9 = headers["content-type"] || headers["Content-Type"];
    const decoder3 = this.decoders[contentType9];
    if (!decoder3) {
      return {
        error: {
          status: 415,
          message: `The server cannot process the request because the payload format is not supported. Please check the content-type header and try again with a supported media type.`,
          headers: {
            accept: Object.keys(this.decoders).join(", ")
          }
        }
      };
    }
    const accept3 = parseAcceptHeader(headers.accept || headers.Accept || "*/*");
    for (const { category, type: type2 } of accept3) {
      for (const encoder3 of this.encoders) {
        const select2 = (category === "*" || category === encoder3.category) && (type2 === "*" || type2 === encoder3.type);
        if (select2) {
          return { ok: { ...encoder3, decoder: decoder3 } };
        }
      }
    }
    return {
      error: {
        status: 406,
        message: `The requested resource cannot be served in the requested content type. Please specify a supported content type using the Accept header.`,
        headers: {
          accept: formatAcceptHeader(Object.values(this.encoders))
        }
      }
    };
  }
  /**
   * @param {object} source
   * @param {Record<string, API.Transport.RequestDecoder>} source.decoders
   * @param {Record<string, API.Transport.ResponseEncoder>} source.encoders
   */
  constructor({ decoders = {}, encoders = {} }) {
    this.decoders = decoders;
    if (Object.keys(decoders).length === 0) {
      throw new Error("At least one decoder MUST be provided");
    }
    this.encoders = Object.entries(encoders).map(([mediaType, encoder3]) => {
      return { ...parseMediaType(mediaType), encoder: encoder3 };
    }).sort((a, b) => b.preference - a.preference);
    if (this.encoders.length === 0) {
      throw new Error("At least one encoder MUST be provided");
    }
  }
};
var outbound = (source) => new Outbound(source);
var Outbound = class {
  /**
   * @param {object} source
   * @param {Record<string, API.Transport.RequestEncoder>} source.encoders
   * @param {Record<string, API.Transport.ResponseDecoder>} source.decoders
   */
  constructor({ decoders = {}, encoders = {} }) {
    this.decoders = decoders;
    if (Object.keys(decoders).length === 0) {
      throw new Error("At least one decoder MUST be provided");
    }
    this.encoders = Object.entries(encoders).map(([mediaType, encoder3]) => {
      return { ...parseMediaType(mediaType), encoder: encoder3 };
    }).sort((a, b) => b.preference - a.preference);
    this.acceptType = formatAcceptHeader(this.encoders);
    if (this.encoders.length === 0) {
      throw new Error("At least one encoder MUST be provided");
    }
    this.encoder = this.encoders[0].encoder;
  }
  /**
   * @template {API.AgentMessage} Message
   * @param {Message} message
   */
  encode(message) {
    return this.encoder.encode(message, {
      accept: this.acceptType
    });
  }
  /**
   * @template {API.AgentMessage} Message
   * @param {API.HTTPResponse<Message>} response
   * @returns {API.Await<Message>}
   */
  decode(response) {
    const { headers } = response;
    const contentType9 = headers["content-type"] || headers["Content-Type"];
    const decoder3 = this.decoders[contentType9] || this.decoders["*/*"];
    switch (response.status) {
      case 415:
      case 406:
        throw Object.assign(
          new RangeError(new TextDecoder().decode(response.body)),
          {
            status: response.status,
            headers: response.headers
          }
        );
    }
    if (!decoder3) {
      throw Object.assign(
        TypeError(
          `Can not decode response with content-type '${contentType9}' because no matching transport decoder is configured.`
        ),
        {
          error: true
        }
      );
    }
    return decoder3.decode(response);
  }
};
var parseMediaType = (source) => {
  const [mediaType = "*/*", mediaRange = ""] = source.trim().split(";");
  const [category = "*", type2 = "*"] = mediaType.split("/");
  const params = new URLSearchParams(mediaRange);
  const preference = parseFloat(params.get("q") || "0");
  return {
    category,
    type: type2,
    /* c8 ignore next */
    preference: isNaN(preference) ? 0 : preference
  };
};
var formatMediaType = ({ category, type: type2, preference }) => (
  /** @type {MediaType}  */
  `${category}/${type2}${preference ? `;q=${preference}` : ""}`
);
var parseAcceptHeader = (source) => source.split(",").map(parseMediaType).sort((a, b) => b.preference - a.preference);
var formatAcceptHeader = (source) => source.map(formatMediaType).join(", ");

// node_modules/@ucanto/transport/src/car.js
var contentType5 = car_exports.contentType;
var inbound2 = inbound({
  decoders: {
    [contentType3]: request_exports
  },
  encoders: {
    [contentType4]: response_exports
  }
});
var outbound2 = outbound({
  encoders: {
    [contentType3]: request_exports
  },
  decoders: {
    [contentType4]: response_exports
  }
});

// node_modules/@ucanto/transport/src/http.js
var http_exports = {};
__export(http_exports, {
  open: () => open
});
var API19 = __toESM(require_lib());
var open = ({ url, method = "POST", fetch, headers }) => {
  if (!fetch) {
    if (typeof globalThis.fetch !== "undefined") {
      fetch = globalThis.fetch.bind(globalThis);
    } else {
      throw new TypeError(
        `ucanto HTTP transport got undefined \`fetch\`. Try passing in a \`fetch\` implementation explicitly.`
      );
    }
  }
  return new Channel({ url, method, fetch, headers });
};
var Channel = class {
  /**
   * @param {object} options
   * @param {URL} options.url
   * @param {Fetcher} options.fetch
   * @param {string} [options.method]
   * @param {Record<string, string>} [options.headers]
   */
  constructor({ url, fetch, method, headers }) {
    this.fetch = fetch;
    this.method = method;
    this.url = url;
    this.headers = headers;
  }
  /**
   * @template {API.Tuple<API.ServiceInvocation<API.Capability, S>>} I
   * @param {API.HTTPRequest<API.AgentMessage<{ In: API.InferInvocations<I>, Out: API.Tuple<API.Receipt> }>>} request
   * @returns {Promise<API.HTTPResponse<API.AgentMessage<{ Out: API.InferReceipts<I, S>, In: API.Tuple<API.Invocation> }>>>}
   */
  async request({ headers, body }) {
    const response = await this.fetch(this.url.href, {
      headers: { ...this.headers, ...headers },
      body,
      method: this.method
    });
    const buffer2 = response.ok ? await response.arrayBuffer() : HTTPError.throw(`HTTP Request failed. ${this.method} ${this.url.href} → ${response.status}`, response);
    return {
      headers: response.headers.entries ? Object.fromEntries(response.headers.entries()) : (
        /* c8 ignore next */
        {}
      ),
      body: new Uint8Array(buffer2)
    };
  }
};
var HTTPError = class extends Error {
  /**
   * @param {string} message
   * @param {Options} options
   * @returns {never}
   */
  static throw(message, options) {
    throw new this(message, options);
  }
  /**
   * @param {string} message
   * @param {Options} options
   */
  constructor(message, { url, status = 500, statusText = "Server error" }) {
    super(message);
    this.name = "HTTPError";
    this.url = url;
    this.status = status;
    this.statusText = statusText;
  }
};

// node_modules/@storacha/capabilities/dist/space.js
var space_exports = {};
__export(space_exports, {
  EncryptionKeyDecrypt: () => EncryptionKeyDecrypt,
  EncryptionSetup: () => EncryptionSetup,
  Store: () => store_exports,
  allocate: () => allocate,
  contentServe: () => contentServe,
  decrypt: () => decrypt,
  egressRecord: () => egressRecord,
  info: () => info,
  space: () => space,
  top: () => top
});

// node_modules/@storacha/capabilities/dist/store.js
var store_exports = {};
__export(store_exports, {
  CARLink: () => CARLink,
  Link: () => link_exports2,
  Schema: () => schema_exports3,
  add: () => add,
  all: () => all,
  code: () => code7,
  get: () => get2,
  list: () => list,
  remove: () => remove,
  store: () => store
});

// node_modules/@ucanto/validator/src/lib.js
var API23 = __toESM(require_lib());

// node_modules/@ucanto/validator/src/capability.js
var API21 = __toESM(require_lib(), 1);

// node_modules/@ucanto/validator/src/util.js
var the = (value) => value;
var entries = (object) => (
  /** @type {any} */
  Object.entries(object)
);
var combine = ([first, ...rest]) => {
  const results = first.map((value) => [value]);
  for (const values2 of rest) {
    const tuples = results.splice(0);
    for (const value of values2) {
      for (const tuple2 of tuples) {
        results.push([...tuple2, value]);
      }
    }
  }
  return results;
};
var intersection2 = (left, right) => {
  const [result, other] = left.length < right.length ? [new Set(left), new Set(right)] : [new Set(right), new Set(left)];
  for (const item of result) {
    if (!other.has(item)) {
      result.delete(item);
    }
  }
  return [...result];
};

// node_modules/@ucanto/validator/src/error.js
var API20 = __toESM(require_lib(), 1);
var EscalatedCapability = class extends Failure {
  /**
   * @param {API.ParsedCapability} claimed
   * @param {object} delegated
   * @param {API.Failure} cause
   */
  constructor(claimed, delegated, cause) {
    super();
    this.claimed = claimed;
    this.delegated = delegated;
    this.cause = cause;
    this.name = the("EscalatedCapability");
  }
  describe() {
    return `Constraint violation: ${this.cause.message}`;
  }
};
var DelegationError = class extends Failure {
  /**
   * @param {(API.InvalidCapability | API.EscalatedDelegation | API.DelegationError)[]} causes
   * @param {object} context
   */
  constructor(causes, context2) {
    super();
    this.name = the("InvalidClaim");
    this.causes = causes;
    this.context = context2;
  }
  describe() {
    return [
      `Can not derive ${this.context} from delegated capabilities:`,
      ...this.causes.map((cause) => li2(cause.message))
    ].join("\n");
  }
  /**
   * @type {API.InvalidCapability | API.EscalatedDelegation | API.DelegationError}
   */
  get cause() {
    if (this.causes.length !== 1) {
      return this;
    } else {
      const [cause] = this.causes;
      const value = cause.name === "InvalidClaim" ? cause.cause : cause;
      Object.defineProperties(this, { cause: { value } });
      return value;
    }
  }
};
var MalformedCapability = class extends Failure {
  /**
   * @param {API.Capability} capability
   * @param {API.Failure} cause
   */
  constructor(capability2, cause) {
    super();
    this.name = the("MalformedCapability");
    this.capability = capability2;
    this.cause = cause;
  }
  describe() {
    return [
      `Encountered malformed '${this.capability.can}' capability: ${format7(
        this.capability
      )}`,
      li2(this.cause.message)
    ].join("\n");
  }
};
var UnknownCapability = class extends Failure {
  /**
   * @param {API.Capability} capability
   */
  constructor(capability2) {
    super();
    this.name = the("UnknownCapability");
    this.capability = capability2;
  }
  /* c8 ignore next 3 */
  describe() {
    return `Encountered unknown capability: ${format7(this.capability)}`;
  }
};
var format7 = (capability2, space2) => JSON.stringify(
  capability2,
  (_key, value) => {
    if (isLink(value)) {
      return value.toString();
    } else {
      return value;
    }
  },
  space2
);
var indent2 = (message, indent3 = "  ") => `${indent3}${message.split("\n").join(`
${indent3}`)}`;
var li2 = (message) => indent2(`- ${message}`);

// node_modules/@ucanto/validator/src/capability.js
var capability = ({
  derives = defaultDerives,
  nb = defaultNBSchema,
  ...etc
}) => new Capability({ derives, nb, ...etc });
var defaultNBSchema = (
  /** @type {Schema.MapRepresentation<any>} */
  schema_exports3.struct({})
);
var or3 = (left, right) => new Or(left, right);
var and2 = (...selectors) => new And(selectors);
var derive = ({ from: from18, to, derives }) => new Derive(from18, to, derives);
var View2 = class {
  /**
   * @param {API.Source} source
   * @returns {API.MatchResult<M>}
   */
  /* c8 ignore next 3 */
  match(source) {
    return { error: new UnknownCapability(source.capability) };
  }
  /**
   * @param {API.Source[]} capabilities
   * @returns {API.Select<M>}
   */
  select(capabilities) {
    return select(this, capabilities);
  }
  /**
   * @template {API.ParsedCapability} U
   * @param {object} source
   * @param {API.TheCapabilityParser<API.DirectMatch<U>>} source.to
   * @param {API.Derives<U, API.InferDeriveProof<M['value']>>} source.derives
   * @returns {API.TheCapabilityParser<API.DerivedMatch<U, M>>}
   */
  derive({ derives, to }) {
    return derive({ derives, to, from: this });
  }
};
var Unit = class extends View2 {
  /**
   * @template {API.Match} W
   * @param {API.MatchSelector<W>} other
   * @returns {API.CapabilityParser<M | W>}
   */
  or(other) {
    return or3(this, other);
  }
  /**
   * @template {API.Match} W
   * @param {API.CapabilityParser<W>} other
   * @returns {API.CapabilitiesParser<[M, W]>}
   */
  and(other) {
    return and2(
      /** @type {API.CapabilityParser<M>} */
      this,
      other
    );
  }
};
var Capability = class extends Unit {
  /**
   * @param {Required<Descriptor<A, R, C>>} descriptor
   */
  constructor(descriptor) {
    super();
    this.descriptor = descriptor;
    this.schema = schema_exports3.struct({
      can: schema_exports3.literal(descriptor.can),
      with: descriptor.with,
      nb: descriptor.nb
    });
  }
  /**
   * @param {API.InferCreateOptions<R, C>} options
   */
  create(options) {
    const { descriptor, can } = this;
    const decoders = descriptor.nb;
    const data = (
      /** @type {C} */
      options.nb || {}
    );
    const resource = descriptor.with.read(options.with);
    if (resource.error) {
      throw Object.assign(
        new Error(`Invalid 'with' - ${resource.error.message}`),
        {
          cause: resource
        }
      );
    }
    const nb = descriptor.nb.read(data);
    if (nb.error) {
      throw Object.assign(new Error(`Invalid 'nb' - ${nb.error.message}`), {
        cause: nb
      });
    }
    return createCapability({ can, with: resource.ok, nb: nb.ok });
  }
  /**
   * @param {API.InferInvokeOptions<R, C>} options
   */
  invoke({ with: with_, nb, ...options }) {
    return invoke({
      ...options,
      capability: this.create(
        /** @type {API.InferCreateOptions<R, C>} */
        { with: with_, nb }
      )
    });
  }
  /**
   * @param {API.InferDelegationOptions<R, C>} options
   * @returns {Promise<API.Delegation<[API.InferDelegatedCapability<API.ParsedCapability<A, R, C>>]>>}
   */
  async delegate({ nb: input11 = {}, with: with_, ...options }) {
    const { descriptor, can } = this;
    const readers = descriptor.nb;
    const resource = descriptor.with.read(with_);
    if (resource.error) {
      throw Object.assign(
        new Error(`Invalid 'with' - ${resource.error.message}`),
        {
          cause: resource
        }
      );
    }
    const nb = descriptor.nb.partial().read(input11);
    if (nb.error) {
      throw Object.assign(new Error(`Invalid 'nb' - ${nb.error.message}`), {
        cause: nb
      });
    }
    return delegate({
      capabilities: [createCapability({ can, with: resource.ok, nb: nb.ok })],
      ...options
    });
  }
  get can() {
    return this.descriptor.can;
  }
  /**
   * @param {API.Source} source
   * @returns {API.MatchResult<API.DirectMatch<API.ParsedCapability<A, R, C>>>}
   */
  match(source) {
    const result = parseCapability(this.descriptor, source);
    return result.error ? result : { ok: new Match2(source, result.ok, this.descriptor) };
  }
  toString() {
    return JSON.stringify({ can: this.descriptor.can });
  }
};
var createCapability = ({ can, with: with_, nb }) => (
  /** @type {API.InferCapability<T>} */
  {
    can,
    with: with_,
    ...isEmpty(nb) ? {} : { nb }
  }
);
var isEmpty = (object) => {
  for (const _ in object) {
    return false;
  }
  return true;
};
var Or = class extends Unit {
  /**
   * @param {API.Matcher<M>} left
   * @param {API.Matcher<W>} right
   */
  constructor(left, right) {
    super();
    this.left = left;
    this.right = right;
  }
  /**
   * @param {API.Source} capability
   * @return {API.MatchResult<M|W>}
   */
  match(capability2) {
    const left = this.left.match(capability2);
    if (left.error) {
      const right = this.right.match(capability2);
      if (right.error) {
        return right.error.name === "MalformedCapability" ? (
          //
          right
        ) : (
          //
          left
        );
      } else {
        return right;
      }
    } else {
      return left;
    }
  }
  toString() {
    return `${this.left.toString()}|${this.right.toString()}`;
  }
};
var And = class _And extends View2 {
  /**
   * @param {Selectors} selectors
   */
  constructor(selectors) {
    super();
    this.selectors = selectors;
  }
  /**
   * @param {API.Source} capability
   * @returns {API.MatchResult<API.Amplify<API.InferMembers<Selectors>>>}
   */
  match(capability2) {
    const group2 = [];
    for (const selector of this.selectors) {
      const result = selector.match(capability2);
      if (result.error) {
        return result;
      } else {
        group2.push(result.ok);
      }
    }
    return {
      ok: new AndMatch(
        /** @type {API.InferMembers<Selectors>} */
        group2
      )
    };
  }
  /**
   * @param {API.Source[]} capabilities
   */
  select(capabilities) {
    return selectGroup(this, capabilities);
  }
  /**
   * @template E
   * @template {API.Match} X
   * @param {API.MatchSelector<API.Match<E, X>>} other
   * @returns {API.CapabilitiesParser<[...API.InferMembers<Selectors>, API.Match<E, X>]>}
   */
  and(other) {
    return new _And([...this.selectors, other]);
  }
  toString() {
    return `[${this.selectors.map(String).join(", ")}]`;
  }
};
var Derive = class extends Unit {
  /**
   * @param {API.MatchSelector<M>} from
   * @param {API.TheCapabilityParser<API.DirectMatch<T>>} to
   * @param {API.Derives<T, API.InferDeriveProof<M['value']>>} derives
   */
  constructor(from18, to, derives) {
    super();
    this.from = from18;
    this.to = to;
    this.derives = derives;
  }
  /**
   * @type {typeof this.to['create']}
   */
  create(options) {
    return this.to.create(options);
  }
  /**
   * @type {typeof this.to['invoke']}
   */
  invoke(options) {
    return this.to.invoke(options);
  }
  /**
   * @type {typeof this.to['delegate']}
   */
  delegate(options) {
    return this.to.delegate(options);
  }
  get can() {
    return this.to.can;
  }
  /**
   * @param {API.Source} capability
   * @returns {API.MatchResult<API.DerivedMatch<T, M>>}
   */
  match(capability2) {
    const match6 = this.to.match(capability2);
    if (match6.error) {
      return match6;
    } else {
      return { ok: new DerivedMatch(match6.ok, this.from, this.derives) };
    }
  }
  toString() {
    return this.to.toString();
  }
};
var Match2 = class _Match {
  /**
   * @param {API.Source} source
   * @param {API.ParsedCapability<A, R, C>} value
   * @param {Required<Descriptor<A, R, C>>} descriptor
   */
  constructor(source, value, descriptor) {
    this.source = [source];
    this.value = value;
    this.descriptor = descriptor;
  }
  get can() {
    return this.value.can;
  }
  get proofs() {
    const proofs2 = [this.source[0].delegation];
    Object.defineProperties(this, {
      proofs: { value: proofs2 }
    });
    return proofs2;
  }
  /**
   * @param {API.CanIssue} context
   * @returns {API.DirectMatch<API.ParsedCapability<A, R, C>>|null}
   */
  prune(context2) {
    if (context2.canIssue(this.value, this.source[0].delegation.issuer.did())) {
      return null;
    } else {
      return this;
    }
  }
  /**
   * @param {API.Source[]} capabilities
   * @returns {API.Select<API.DirectMatch<API.ParsedCapability<A, R, C>>>}
   */
  select(capabilities) {
    const unknown2 = [];
    const errors = [];
    const matches = [];
    for (const capability2 of capabilities) {
      const result = resolveCapability(this.descriptor, this.value, capability2);
      if (result.ok) {
        const claim5 = this.descriptor.derives(this.value, result.ok);
        if (claim5.error) {
          errors.push(
            new DelegationError(
              [new EscalatedCapability(this.value, result.ok, claim5.error)],
              this
            )
          );
        } else {
          matches.push(new _Match(capability2, result.ok, this.descriptor));
        }
      } else {
        switch (result.error.name) {
          case "UnknownCapability":
            unknown2.push(result.error.capability);
            break;
          case "MalformedCapability":
          default:
            errors.push(new DelegationError([result.error], this));
        }
      }
    }
    return { matches, unknown: unknown2, errors };
  }
  toString() {
    const { nb } = this.value;
    return JSON.stringify({
      can: this.descriptor.can,
      with: this.value.with,
      nb: nb && Object.keys(nb).length > 0 ? nb : void 0
    });
  }
};
var DerivedMatch = class _DerivedMatch {
  /**
   * @param {API.DirectMatch<T>} selected
   * @param {API.MatchSelector<M>} from
   * @param {API.Derives<T, API.InferDeriveProof<M['value']>>} derives
   */
  constructor(selected, from18, derives) {
    this.selected = selected;
    this.from = from18;
    this.derives = derives;
  }
  get can() {
    return this.value.can;
  }
  get source() {
    return this.selected.source;
  }
  get proofs() {
    const proofs2 = [];
    for (const { delegation } of this.selected.source) {
      proofs2.push(delegation);
    }
    Object.defineProperties(this, { proofs: { value: proofs2 } });
    return proofs2;
  }
  get value() {
    return this.selected.value;
  }
  /**
   * @param {API.CanIssue} context
   */
  prune(context2) {
    const selected = (
      /** @type {API.DirectMatch<T>|null} */
      this.selected.prune(context2)
    );
    return selected ? new _DerivedMatch(selected, this.from, this.derives) : null;
  }
  /**
   * @param {API.Source[]} capabilities
   */
  select(capabilities) {
    const { derives, selected, from: from18 } = this;
    const { value } = selected;
    const direct = selected.select(capabilities);
    const derived = from18.select(capabilities);
    const matches = [];
    const errors = [];
    for (const match6 of derived.matches) {
      const result = derives(value, match6.value);
      if (result.error) {
        errors.push(
          new DelegationError(
            [new EscalatedCapability(value, match6.value, result.error)],
            this
          )
        );
      } else {
        matches.push(match6);
      }
    }
    return {
      unknown: intersection2(direct.unknown, derived.unknown),
      errors: [
        ...errors,
        ...direct.errors,
        ...derived.errors.map((error4) => new DelegationError([error4], this))
      ],
      matches: [
        ...direct.matches.map((match6) => new _DerivedMatch(match6, from18, derives)),
        ...matches
      ]
    };
  }
  toString() {
    return this.selected.toString();
  }
};
var AndMatch = class _AndMatch {
  /**
   * @param {API.Match[]} matches
   */
  constructor(matches) {
    this.matches = matches;
  }
  get selectors() {
    return this.matches;
  }
  /**
   * @returns {API.Source[]}
   */
  get source() {
    const source = [];
    for (const match6 of this.matches) {
      source.push(...match6.source);
    }
    Object.defineProperties(this, { source: { value: source } });
    return source;
  }
  /**
   * @param {API.CanIssue} context
   */
  prune(context2) {
    const matches = [];
    for (const match6 of this.matches) {
      const pruned = match6.prune(context2);
      if (pruned) {
        matches.push(pruned);
      }
    }
    return matches.length === 0 ? null : new _AndMatch(matches);
  }
  get proofs() {
    const proofs2 = [];
    for (const { delegation } of this.source) {
      proofs2.push(delegation);
    }
    Object.defineProperties(this, { proofs: { value: proofs2 } });
    return proofs2;
  }
  /**
   * @type {API.InferValue<API.InferMembers<Selectors>>}
   */
  get value() {
    const value = [];
    for (const match6 of this.matches) {
      value.push(match6.value);
    }
    Object.defineProperties(this, { value: { value } });
    return (
      /** @type {any} */
      value
    );
  }
  /**
   * @param {API.Source[]} capabilities
   */
  select(capabilities) {
    return selectGroup(this, capabilities);
  }
  toString() {
    return `[${this.matches.map((match6) => match6.toString()).join(", ")}]`;
  }
};
var resolveAbility = (pattern, can, fallback) => {
  switch (pattern) {
    case can:
    case "*":
      return can;
    default:
      return pattern.endsWith("/*") && can.startsWith(pattern.slice(0, -1)) ? can : fallback;
  }
};
var resolveResource = (source, uri2, fallback) => {
  switch (source) {
    case uri2:
    case "ucan:*":
      return uri2;
    default:
      return fallback;
  }
};
var parseCapability = (descriptor, source) => {
  const { delegation } = source;
  const capability2 = (
    /** @type {API.Capability<A, R, C>} */
    source.capability
  );
  if (descriptor.can !== capability2.can) {
    return { error: new UnknownCapability(capability2) };
  }
  const uri2 = descriptor.with.read(capability2.with);
  if (uri2.error) {
    return { error: new MalformedCapability(capability2, uri2.error) };
  }
  const nb = descriptor.nb.read(capability2.nb || {});
  if (nb.error) {
    return { error: new MalformedCapability(capability2, nb.error) };
  }
  return { ok: new CapabilityView(descriptor.can, uri2.ok, nb.ok, delegation) };
};
var resolveCapability = (descriptor, claimed, { capability: capability2, delegation }) => {
  const can = resolveAbility(capability2.can, claimed.can, null);
  if (can == null) {
    return { error: new UnknownCapability(capability2) };
  }
  const resource = resolveResource(
    capability2.with,
    claimed.with,
    capability2.with
  );
  const uri2 = descriptor.with.read(resource);
  if (uri2.error) {
    return { error: new MalformedCapability(capability2, uri2.error) };
  }
  const nb = descriptor.nb.read({
    ...claimed.nb,
    ...capability2.nb
  });
  if (nb.error) {
    return { error: new MalformedCapability(capability2, nb.error) };
  }
  return { ok: new CapabilityView(can, uri2.ok, nb.ok, delegation) };
};
var CapabilityView = class {
  /**
   * @param {A} can
   * @param {R} with_
   * @param {C} nb
   * @param {API.Delegation} delegation
   */
  constructor(can, with_, nb, delegation) {
    this.can = can;
    this.with = with_;
    this.delegation = delegation;
    this.nb = nb;
  }
};
var select = (matcher, capabilities) => {
  const unknown2 = [];
  const matches = [];
  const errors = [];
  for (const capability2 of capabilities) {
    const result = matcher.match(capability2);
    if (result.error) {
      switch (result.error.name) {
        case "UnknownCapability":
          unknown2.push(result.error.capability);
          break;
        case "MalformedCapability":
        default:
          errors.push(new DelegationError([result.error], result.error.capability));
      }
    } else {
      matches.push(result.ok);
    }
  }
  return { matches, errors, unknown: unknown2 };
};
var selectGroup = (self2, capabilities) => {
  let unknown2;
  const data = [];
  const errors = [];
  for (const selector of self2.selectors) {
    const selected = selector.select(capabilities);
    unknown2 = unknown2 ? intersection2(unknown2, selected.unknown) : selected.unknown;
    for (const error4 of selected.errors) {
      errors.push(new DelegationError([error4], self2));
    }
    data.push(selected.matches);
  }
  const matches = combine(data).map((group2) => new AndMatch(group2));
  return {
    unknown: (
      /* c8 ignore next */
      unknown2 || []
    ),
    errors,
    matches
  };
};
var defaultDerives = (claimed, delegated) => {
  if (delegated.with.endsWith("*")) {
    if (!claimed.with.startsWith(delegated.with.slice(0, -1))) {
      return schema_exports3.error(
        `Resource ${claimed.with} does not match delegated ${delegated.with} `
      );
    }
  } else if (delegated.with !== claimed.with) {
    return schema_exports3.error(
      `Resource ${claimed.with} is not contained by ${delegated.with}`
    );
  }
  const caveats = delegated.nb || {};
  const nb = claimed.nb || {};
  const kv = entries(caveats);
  for (const [name15, value] of kv) {
    if (nb[name15] != value) {
      return schema_exports3.error(`${String(name15)}: ${nb[name15]} violates ${value}`);
    }
  }
  return { ok: true };
};

// node_modules/@ucanto/validator/src/authorization.js
var API22 = __toESM(require_lib(), 1);

// node_modules/@storacha/capabilities/dist/utils.js
var API24 = __toESM(require_lib(), 1);
var ProviderDID = did_exports2.match({ method: "web" });
var SpaceDID = did_exports2.match({ method: "key" });
var AccountDID = did_exports2.match({ method: "mailto" });
var Await = schema_exports3.struct({
  "ucan/await": schema_exports3.tuple([schema_exports3.string(), schema_exports3.link()])
});
function equalWith(child, parent) {
  return child.with === parent.with ? ok({}) : fail2(`Can not derive ${child.can} with ${child.with} from ${parent.with}`);
}
function equal(child, parent, constraint) {
  if (parent === void 0 || parent === "*") {
    return ok({});
  } else if (String(child) === String(parent)) {
    return ok({});
  } else {
    return fail2(`Constraint violation: ${child} violates imposed ${constraint} constraint ${parent}`);
  }
}
var equalLink = (claimed, delegated) => {
  if (claimed.with !== delegated.with) {
    return fail2(`Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`);
  } else if (delegated.nb.link && `${delegated.nb.link}` !== `${claimed.nb.link}`) {
    return fail2(`Link ${claimed.nb.link ? `${claimed.nb.link}` : ""} violates imposed ${delegated.nb.link} constraint.`);
  } else {
    return ok({});
  }
};
var toDigestBytes = (linkOrDigest2) => "multihash" in linkOrDigest2 ? linkOrDigest2.multihash.bytes : linkOrDigest2.digest;
var equalLinkOrDigestContent = (claimed, delegated) => {
  if (delegated.nb.content) {
    const delegatedBytes = toDigestBytes(delegated.nb.content);
    if (!claimed.nb.content) {
      return fail2(`Constraint violation: undefined violates imposed content constraint ${base58btc.encode(delegatedBytes)}`);
    }
    const claimedBytes = toDigestBytes(claimed.nb.content);
    if (!equals(claimedBytes, delegatedBytes)) {
      return fail2(`Constraint violation: ${base58btc.encode(claimedBytes)} violates imposed content constraint ${base58btc.encode(delegatedBytes)}`);
    }
  }
  return ok({});
};
var equalBlob = (claimed, delegated) => {
  if (claimed.with !== delegated.with) {
    return fail2(`Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`);
  } else if (delegated.nb.blob.digest && !equals(delegated.nb.blob.digest, claimed.nb.blob.digest)) {
    return fail2(`Link ${claimed.nb.blob.digest ? `${claimed.nb.blob.digest}` : ""} violates imposed ${delegated.nb.blob.digest} constraint.`);
  } else if (claimed.nb.blob.size !== void 0 && delegated.nb.blob.size !== void 0) {
    return claimed.nb.blob.size > delegated.nb.blob.size ? fail2(`Size constraint violation: ${claimed.nb.blob.size} > ${delegated.nb.blob.size}`) : ok({});
  } else {
    return ok({});
  }
};
var equalBody = (claimed, delegated) => {
  if (claimed.with !== delegated.with) {
    return fail2(`Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`);
  } else if (delegated.nb.body.digest && !equals(delegated.nb.body.digest, claimed.nb.body.digest)) {
    return fail2(`Link ${claimed.nb.body.digest ? `${claimed.nb.body.digest}` : ""} violates imposed ${delegated.nb.body.digest} constraint.`);
  } else if (claimed.nb.body.size !== void 0 && delegated.nb.body.size !== void 0) {
    return claimed.nb.body.size !== delegated.nb.body.size ? fail2(`Size constraint violation: ${claimed.nb.body.size} !== ${delegated.nb.body.size}`) : ok({});
  } else {
    return ok({});
  }
};
var checkLink = (claimed, imposed, at2) => {
  return equal(String(claimed), imposed === void 0 ? void 0 : String(imposed), at2);
};
var and3 = (result) => result.error ? result : void 0;
function parseAbility(ability11) {
  const [namespace, ...segments] = ability11.split("/");
  return { namespace, segments };
}
function canDelegateAbility(parent, child) {
  const parsedParent = parseAbility(parent);
  const parsedChild = parseAbility(child);
  if (parsedParent.namespace === "*" && parsedParent.segments.length === 0) {
    return true;
  }
  if (parsedChild.namespace === "*" && parsedChild.segments.length === 0) {
    return false;
  }
  if (parsedParent.namespace !== parsedChild.namespace) {
    return false;
  }
  if (parsedParent.segments[0] === "*") {
    return true;
  }
  if (parsedParent.segments.length !== parsedChild.segments.length) {
    return false;
  }
  return parsedParent.segments.reduce((acc, v, i) => acc && parsedChild.segments[i] === v, true);
}

// node_modules/@storacha/capabilities/dist/store.js
var code7 = 514;
var CARLink = schema_exports3.link({ code: code7, version: 1 });
var store = capability({
  can: "store/*",
  /**
   * DID of the (memory) space where CAR is intended to
   * be stored.
   */
  with: SpaceDID,
  derives: equalWith
});
var add = capability({
  can: "store/add",
  /**
   * DID of the (memory) space where CAR is intended to
   * be stored.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * CID of the CAR file to be stored. Service will provision write target
     * for this exact CAR file for agent to PUT or POST it. Attempt to write
     * any other content will fail.
     */
    link: CARLink,
    /**
     * Size of the CAR file to be stored. Service will provision write target
     * for this exact size. Attempt to write a larger CAR file will fail.
     */
    size: schema_exports3.integer(),
    /**
     * Agent may optionally provide a link to a related CAR file using `origin`
     * field. This is useful when storing large DAGs, agent could shard it
     * across multiple CAR files and then link each shard with a previous one.
     *
     * Providing this relation tells service that given CAR is shard of the
     * larger DAG as opposed to it being intentionally partial DAG. When DAG is
     * not sharded, there will be only one `store/add` with `origin` left out.
     */
    origin: link_exports2.optional()
  }),
  derives: (claim5, from18) => {
    const result = equalLink(claim5, from18);
    if (result.error) {
      return result;
    } else if (claim5.nb.size !== void 0 && from18.nb.size !== void 0) {
      return claim5.nb.size > from18.nb.size ? fail2(`Size constraint violation: ${claim5.nb.size} > ${from18.nb.size}`) : ok({});
    } else {
      return ok({});
    }
  }
});
var get2 = capability({
  can: "store/get",
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * shard CID to fetch info about.
     */
    link: CARLink.optional()
  }),
  derives: equalLink
});
var remove = capability({
  can: "store/remove",
  /**
   * DID of the (memory) space where CAR is intended to
   * be stored.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * CID of the CAR file to be removed from the store.
     */
    link: CARLink
  }),
  derives: equalLink
});
var list = capability({
  can: "store/list",
  /**
   * DID of the (memory) space where CAR is intended to
   * be stored.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * A pointer that can be moved back and forth on the list.
     * It can be used to paginate a list for instance.
     */
    cursor: schema_exports3.string().optional(),
    /**
     * Maximum number of items per page.
     */
    size: schema_exports3.integer().optional(),
    /**
     * If true, return page of results preceding cursor. Defaults to false.
     */
    pre: schema_exports3.boolean().optional()
  }),
  derives: (claimed, delegated) => {
    if (claimed.with !== delegated.with) {
      return fail2(`Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`);
    }
    return ok({});
  }
});
var all = add.or(remove).or(list);

// node_modules/@storacha/capabilities/dist/upload.js
var upload_exports = {};
__export(upload_exports, {
  Link: () => link_exports2,
  Schema: () => schema_exports3,
  add: () => add2,
  all: () => all2,
  get: () => get3,
  list: () => list2,
  remove: () => remove2,
  upload: () => upload
});
var upload = capability({
  can: "upload/*",
  /**
   * DID of the (memory) space where upload is add to the
   * upload list.
   */
  with: SpaceDID,
  derives: equalWith
});
var CARLink2 = link_exports2.match({ code: car_exports.code, version: 1 });
var add2 = capability({
  can: "upload/add",
  /**
   * DID of the (memory) space where uploaded is added.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * Root CID of the DAG to be added to the upload list.
     */
    root: link_exports2,
    /**
     * CIDs to the CAR files that contain blocks of the DAG.
     */
    shards: CARLink2.array().optional()
  }),
  derives: (self2, from18) => {
    return and3(equalWith(self2, from18)) || and3(equal(self2.nb.root, from18.nb.root, "root")) || and3(equal(self2.nb.shards, from18.nb.shards, "shards")) || ok({});
  }
});
var get3 = capability({
  can: "upload/get",
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * Root CID of the DAG to fetch upload info about.
     */
    root: link_exports2.optional()
  }),
  derives: (self2, from18) => {
    const res = equalWith(self2, from18);
    if (res.error) {
      return res;
    }
    if (!from18.nb.root) {
      return res;
    }
    return equal(self2.nb.root, from18.nb.root, "root");
  }
});
var remove2 = capability({
  can: "upload/remove",
  /**
   * DID of the (memory) space where uploaded is removed from.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * Root CID of the DAG to be removed from the upload list.
     */
    root: link_exports2
  }),
  derives: (self2, from18) => {
    return and3(equalWith(self2, from18)) || and3(equal(self2.nb.root, from18.nb.root, "root")) || ok({});
  }
});
var list2 = capability({
  can: "upload/list",
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * A pointer that can be moved back and forth on the list.
     * It can be used to paginate a list for instance.
     */
    cursor: schema_exports3.string().optional(),
    /**
     * Maximum number of items per page.
     */
    size: schema_exports3.integer().optional(),
    /**
     * If true, return page of results preceding cursor. Defaults to false.
     */
    pre: schema_exports3.boolean().optional()
  })
});
var all2 = add2.or(remove2).or(list2);

// node_modules/@storacha/capabilities/dist/top.js
var top = capability({
  can: "*",
  with: schema_exports3.or(schema_exports3.did(), schema_exports3.literal("ucan:*")),
  derives: equalWith
});

// node_modules/@storacha/capabilities/dist/space.js
var space = capability({
  can: "space/*",
  with: SpaceDID,
  derives: equalWith
});
var info = add.or(list).or(remove).or(add2).or(list2).or(remove2).derive({
  to: capability({
    can: "space/info",
    with: SpaceDID
  }),
  derives: equalWith
});
var allocate = capability({
  can: "space/allocate",
  with: SpaceDID,
  nb: schema_exports3.struct({
    size: schema_exports3.integer()
  }),
  derives: (child, parent) => {
    const result = equalWith(child, parent);
    if (result.ok) {
      return child.nb.size <= parent.nb.size ? ok({}) : fail2(`Claimed size ${child.nb.size} escalates delegated size ${parent.nb.size}`);
    } else {
      return result;
    }
  }
});
var contentServe = capability({
  can: "space/content/serve/*",
  with: SpaceDID,
  derives: equalWith
});
var egressRecord = capability({
  can: "space/content/serve/egress/record",
  with: SpaceDID,
  nb: schema_exports3.struct({
    /** CID of the resource that was served. */
    resource: schema_exports3.link(),
    /** Amount of bytes served. */
    bytes: schema_exports3.integer().greaterThan(0),
    /** Timestamp of the event in milliseconds after Unix epoch. */
    servedAt: schema_exports3.integer().greaterThan(-1)
  }),
  derives: equalWith
});
var decrypt = capability({
  can: "space/content/decrypt",
  with: SpaceDID,
  nb: schema_exports3.struct({
    resource: schema_exports3.link()
  }),
  derives: (child, parent) => {
    if (child.with !== parent.with) {
      return fail2(`Can not derive ${child.can} with ${child.with} from ${parent.with}`);
    }
    if (child.nb.resource.toString() !== parent.nb.resource.toString()) {
      return fail2(`Can not derive ${child.can} resource ${child.nb.resource} from ${parent.nb.resource}`);
    }
    return ok({});
  }
});
var EncryptionSetup = capability({
  can: "space/encryption/setup",
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * The location of the KMS key to use for encryption. If not provided, the Storacha Key Manager will use the default location.
     */
    location: schema_exports3.string().optional(),
    /**
     * The keyring of the KMS key to use for encryption. If not provided, the Storacha Key Manager will use the default keyring.
     */
    keyring: schema_exports3.string().optional()
  }),
  derives: (child, parent) => {
    if (child.with !== parent.with) {
      return fail2(`Can not derive ${child.can} with ${child.with} from ${parent.with}`);
    }
    if (child.nb.location !== parent.nb.location) {
      return fail2(`Can not derive ${child.can} location ${child.nb.location} from ${parent.nb.location}`);
    }
    if (child.nb.keyring !== parent.nb.keyring) {
      return fail2(`Can not derive ${child.can} keyring ${child.nb.keyring} from ${parent.nb.keyring}`);
    }
    return ok({});
  }
});
var EncryptionKeyDecrypt = capability({
  can: "space/encryption/key/decrypt",
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * The encrypted symmetric key to be decrypted
     */
    key: schema_exports3.bytes()
  }),
  derives: (child, parent) => {
    if (child.with !== parent.with) {
      return fail2(`Can not derive ${child.can} with ${child.with} from ${parent.with}`);
    }
    if (child.nb.key !== parent.nb.key) {
      return fail2(`Can not derive ${child.can} key ${child.nb.key} from ${parent.nb.key}`);
    }
    return ok({});
  }
});

// node_modules/@storacha/capabilities/dist/ucan.js
var ucan_exports2 = {};
__export(ucan_exports2, {
  UCANLink: () => UCANLink,
  attest: () => attest,
  conclude: () => conclude,
  revoke: () => revoke,
  ucan: () => ucan
});
var API25 = __toESM(require_lib(), 1);
var UCANLink = (
  /** @type {Schema.Schema<API.UCANLink, unknown>} */
  schema_exports3.link({ version: 1 })
);
var ucan = capability({
  can: "ucan/*",
  with: schema_exports3.did(),
  derives: equalWith
});
var revoke = capability({
  can: "ucan/revoke",
  /**
   * DID of the principal authorizing revocation.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * UCAN being revoked from all proof chains that lead to the UCAN that is
     * either issued (iss) by or delegated to (aud) the principal identified
     * by the `with` field.
     */
    ucan: UCANLink,
    /**
     * Proof chain illustrating the path from revoked UCAN to the one that is
     * either issued (iss) by or delegated to (aud) the principal identified
     * by the `with` field.
     *
     * If the UCAN being revoked is either issued (iss) by or delegated to (aud)
     * the principal identified by the `with` field no `proof` is required and
     * it can be omitted or set to an empty array.
     *
     * It is RECOMMENDED that `proof` is provided in all other cases otherwise
     * it MAY not be possible to verify that revoking principal is a participant
     * in the proof chain.
     */
    proof: UCANLink.array().optional()
  }),
  derives: (claim5, from18) => (
    // With field MUST be the same
    and3(equalWith(claim5, from18)) ?? // UCAN being revoked MUST be the same
    and3(checkLink(claim5.nb.ucan, from18.nb.ucan, "nb.ucan")) ?? // And proof chain MUST be the same
    equal((claim5.nb.proof ?? []).join("/"), (from18.nb.proof ?? []).join("/"), "nb.proof")
  )
});
var conclude = capability({
  can: "ucan/conclude",
  /**
   * DID of the principal representing the Conclusion Authority.
   * MUST be the DID of the audience of the ran invocation.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * CID of the content with the Receipt.
     */
    receipt: schema_exports3.link()
  }),
  derives: (claim5, from18) => (
    // With field MUST be the same
    and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.receipt, from18.nb.receipt, "nb.receipt")) || ok({})
  )
});
var attest = capability({
  can: "ucan/attest",
  // Should be storacha.network DID
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    // UCAN delegation that is being attested.
    proof: schema_exports3.link({ version: 1 })
  }),
  derives: (claim5, from18) => (
    // With field MUST be the same
    and3(equalWith(claim5, from18)) ?? // UCAN link MUST be the same
    checkLink(claim5.nb.proof, from18.nb.proof, "nb.proof")
  )
});

// node_modules/@storacha/access/dist/access.js
var access_exports2 = {};
__export(access_exports2, {
  GrantedAccess: () => GrantedAccess,
  accountAccess: () => accountAccess,
  claim: () => claim2,
  createPendingAccessRequest: () => createPendingAccessRequest,
  delegate: () => delegate4,
  request: () => request,
  spaceAccess: () => spaceAccess,
  toCapabilities: () => toCapabilities
});

// node_modules/@storacha/capabilities/dist/access.js
var access_exports = {};
__export(access_exports, {
  Account: () => Account,
  AuthorizationRequest: () => AuthorizationRequest,
  CapabilityRequest: () => CapabilityRequest,
  access: () => access,
  authorize: () => authorize,
  claim: () => claim,
  confirm: () => confirm,
  delegate: () => delegate3,
  session: () => session,
  top: () => top
});
var Types2 = __toESM(require_lib(), 1);
var session = attest;
var Account = did_exports2.match({ method: "mailto" });
var CapabilityRequest = schema_exports3.struct({
  /**
   * If set to `"*"` it corresponds to "sudo" access.
   */
  can: schema_exports3.string()
});
var AuthorizationRequest = schema_exports3.struct({
  /**
   * DID of the Account authorization is requested from.
   */
  iss: Account.optional(),
  /**
   * Capabilities agent wishes to be granted.
   */
  att: CapabilityRequest.array()
});
var access = capability({
  can: "access/*",
  with: uri_exports.match({ protocol: "did:" })
});
var authorize = capability({
  can: "access/authorize",
  with: did_exports2.match({ method: "key" }),
  /**
   * Authorization request describing set of desired capabilities
   */
  nb: AuthorizationRequest,
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.iss, parent.nb.iss, "iss")) || and3(subsetCapabilities(child.nb.att, parent.nb.att)) || ok({});
  }
});
var confirm = capability({
  can: "access/confirm",
  with: did_exports2,
  nb: schema_exports3.struct({
    /**
     * Link to the `access/authorize` request that this delegation was created
     * for.
     */
    cause: schema_exports3.link({ version: 1 }),
    iss: Account,
    aud: schema_exports3.did(),
    att: CapabilityRequest.array()
  }),
  derives: (claim5, proof) => {
    return and3(equalWith(claim5, proof)) || and3(equal(claim5.nb.iss, proof.nb.iss, "iss")) || and3(equal(claim5.nb.aud, proof.nb.aud, "aud")) || and3(subsetCapabilities(claim5.nb.att, proof.nb.att)) || and3(checkLink(claim5.nb.cause, proof.nb.cause, "nb.cause")) || ok({});
  }
});
var claim = capability({
  can: "access/claim",
  with: did_exports2.match({ method: "key" }).or(did_exports2.match({ method: "mailto" }))
});
var delegate3 = capability({
  can: "access/delegate",
  /**
   * Field MUST be a space DID with a storage provider. Delegation will be stored just like any other DAG stored using store/add capability.
   *
   * @see https://github.com/storacha/specs/blob/main/w3-access.md#delegate-with
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    // keys SHOULD be CIDs, but we won't require it in the schema
    /**
     * @type {Schema.Schema<AccessDelegateDelegations>}
     */
    delegations: schema_exports3.dictionary({
      value: schema_exports3.Link.match()
    })
  }),
  derives: (claim5, proof) => {
    return and3(equalWith(claim5, proof)) || and3(subsetsNbDelegations(claim5, proof)) || ok({});
  }
});
function subsetsNbDelegations(claim5, proof) {
  const missingProofs = setDifference(delegatedCids(claim5), new Set(delegatedCids(proof)));
  if (missingProofs.size > 0) {
    return fail2(`unauthorized nb.delegations ${[...missingProofs].join(", ")}`);
  }
  return ok({});
}
var subsetCapabilities = (claim5, proof) => {
  const allowed = new Set(proof.map((p) => p.can));
  if (allowed.has("*")) {
    return ok({});
  }
  const escalated = setDifference(claim5.map((c) => c.can), allowed);
  if (escalated.size > 0) {
    return fail2(`unauthorized nb.att.can ${[...escalated].join(", ")}`);
  }
  return ok({});
};
function* delegatedCids(delegate6) {
  for (const d of Object.values(delegate6.nb.delegations || {})) {
    yield d.toString();
  }
}
function setDifference(minuend, subtrahend) {
  const difference = /* @__PURE__ */ new Set();
  for (const e of minuend) {
    if (!subtrahend.has(e)) {
      difference.add(e);
    }
  }
  return difference;
}

// node_modules/@storacha/access/dist/types.js
var types_exports = {};
__export(types_exports, {
  AppName: () => AppName,
  email: () => email,
  fromEmail: () => fromEmail,
  fromString: () => fromString3,
  toEmail: () => toEmail
});
__reExport(types_exports, __toESM(require_lib(), 1));

// node_modules/@storacha/did-mailto/dist/index.js
var dist_exports = {};
__export(dist_exports, {
  email: () => email,
  fromEmail: () => fromEmail,
  fromString: () => fromString3,
  toEmail: () => toEmail
});
function fromEmail(email2) {
  const { domain, local } = parseEmail(email2);
  const did2 = (
    /** @type {const} */
    `did:mailto:${encodeURIComponent(domain)}:${encodeURIComponent(local)}`
  );
  return did2;
}
function toEmail(did2) {
  const parts = did2.split(":");
  if (parts[1] !== "mailto") {
    throw new Error(`DID ${did2} is not a mailto did.`);
  }
  return `${decodeURIComponent(parts[3])}@${decodeURIComponent(parts[2])}`;
}
function email(input11) {
  const { domain, local } = parseEmail(input11);
  const emailAddress = `${local}@${domain}`;
  return emailAddress;
}
function fromString3(input11) {
  const colonParts = input11.split(":");
  if (colonParts.length !== 4) {
    throw new TypeError(`expected did:mailto to have 4 colon-delimited segments, but got ${colonParts.length}`);
  }
  const [domain, local] = [colonParts[2], colonParts[3]];
  return `did:mailto:${domain}:${local}`;
}
function parseEmail(email2) {
  const atParts = email2.split("@");
  if (atParts.length < 2) {
    throw new TypeError(`expected at least 2 @-delimited segments, but got ${atParts.length}`);
  }
  const domain = atParts.at(-1) ?? "";
  const local = atParts.slice(0, -1).join("@");
  return { domain, local };
}

// node_modules/@storacha/access/dist/types.js
var AppName;
(function(AppName2) {
  AppName2["BskyBackups"] = "bsky-backups";
  AppName2["TGMiniapp"] = "tg-miniapp";
  AppName2["Console"] = "console";
})(AppName || (AppName = {}));

// node_modules/uint8arrays/dist/src/equals.js
function equals3(a, b) {
  if (a === b) {
    return true;
  }
  if (a.byteLength !== b.byteLength) {
    return false;
  }
  for (let i = 0; i < a.byteLength; i++) {
    if (a[i] !== b[i]) {
      return false;
    }
  }
  return true;
}

// node_modules/uint8arrays/dist/src/alloc.js
function alloc2(size5 = 0) {
  return new Uint8Array(size5);
}
function allocUnsafe(size5 = 0) {
  return new Uint8Array(size5);
}

// node_modules/uint8arrays/dist/src/util/as-uint8array.js
function asUint8Array(buf2) {
  return buf2;
}

// node_modules/uint8arrays/dist/src/compare.js
function compare2(a, b) {
  for (let i = 0; i < a.byteLength; i++) {
    if (a[i] < b[i]) {
      return -1;
    }
    if (a[i] > b[i]) {
      return 1;
    }
  }
  if (a.byteLength > b.byteLength) {
    return 1;
  }
  if (a.byteLength < b.byteLength) {
    return -1;
  }
  return 0;
}

// node_modules/uint8arrays/dist/src/concat.js
function concat2(arrays, length2) {
  if (length2 == null) {
    length2 = arrays.reduce((acc, curr) => acc + curr.length, 0);
  }
  const output = allocUnsafe(length2);
  let offset2 = 0;
  for (const arr of arrays) {
    output.set(arr, offset2);
    offset2 += arr.length;
  }
  return asUint8Array(output);
}

// node_modules/multiformats/dist/src/bases/base10.js
var base10_exports = {};
__export(base10_exports, {
  base10: () => base10
});
var base10 = baseX({
  prefix: "9",
  name: "base10",
  alphabet: "0123456789"
});

// node_modules/multiformats/dist/src/bases/base16.js
var base16_exports = {};
__export(base16_exports, {
  base16: () => base16,
  base16upper: () => base16upper
});
var base16 = rfc4648({
  prefix: "f",
  name: "base16",
  alphabet: "0123456789abcdef",
  bitsPerChar: 4
});
var base16upper = rfc4648({
  prefix: "F",
  name: "base16upper",
  alphabet: "0123456789ABCDEF",
  bitsPerChar: 4
});

// node_modules/multiformats/dist/src/bases/base2.js
var base2_exports = {};
__export(base2_exports, {
  base2: () => base2
});
var base2 = rfc4648({
  prefix: "0",
  name: "base2",
  alphabet: "01",
  bitsPerChar: 1
});

// node_modules/multiformats/dist/src/bases/base256emoji.js
var base256emoji_exports = {};
__export(base256emoji_exports, {
  base256emoji: () => base256emoji
});
var alphabet = Array.from("🚀🪐☄🛰🌌🌑🌒🌓🌔🌕🌖🌗🌘🌍🌏🌎🐉☀💻🖥💾💿😂❤😍🤣😊🙏💕😭😘👍😅👏😁🔥🥰💔💖💙😢🤔😆🙄💪😉☺👌🤗💜😔😎😇🌹🤦🎉💞✌✨🤷😱😌🌸🙌😋💗💚😏💛🙂💓🤩😄😀🖤😃💯🙈👇🎶😒🤭❣😜💋👀😪😑💥🙋😞😩😡🤪👊🥳😥🤤👉💃😳✋😚😝😴🌟😬🙃🍀🌷😻😓⭐✅🥺🌈😈🤘💦✔😣🏃💐☹🎊💘😠☝😕🌺🎂🌻😐🖕💝🙊😹🗣💫💀👑🎵🤞😛🔴😤🌼😫⚽🤙☕🏆🤫👈😮🙆🍻🍃🐶💁😲🌿🧡🎁⚡🌞🎈❌✊👋😰🤨😶🤝🚶💰🍓💢🤟🙁🚨💨🤬✈🎀🍺🤓😙💟🌱😖👶🥴▶➡❓💎💸⬇😨🌚🦋😷🕺⚠🙅😟😵👎🤲🤠🤧📌🔵💅🧐🐾🍒😗🤑🌊🤯🐷☎💧😯💆👆🎤🙇🍑❄🌴💣🐸💌📍🥀🤢👅💡💩👐📸👻🤐🤮🎼🥵🚩🍎🍊👼💍📣🥂");
var alphabetBytesToChars = alphabet.reduce((p, c, i) => {
  p[i] = c;
  return p;
}, []);
var alphabetCharsToBytes = alphabet.reduce((p, c, i) => {
  const codePoint = c.codePointAt(0);
  if (codePoint == null) {
    throw new Error(`Invalid character: ${c}`);
  }
  p[codePoint] = i;
  return p;
}, []);
function encode19(data) {
  return data.reduce((p, c) => {
    p += alphabetBytesToChars[c];
    return p;
  }, "");
}
function decode21(str) {
  const byts = [];
  for (const char of str) {
    const codePoint = char.codePointAt(0);
    if (codePoint == null) {
      throw new Error(`Invalid character: ${char}`);
    }
    const byt = alphabetCharsToBytes[codePoint];
    if (byt == null) {
      throw new Error(`Non-base256emoji character: ${char}`);
    }
    byts.push(byt);
  }
  return new Uint8Array(byts);
}
var base256emoji = from({
  prefix: "🚀",
  name: "base256emoji",
  encode: encode19,
  decode: decode21
});

// node_modules/multiformats/dist/src/bases/base8.js
var base8_exports = {};
__export(base8_exports, {
  base8: () => base8
});
var base8 = rfc4648({
  prefix: "7",
  name: "base8",
  alphabet: "01234567",
  bitsPerChar: 3
});

// node_modules/multiformats/dist/src/bases/identity.js
var identity_exports2 = {};
__export(identity_exports2, {
  identity: () => identity2
});
var identity2 = from({
  prefix: "\0",
  name: "identity",
  encode: (buf2) => toString2(buf2),
  decode: (str) => fromString2(str)
});

// node_modules/multiformats/dist/src/codecs/json.js
var textEncoder2 = new TextEncoder();
var textDecoder2 = new TextDecoder();

// node_modules/multiformats/dist/src/basics.js
var bases = { ...identity_exports2, ...base2_exports, ...base8_exports, ...base10_exports, ...base16_exports, ...base32_exports, ...base36_exports, ...base58_exports, ...base64_exports, ...base256emoji_exports };
var hashes = { ...sha2_browser_exports, ...identity_exports };

// node_modules/uint8arrays/dist/src/util/bases.js
function createCodec(name15, prefix2, encode32, decode36) {
  return {
    name: name15,
    prefix: prefix2,
    encoder: {
      name: name15,
      prefix: prefix2,
      encode: encode32
    },
    decoder: {
      decode: decode36
    }
  };
}
var string2 = createCodec("utf8", "u", (buf2) => {
  const decoder3 = new TextDecoder("utf8");
  return "u" + decoder3.decode(buf2);
}, (str) => {
  const encoder3 = new TextEncoder();
  return encoder3.encode(str.substring(1));
});
var ascii = createCodec("ascii", "a", (buf2) => {
  let string3 = "a";
  for (let i = 0; i < buf2.length; i++) {
    string3 += String.fromCharCode(buf2[i]);
  }
  return string3;
}, (str) => {
  str = str.substring(1);
  const buf2 = allocUnsafe(str.length);
  for (let i = 0; i < str.length; i++) {
    buf2[i] = str.charCodeAt(i);
  }
  return buf2;
});
var BASES = {
  utf8: string2,
  "utf-8": string2,
  hex: bases.base16,
  latin1: ascii,
  ascii,
  binary: ascii,
  ...bases
};

// node_modules/@storacha/access/dist/encoding.js
var Types3 = __toESM(require_lib(), 1);
function bytesToDelegations(bytes3) {
  if (!(bytes3 instanceof Uint8Array) || bytes3.length === 0) {
    throw new TypeError("Input should be a non-empty Uint8Array.");
  }
  const reader = CarBufferReader.fromBytes(bytes3);
  const roots = reader.getRoots();
  const delegations = [];
  for (const root2 of roots) {
    const rootBlock = reader.get(root2);
    if (rootBlock) {
      const blocks = /* @__PURE__ */ new Map();
      for (const block of reader.blocks()) {
        if (block.cid.toString() !== root2.toString())
          blocks.set(block.cid.toString(), block);
      }
      delegations.push(new Delegation(rootBlock, blocks));
    } else {
      throw new Error("Failed to find root from raw delegation.");
    }
  }
  return delegations;
}

// node_modules/@storacha/access/dist/access.js
var delegate4 = async (agent, { delegations, proofs: proofs2 = [], space: space2 = agent.currentSpace() }) => {
  if (!space2) {
    return fail2("Space must be specified");
  }
  const entries3 = Object.values(delegations).map((proof) => [
    proof.cid.toString(),
    proof.cid
  ]);
  const { out } = await agent.invokeAndExecute(delegate3, {
    with: space2,
    nb: {
      delegations: Object.fromEntries(entries3)
    },
    // must be embedded here because it's referenced by cid in .nb.delegations
    proofs: [...delegations, ...proofs2]
  });
  return out;
};
var request = async (agent, { account, provider = (
  /** @type {API.ProviderDID} */
  agent.connection.id.did()
), audience = agent.did(), access: access2 = spaceAccess, appName, sso }) => {
  const facts = [];
  if (appName) {
    facts.push({ appName });
  }
  if (sso) {
    if (typeof sso !== "object") {
      return fail2("SSO parameter must be an object");
    }
    if (!sso.authProvider || typeof sso.authProvider !== "string") {
      return fail2("SSO authProvider must be a non-empty string");
    }
    if (!sso.externalUserId || typeof sso.externalUserId !== "string") {
      return fail2("SSO externalUserId must be a non-empty string");
    }
    if (!sso.externalSessionToken || typeof sso.externalSessionToken !== "string") {
      return fail2("SSO externalSessionToken must be a non-empty string");
    }
    facts.push({
      sso: {
        authProvider: sso.authProvider,
        externalUserId: sso.externalUserId,
        externalSessionToken: sso.externalSessionToken
      }
    });
  }
  const { out: result } = await agent.invokeAndExecute(authorize, {
    audience: did_exports.parse(provider),
    with: audience,
    nb: {
      iss: account,
      // New ucan spec moved to recap style layout for capabilities and new
      // `access/request` will use similar format as opposed to legacy one,
      // in the meantime we translate new format to legacy format here.
      att: [...toCapabilities(access2)]
    },
    facts
  });
  return result.error ? result : {
    ok: new PendingAccessRequest({
      ...result.ok,
      agent,
      audience,
      provider
    })
  };
};
var claim2 = async (agent, { provider = (
  /** @type {API.ProviderDID} */
  agent.connection.id.did()
), audience = agent.did() } = {}) => {
  const { out: result } = await agent.invokeAndExecute(claim, {
    audience: did_exports.parse(provider),
    with: audience
  });
  if (result.error) {
    return result;
  } else {
    const delegations = Object.values(result.ok.delegations);
    const proofs2 = (
      /** @type {API.Tuple<API.Delegation>} */
      delegations.flatMap((proof) => bytesToDelegations(proof))
    );
    return { ok: new GrantedAccess({ agent, proofs: proofs2 }) };
  }
};
var createPendingAccessRequest = (agent, { request: request3, expiration, provider = (
  /** @type {API.ProviderDID} */
  agent.connection.id.did()
), audience = agent.did() }) => new PendingAccessRequest({ agent, request: request3, expiration, provider, audience });
var PendingAccessRequest = class {
  /**
   * @typedef {object} PendingAccessRequestModel
   * @property {API.Agent} agent - Agent handling interaction.
   * @property {API.DID} audience - Principal requesting an access.
   * @property {API.ProviderDID} provider - Provider handling request.
   * @property {API.UTCUnixTimestamp} expiration - Seconds in UTC.
   * @property {API.Link} request - Link to the `access/authorize` invocation.
   *
   * @param {PendingAccessRequestModel} model
   */
  constructor(model) {
    this.model = model;
  }
  get agent() {
    return this.model.agent;
  }
  get audience() {
    return this.model.audience;
  }
  get expiration() {
    return new Date(this.model.expiration * 1e3);
  }
  get request() {
    return this.model.request;
  }
  get provider() {
    return this.model.provider;
  }
  /**
   * Low level method and most likely you want to use `.claim` instead. This method will poll
   * fetch delegations **just once** and will return proofs matching to this request. Please note
   * that there may not be any matches in which case result will be `{ ok: [] }`.
   *
   * If you do want to continuously poll until request is approved or expired, you should use
   * `.claim` method instead.
   *
   * @returns {Promise<API.Result<API.Delegation[], API.InvocationError|API.AccessClaimFailure|RequestExpired>>}
   */
  async poll() {
    const { agent, audience, provider, expiration } = this.model;
    const timeout = expiration * 1e3 - Date.now();
    if (timeout <= 0) {
      return { error: new RequestExpired(this.model) };
    } else {
      const result = await claim2(agent, { audience, provider });
      return result.error ? result : {
        ok: result.ok.proofs.filter((proof) => isRequestedAccess(proof, this.model))
      };
    }
  }
  /**
   * Continuously polls delegations until this request is approved or expired. Returns
   * a `GrantedAccess` object (view over the delegations) that can be used in the
   * invocations or can be saved in the agent (store) using `.save()` method.
   *
   * @param {object} options
   * @param {number} [options.interval]
   * @param {AbortSignal} [options.signal]
   * @returns {Promise<API.Result<GrantedAccess, Error>>}
   */
  async claim({ signal, interval = 250 } = {}) {
    while ((signal == null ? void 0 : signal.aborted) !== true) {
      const result = await this.poll();
      if (result.error) {
        return result;
      } else if (result.ok.length > 0) {
        return {
          ok: new GrantedAccess({
            agent: this.agent,
            proofs: (
              /** @type {API.Tuple<API.Delegation>} */
              result.ok
            )
          })
        };
      }
      await new Promise((resolve) => setTimeout(resolve, interval));
    }
    return {
      error: Object.assign(new Error("Aborted"), { reason: signal.reason })
    };
  }
};
var RequestExpired = class extends Failure {
  /**
   * @param {PendingAccessRequestModel} model
   */
  constructor(model) {
    super();
    this.model = model;
  }
  get name() {
    return "RequestExpired";
  }
  get request() {
    return this.model.request;
  }
  get expiredAt() {
    return new Date(this.model.expiration * 1e3);
  }
  describe() {
    return `Access request expired at ${this.expiredAt} for ${this.request} request.`;
  }
};
var GrantedAccess = class {
  /**
   * @typedef {object} GrantedAccessModel
   * @property {API.Agent} agent - Agent that processed the request.
   * @property {API.Tuple<API.Delegation>} proofs - Delegations that grant access.
   *
   * @param {GrantedAccessModel} model
   */
  constructor(model) {
    this.model = model;
  }
  get proofs() {
    return this.model.proofs;
  }
  /**
   * Saves access into the agents proofs store so that it can be retained
   * between sessions.
   *
   * @param {object} input
   * @param {API.Agent} [input.agent]
   */
  save({ agent = this.model.agent } = {}) {
    return importAuthorization(agent, this);
  }
};
var isRequestedAccess = (delegation, { request: request3 }) => (
  // `access/confirm` handler adds facts to the delegation issued by the account
  // so that principal requesting access can identify correct delegation when
  // access is granted.
  delegation.facts.some((fact) => `${fact["access/request"]}` === `${request3}`)
);
var toCapabilities = (access2) => {
  const abilities = [];
  const entries3 = (
    /** @type {[API.Ability, API.Unit][]} */
    Object.entries(access2)
  );
  for (const [can, details] of entries3) {
    if (details) {
      abilities.push({ can });
    }
  }
  return abilities;
};
var spaceAccess = {
  "space/*": {},
  "blob/*": {},
  "index/*": {},
  "store/*": {},
  "upload/*": {},
  "access/*": {},
  "filecoin/*": {},
  "usage/*": {}
};
var accountAccess = {
  "*": {}
};

// node_modules/@storacha/access/dist/space.js
var space_exports2 = {};
__export(space_exports2, {
  OwnedSpace: () => OwnedSpace,
  SESSION_LIFETIME: () => SESSION_LIFETIME,
  SharedSpace: () => SharedSpace,
  createAuthorization: () => createAuthorization,
  createRecovery: () => createRecovery,
  fromDelegation: () => fromDelegation,
  fromMnemonic: () => fromMnemonic,
  generate: () => generate2,
  provision: () => provision,
  toMnemonic: () => toMnemonic
});

// node_modules/@ucanto/principal/src/ed25519.js
var ed25519_exports = {};
__export(ed25519_exports, {
  PUB_KEY_OFFSET: () => PUB_KEY_OFFSET,
  Signer: () => signer_exports,
  Verifier: () => verifier_exports,
  code: () => code9,
  decode: () => decode23,
  derive: () => derive2,
  encode: () => encode21,
  format: () => format9,
  from: () => from9,
  generate: () => generate,
  name: () => name7,
  or: () => or7,
  parse: () => parse7,
  signatureAlgorithm: () => signatureAlgorithm2,
  signatureCode: () => signatureCode2
});

// node_modules/@ucanto/principal/src/ed25519/signer.js
var signer_exports = {};
__export(signer_exports, {
  PUB_KEY_OFFSET: () => PUB_KEY_OFFSET,
  code: () => code9,
  decode: () => decode23,
  derive: () => derive2,
  encode: () => encode21,
  format: () => format9,
  from: () => from9,
  generate: () => generate,
  name: () => name7,
  or: () => or7,
  parse: () => parse7,
  signatureAlgorithm: () => signatureAlgorithm2,
  signatureCode: () => signatureCode2
});

// node_modules/@noble/ed25519/lib/esm/index.js
var nodeCrypto = __toESM(require_crypto());
var _0n = BigInt(0);
var _1n = BigInt(1);
var _2n = BigInt(2);
var _8n = BigInt(8);
var CU_O = BigInt("7237005577332262213973186563042994240857116359379907606001950938285454250989");
var CURVE = Object.freeze({
  a: BigInt(-1),
  d: BigInt("37095705934669439343138083508754565189542113879843219016388785533085940283555"),
  P: BigInt("57896044618658097711785492504343953926634992332820282019728792003956564819949"),
  l: CU_O,
  n: CU_O,
  h: BigInt(8),
  Gx: BigInt("15112221349535400772501151409588531511454012693041857206046113283949847762202"),
  Gy: BigInt("46316835694926478169428394003475163141307993866256225615783033603165251855960")
});
var POW_2_256 = BigInt("0x10000000000000000000000000000000000000000000000000000000000000000");
var SQRT_M1 = BigInt("19681161376707505956807079304988542015446066515923890162744021073123829784752");
var SQRT_D = BigInt("6853475219497561581579357271197624642482790079785650197046958215289687604742");
var SQRT_AD_MINUS_ONE = BigInt("25063068953384623474111414158702152701244531502492656460079210482610430750235");
var INVSQRT_A_MINUS_D = BigInt("54469307008909316920995813868745141605393597292927456921205312896311721017578");
var ONE_MINUS_D_SQ = BigInt("1159843021668779879193775521855586647937357759715417654439879720876111806838");
var D_MINUS_ONE_SQ = BigInt("40440834346308536858101042469323190826248399146238708352240133220865137265952");
var ExtendedPoint = class _ExtendedPoint {
  constructor(x, y, z, t) {
    this.x = x;
    this.y = y;
    this.z = z;
    this.t = t;
  }
  static fromAffine(p) {
    if (!(p instanceof Point)) {
      throw new TypeError("ExtendedPoint#fromAffine: expected Point");
    }
    if (p.equals(Point.ZERO))
      return _ExtendedPoint.ZERO;
    return new _ExtendedPoint(p.x, p.y, _1n, mod2(p.x * p.y));
  }
  static toAffineBatch(points) {
    const toInv = invertBatch(points.map((p) => p.z));
    return points.map((p, i) => p.toAffine(toInv[i]));
  }
  static normalizeZ(points) {
    return this.toAffineBatch(points).map(this.fromAffine);
  }
  equals(other) {
    assertExtPoint(other);
    const { x: X1, y: Y1, z: Z1 } = this;
    const { x: X2, y: Y2, z: Z2 } = other;
    const X1Z2 = mod2(X1 * Z2);
    const X2Z1 = mod2(X2 * Z1);
    const Y1Z2 = mod2(Y1 * Z2);
    const Y2Z1 = mod2(Y2 * Z1);
    return X1Z2 === X2Z1 && Y1Z2 === Y2Z1;
  }
  negate() {
    return new _ExtendedPoint(mod2(-this.x), this.y, this.z, mod2(-this.t));
  }
  double() {
    const { x: X1, y: Y1, z: Z1 } = this;
    const { a } = CURVE;
    const M = mod2;
    const A = M(X1 * X1);
    const B = M(Y1 * Y1);
    const C = M(_2n * M(Z1 * Z1));
    const D = M(a * A);
    const x1y1 = X1 + Y1;
    const E = M(M(x1y1 * x1y1) - A - B);
    const G = D + B;
    const F = G - C;
    const H = D - B;
    const X3 = M(E * F);
    const Y3 = M(G * H);
    const T3 = M(E * H);
    const Z3 = M(F * G);
    return new _ExtendedPoint(X3, Y3, Z3, T3);
  }
  add(other) {
    const { x: X1, y: Y1, z: Z1, t: T1 } = this;
    assertExtPoint(other);
    const { x: X2, y: Y2, z: Z2, t: T2 } = other;
    const { a, d } = CURVE;
    const M = mod2;
    const A = M(X1 * X2);
    const B = M(Y1 * Y2);
    const C = M(T1 * d * T2);
    const D = M(Z1 * Z2);
    const E = M((X1 + Y1) * (X2 + Y2) - A - B);
    const F = M(D - C);
    const G = M(D + C);
    const H = M(B - a * A);
    const X3 = M(E * F);
    const Y3 = M(G * H);
    const T3 = M(E * H);
    const Z3 = M(F * G);
    return new _ExtendedPoint(X3, Y3, Z3, T3);
  }
  subtract(other) {
    return this.add(other.negate());
  }
  precomputeWindow(W) {
    const windows = 1 + 256 / W;
    const points = [];
    let p = this;
    let base3 = p;
    for (let window2 = 0; window2 < windows; window2++) {
      base3 = p;
      points.push(base3);
      for (let i = 1; i < 2 ** (W - 1); i++) {
        base3 = base3.add(p);
        points.push(base3);
      }
      p = base3.double();
    }
    return points;
  }
  wNAF(n, affinePoint) {
    if (!affinePoint && this.equals(_ExtendedPoint.BASE))
      affinePoint = Point.BASE;
    const W = affinePoint && affinePoint._WINDOW_SIZE || 1;
    if (256 % W) {
      throw new Error("Point#wNAF: Invalid precomputation window, must be power of 2");
    }
    let precomputes = affinePoint && pointPrecomputes.get(affinePoint);
    if (!precomputes) {
      precomputes = this.precomputeWindow(W);
      if (affinePoint && W !== 1) {
        precomputes = _ExtendedPoint.normalizeZ(precomputes);
        pointPrecomputes.set(affinePoint, precomputes);
      }
    }
    let p = _ExtendedPoint.ZERO;
    let f = _ExtendedPoint.BASE;
    const windows = 1 + 256 / W;
    const windowSize = 2 ** (W - 1);
    const mask2 = BigInt(2 ** W - 1);
    const maxNumber = 2 ** W;
    const shiftBy = BigInt(W);
    for (let window2 = 0; window2 < windows; window2++) {
      const offset2 = window2 * windowSize;
      let wbits = Number(n & mask2);
      n >>= shiftBy;
      if (wbits > windowSize) {
        wbits -= maxNumber;
        n += _1n;
      }
      const offset1 = offset2;
      const offset22 = offset2 + Math.abs(wbits) - 1;
      const cond1 = window2 % 2 !== 0;
      const cond2 = wbits < 0;
      if (wbits === 0) {
        f = f.add(constTimeNegate(cond1, precomputes[offset1]));
      } else {
        p = p.add(constTimeNegate(cond2, precomputes[offset22]));
      }
    }
    return _ExtendedPoint.normalizeZ([p, f])[0];
  }
  multiply(scalar, affinePoint) {
    return this.wNAF(normalizeScalar(scalar, CURVE.l), affinePoint);
  }
  multiplyUnsafe(scalar) {
    let n = normalizeScalar(scalar, CURVE.l, false);
    const G = _ExtendedPoint.BASE;
    const P0 = _ExtendedPoint.ZERO;
    if (n === _0n)
      return P0;
    if (this.equals(P0) || n === _1n)
      return this;
    if (this.equals(G))
      return this.wNAF(n);
    let p = P0;
    let d = this;
    while (n > _0n) {
      if (n & _1n)
        p = p.add(d);
      d = d.double();
      n >>= _1n;
    }
    return p;
  }
  isSmallOrder() {
    return this.multiplyUnsafe(CURVE.h).equals(_ExtendedPoint.ZERO);
  }
  isTorsionFree() {
    let p = this.multiplyUnsafe(CURVE.l / _2n).double();
    if (CURVE.l % _2n)
      p = p.add(this);
    return p.equals(_ExtendedPoint.ZERO);
  }
  toAffine(invZ) {
    const { x, y, z } = this;
    const is0 = this.equals(_ExtendedPoint.ZERO);
    if (invZ == null)
      invZ = is0 ? _8n : invert(z);
    const ax = mod2(x * invZ);
    const ay = mod2(y * invZ);
    const zz = mod2(z * invZ);
    if (is0)
      return Point.ZERO;
    if (zz !== _1n)
      throw new Error("invZ was invalid");
    return new Point(ax, ay);
  }
  fromRistrettoBytes() {
    legacyRist();
  }
  toRistrettoBytes() {
    legacyRist();
  }
  fromRistrettoHash() {
    legacyRist();
  }
};
ExtendedPoint.BASE = new ExtendedPoint(CURVE.Gx, CURVE.Gy, _1n, mod2(CURVE.Gx * CURVE.Gy));
ExtendedPoint.ZERO = new ExtendedPoint(_0n, _1n, _1n, _0n);
function constTimeNegate(condition, item) {
  const neg = item.negate();
  return condition ? neg : item;
}
function assertExtPoint(other) {
  if (!(other instanceof ExtendedPoint))
    throw new TypeError("ExtendedPoint expected");
}
function assertRstPoint(other) {
  if (!(other instanceof RistrettoPoint))
    throw new TypeError("RistrettoPoint expected");
}
function legacyRist() {
  throw new Error("Legacy method: switch to RistrettoPoint");
}
var RistrettoPoint = class _RistrettoPoint {
  constructor(ep) {
    this.ep = ep;
  }
  static calcElligatorRistrettoMap(r0) {
    const { d } = CURVE;
    const r = mod2(SQRT_M1 * r0 * r0);
    const Ns = mod2((r + _1n) * ONE_MINUS_D_SQ);
    let c = BigInt(-1);
    const D = mod2((c - d * r) * mod2(r + d));
    let { isValid: Ns_D_is_sq, value: s } = uvRatio(Ns, D);
    let s_ = mod2(s * r0);
    if (!edIsNegative(s_))
      s_ = mod2(-s_);
    if (!Ns_D_is_sq)
      s = s_;
    if (!Ns_D_is_sq)
      c = r;
    const Nt = mod2(c * (r - _1n) * D_MINUS_ONE_SQ - D);
    const s2 = s * s;
    const W0 = mod2((s + s) * D);
    const W1 = mod2(Nt * SQRT_AD_MINUS_ONE);
    const W2 = mod2(_1n - s2);
    const W3 = mod2(_1n + s2);
    return new ExtendedPoint(mod2(W0 * W3), mod2(W2 * W1), mod2(W1 * W3), mod2(W0 * W2));
  }
  static hashToCurve(hex2) {
    hex2 = ensureBytes(hex2, 64);
    const r1 = bytes255ToNumberLE(hex2.slice(0, 32));
    const R1 = this.calcElligatorRistrettoMap(r1);
    const r2 = bytes255ToNumberLE(hex2.slice(32, 64));
    const R2 = this.calcElligatorRistrettoMap(r2);
    return new _RistrettoPoint(R1.add(R2));
  }
  static fromHex(hex2) {
    hex2 = ensureBytes(hex2, 32);
    const { a, d } = CURVE;
    const emsg = "RistrettoPoint.fromHex: the hex is not valid encoding of RistrettoPoint";
    const s = bytes255ToNumberLE(hex2);
    if (!equalBytes(numberTo32BytesLE(s), hex2) || edIsNegative(s))
      throw new Error(emsg);
    const s2 = mod2(s * s);
    const u1 = mod2(_1n + a * s2);
    const u2 = mod2(_1n - a * s2);
    const u1_2 = mod2(u1 * u1);
    const u2_2 = mod2(u2 * u2);
    const v = mod2(a * d * u1_2 - u2_2);
    const { isValid, value: I } = invertSqrt(mod2(v * u2_2));
    const Dx = mod2(I * u2);
    const Dy = mod2(I * Dx * v);
    let x = mod2((s + s) * Dx);
    if (edIsNegative(x))
      x = mod2(-x);
    const y = mod2(u1 * Dy);
    const t = mod2(x * y);
    if (!isValid || edIsNegative(t) || y === _0n)
      throw new Error(emsg);
    return new _RistrettoPoint(new ExtendedPoint(x, y, _1n, t));
  }
  toRawBytes() {
    let { x, y, z, t } = this.ep;
    const u1 = mod2(mod2(z + y) * mod2(z - y));
    const u2 = mod2(x * y);
    const u2sq = mod2(u2 * u2);
    const { value: invsqrt } = invertSqrt(mod2(u1 * u2sq));
    const D1 = mod2(invsqrt * u1);
    const D2 = mod2(invsqrt * u2);
    const zInv = mod2(D1 * D2 * t);
    let D;
    if (edIsNegative(t * zInv)) {
      let _x = mod2(y * SQRT_M1);
      let _y = mod2(x * SQRT_M1);
      x = _x;
      y = _y;
      D = mod2(D1 * INVSQRT_A_MINUS_D);
    } else {
      D = D2;
    }
    if (edIsNegative(x * zInv))
      y = mod2(-y);
    let s = mod2((z - y) * D);
    if (edIsNegative(s))
      s = mod2(-s);
    return numberTo32BytesLE(s);
  }
  toHex() {
    return bytesToHex(this.toRawBytes());
  }
  toString() {
    return this.toHex();
  }
  equals(other) {
    assertRstPoint(other);
    const a = this.ep;
    const b = other.ep;
    const one = mod2(a.x * b.y) === mod2(a.y * b.x);
    const two = mod2(a.y * b.y) === mod2(a.x * b.x);
    return one || two;
  }
  add(other) {
    assertRstPoint(other);
    return new _RistrettoPoint(this.ep.add(other.ep));
  }
  subtract(other) {
    assertRstPoint(other);
    return new _RistrettoPoint(this.ep.subtract(other.ep));
  }
  multiply(scalar) {
    return new _RistrettoPoint(this.ep.multiply(scalar));
  }
  multiplyUnsafe(scalar) {
    return new _RistrettoPoint(this.ep.multiplyUnsafe(scalar));
  }
};
RistrettoPoint.BASE = new RistrettoPoint(ExtendedPoint.BASE);
RistrettoPoint.ZERO = new RistrettoPoint(ExtendedPoint.ZERO);
var pointPrecomputes = /* @__PURE__ */ new WeakMap();
var Point = class _Point {
  constructor(x, y) {
    this.x = x;
    this.y = y;
  }
  _setWindowSize(windowSize) {
    this._WINDOW_SIZE = windowSize;
    pointPrecomputes.delete(this);
  }
  static fromHex(hex2, strict = true) {
    const { d, P } = CURVE;
    hex2 = ensureBytes(hex2, 32);
    const normed = hex2.slice();
    normed[31] = hex2[31] & ~128;
    const y = bytesToNumberLE(normed);
    if (strict && y >= P)
      throw new Error("Expected 0 < hex < P");
    if (!strict && y >= POW_2_256)
      throw new Error("Expected 0 < hex < 2**256");
    const y2 = mod2(y * y);
    const u = mod2(y2 - _1n);
    const v = mod2(d * y2 + _1n);
    let { isValid, value: x } = uvRatio(u, v);
    if (!isValid)
      throw new Error("Point.fromHex: invalid y coordinate");
    const isXOdd = (x & _1n) === _1n;
    const isLastByteOdd = (hex2[31] & 128) !== 0;
    if (isLastByteOdd !== isXOdd) {
      x = mod2(-x);
    }
    return new _Point(x, y);
  }
  static async fromPrivateKey(privateKey) {
    return (await getExtendedPublicKey(privateKey)).point;
  }
  toRawBytes() {
    const bytes3 = numberTo32BytesLE(this.y);
    bytes3[31] |= this.x & _1n ? 128 : 0;
    return bytes3;
  }
  toHex() {
    return bytesToHex(this.toRawBytes());
  }
  toX25519() {
    const { y } = this;
    const u = mod2((_1n + y) * invert(_1n - y));
    return numberTo32BytesLE(u);
  }
  isTorsionFree() {
    return ExtendedPoint.fromAffine(this).isTorsionFree();
  }
  equals(other) {
    return this.x === other.x && this.y === other.y;
  }
  negate() {
    return new _Point(mod2(-this.x), this.y);
  }
  add(other) {
    return ExtendedPoint.fromAffine(this).add(ExtendedPoint.fromAffine(other)).toAffine();
  }
  subtract(other) {
    return this.add(other.negate());
  }
  multiply(scalar) {
    return ExtendedPoint.fromAffine(this).multiply(scalar, this).toAffine();
  }
};
Point.BASE = new Point(CURVE.Gx, CURVE.Gy);
Point.ZERO = new Point(_0n, _1n);
var Signature2 = class _Signature {
  constructor(r, s) {
    this.r = r;
    this.s = s;
    this.assertValidity();
  }
  static fromHex(hex2) {
    const bytes3 = ensureBytes(hex2, 64);
    const r = Point.fromHex(bytes3.slice(0, 32), false);
    const s = bytesToNumberLE(bytes3.slice(32, 64));
    return new _Signature(r, s);
  }
  assertValidity() {
    const { r, s } = this;
    if (!(r instanceof Point))
      throw new Error("Expected Point instance");
    normalizeScalar(s, CURVE.l, false);
    return this;
  }
  toRawBytes() {
    const u8 = new Uint8Array(64);
    u8.set(this.r.toRawBytes());
    u8.set(numberTo32BytesLE(this.s), 32);
    return u8;
  }
  toHex() {
    return bytesToHex(this.toRawBytes());
  }
};
function isBytes(a) {
  return a instanceof Uint8Array || ArrayBuffer.isView(a) && a.constructor.name === "Uint8Array";
}
function abytes(item) {
  if (!isBytes(item))
    throw new Error("Uint8Array expected");
}
function concatBytes(...arrays) {
  arrays.every(abytes);
  if (arrays.length === 1)
    return arrays[0];
  const length2 = arrays.reduce((a, arr) => a + arr.length, 0);
  const result = new Uint8Array(length2);
  for (let i = 0, pad2 = 0; i < arrays.length; i++) {
    const arr = arrays[i];
    result.set(arr, pad2);
    pad2 += arr.length;
  }
  return result;
}
var hexes = Array.from({ length: 256 }, (_, i) => i.toString(16).padStart(2, "0"));
function bytesToHex(bytes3) {
  abytes(bytes3);
  let hex2 = "";
  for (let i = 0; i < bytes3.length; i++) {
    hex2 += hexes[bytes3[i]];
  }
  return hex2;
}
var asciis = { _0: 48, _9: 57, A: 65, F: 70, a: 97, f: 102 };
function asciiToBase16(ch) {
  if (ch >= asciis._0 && ch <= asciis._9)
    return ch - asciis._0;
  if (ch >= asciis.A && ch <= asciis.F)
    return ch - (asciis.A - 10);
  if (ch >= asciis.a && ch <= asciis.f)
    return ch - (asciis.a - 10);
  return;
}
function hexToBytes(hex2) {
  if (typeof hex2 !== "string")
    throw new Error("hex string expected, got " + typeof hex2);
  const hl = hex2.length;
  const al = hl / 2;
  if (hl % 2)
    throw new Error("hex string expected, got unpadded hex of length " + hl);
  const array2 = new Uint8Array(al);
  for (let ai = 0, hi = 0; ai < al; ai++, hi += 2) {
    const n1 = asciiToBase16(hex2.charCodeAt(hi));
    const n2 = asciiToBase16(hex2.charCodeAt(hi + 1));
    if (n1 === void 0 || n2 === void 0) {
      const char = hex2[hi] + hex2[hi + 1];
      throw new Error('hex string expected, got non-hex character "' + char + '" at index ' + hi);
    }
    array2[ai] = n1 * 16 + n2;
  }
  return array2;
}
function numberTo32BytesBE(num) {
  const length2 = 32;
  const hex2 = num.toString(16).padStart(length2 * 2, "0");
  return hexToBytes(hex2);
}
function numberTo32BytesLE(num) {
  return numberTo32BytesBE(num).reverse();
}
function edIsNegative(num) {
  return (mod2(num) & _1n) === _1n;
}
function bytesToNumberLE(uint8a) {
  abytes(uint8a);
  return BigInt("0x" + bytesToHex(Uint8Array.from(uint8a).reverse()));
}
var MAX_255B = BigInt("0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff");
function bytes255ToNumberLE(bytes3) {
  return mod2(bytesToNumberLE(bytes3) & MAX_255B);
}
function mod2(a, b = CURVE.P) {
  const res = a % b;
  return res >= _0n ? res : b + res;
}
function invert(number3, modulo = CURVE.P) {
  if (number3 === _0n || modulo <= _0n) {
    throw new Error(`invert: expected positive integers, got n=${number3} mod=${modulo}`);
  }
  let a = mod2(number3, modulo);
  let b = modulo;
  let x = _0n, y = _1n, u = _1n, v = _0n;
  while (a !== _0n) {
    const q = b / a;
    const r = b % a;
    const m = x - u * q;
    const n = y - v * q;
    b = a, a = r, x = u, y = v, u = m, v = n;
  }
  const gcd2 = b;
  if (gcd2 !== _1n)
    throw new Error("invert: does not exist");
  return mod2(x, modulo);
}
function invertBatch(nums, p = CURVE.P) {
  const tmp = new Array(nums.length);
  const lastMultiplied = nums.reduce((acc, num, i) => {
    if (num === _0n)
      return acc;
    tmp[i] = acc;
    return mod2(acc * num, p);
  }, _1n);
  const inverted = invert(lastMultiplied, p);
  nums.reduceRight((acc, num, i) => {
    if (num === _0n)
      return acc;
    tmp[i] = mod2(acc * tmp[i], p);
    return mod2(acc * num, p);
  }, inverted);
  return tmp;
}
function pow2(x, power) {
  const { P } = CURVE;
  let res = x;
  while (power-- > _0n) {
    res *= res;
    res %= P;
  }
  return res;
}
function pow_2_252_3(x) {
  const { P } = CURVE;
  const _5n = BigInt(5);
  const _10n = BigInt(10);
  const _20n = BigInt(20);
  const _40n = BigInt(40);
  const _80n = BigInt(80);
  const x2 = x * x % P;
  const b2 = x2 * x % P;
  const b4 = pow2(b2, _2n) * b2 % P;
  const b5 = pow2(b4, _1n) * x % P;
  const b10 = pow2(b5, _5n) * b5 % P;
  const b20 = pow2(b10, _10n) * b10 % P;
  const b40 = pow2(b20, _20n) * b20 % P;
  const b80 = pow2(b40, _40n) * b40 % P;
  const b160 = pow2(b80, _80n) * b80 % P;
  const b240 = pow2(b160, _80n) * b80 % P;
  const b250 = pow2(b240, _10n) * b10 % P;
  const pow_p_5_8 = pow2(b250, _2n) * x % P;
  return { pow_p_5_8, b2 };
}
function uvRatio(u, v) {
  const v3 = mod2(v * v * v);
  const v7 = mod2(v3 * v3 * v);
  const pow = pow_2_252_3(u * v7).pow_p_5_8;
  let x = mod2(u * v3 * pow);
  const vx2 = mod2(v * x * x);
  const root1 = x;
  const root2 = mod2(x * SQRT_M1);
  const useRoot1 = vx2 === u;
  const useRoot2 = vx2 === mod2(-u);
  const noRoot = vx2 === mod2(-u * SQRT_M1);
  if (useRoot1)
    x = root1;
  if (useRoot2 || noRoot)
    x = root2;
  if (edIsNegative(x))
    x = mod2(-x);
  return { isValid: useRoot1 || useRoot2, value: x };
}
function invertSqrt(number3) {
  return uvRatio(_1n, number3);
}
function modlLE(hash) {
  return mod2(bytesToNumberLE(hash), CURVE.l);
}
function equalBytes(b1, b2) {
  if (b1.length !== b2.length) {
    return false;
  }
  for (let i = 0; i < b1.length; i++) {
    if (b1[i] !== b2[i]) {
      return false;
    }
  }
  return true;
}
function ensureBytes(hex2, expectedLength) {
  const bytes3 = isBytes(hex2) ? Uint8Array.from(hex2) : hexToBytes(hex2);
  if (typeof expectedLength === "number" && bytes3.length !== expectedLength)
    throw new Error(`Expected ${expectedLength} bytes`);
  return bytes3;
}
function normalizeScalar(num, max, strict = true) {
  if (!max)
    throw new TypeError("Specify max value");
  if (typeof num === "number" && Number.isSafeInteger(num))
    num = BigInt(num);
  if (typeof num === "bigint" && num < max) {
    if (strict) {
      if (_0n < num)
        return num;
    } else {
      if (_0n <= num)
        return num;
    }
  }
  throw new TypeError("Expected valid scalar: 0 < scalar < max");
}
function adjustBytes25519(bytes3) {
  bytes3[0] &= 248;
  bytes3[31] &= 127;
  bytes3[31] |= 64;
  return bytes3;
}
function checkPrivateKey(key) {
  key = typeof key === "bigint" || typeof key === "number" ? numberTo32BytesBE(normalizeScalar(key, POW_2_256)) : ensureBytes(key);
  if (key.length !== 32)
    throw new Error(`Expected 32 bytes`);
  return key;
}
function getKeyFromHash(hashed) {
  const head = adjustBytes25519(hashed.slice(0, 32));
  const prefix2 = hashed.slice(32, 64);
  const scalar = modlLE(head);
  const point = Point.BASE.multiply(scalar);
  const pointBytes = point.toRawBytes();
  return { head, prefix: prefix2, scalar, point, pointBytes };
}
var _sha512Sync;
async function getExtendedPublicKey(key) {
  return getKeyFromHash(await utils.sha512(checkPrivateKey(key)));
}
async function getPublicKey(privateKey) {
  return (await getExtendedPublicKey(privateKey)).pointBytes;
}
async function sign(message, privateKey) {
  message = ensureBytes(message);
  const { prefix: prefix2, scalar, pointBytes } = await getExtendedPublicKey(privateKey);
  const r = modlLE(await utils.sha512(prefix2, message));
  const R = Point.BASE.multiply(r);
  const k = modlLE(await utils.sha512(R.toRawBytes(), pointBytes, message));
  const s = mod2(r + k * scalar, CURVE.l);
  return new Signature2(R, s).toRawBytes();
}
function prepareVerification(sig, message, publicKey) {
  message = ensureBytes(message);
  if (!(publicKey instanceof Point))
    publicKey = Point.fromHex(publicKey, false);
  const { r, s } = sig instanceof Signature2 ? sig.assertValidity() : Signature2.fromHex(sig);
  const SB = ExtendedPoint.BASE.multiplyUnsafe(s);
  return { r, s, SB, pub: publicKey, msg: message };
}
function finishVerification(publicKey, r, SB, hashed) {
  const k = modlLE(hashed);
  const kA = ExtendedPoint.fromAffine(publicKey).multiplyUnsafe(k);
  const RkA = ExtendedPoint.fromAffine(r).add(kA);
  return RkA.subtract(SB).multiplyUnsafe(CURVE.h).equals(ExtendedPoint.ZERO);
}
async function verify(sig, message, publicKey) {
  const { r, SB, msg, pub } = prepareVerification(sig, message, publicKey);
  const hashed = await utils.sha512(r.toRawBytes(), pub.toRawBytes(), msg);
  return finishVerification(pub, r, SB, hashed);
}
Point.BASE._setWindowSize(8);
var crypto2 = {
  node: nodeCrypto,
  web: typeof self === "object" && "crypto" in self ? self.crypto : void 0
};
var utils = {
  bytesToHex,
  hexToBytes,
  concatBytes,
  getExtendedPublicKey,
  mod: mod2,
  invert,
  TORSION_SUBGROUP: [
    "0100000000000000000000000000000000000000000000000000000000000000",
    "c7176a703d4dd84fba3c0b760d10670f2a2053fa2c39ccc64ec7fd7792ac037a",
    "0000000000000000000000000000000000000000000000000000000000000080",
    "26e8958fc2b227b045c3f489f2ef98f0d5dfac05d3c63339b13802886d53fc05",
    "ecffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff7f",
    "26e8958fc2b227b045c3f489f2ef98f0d5dfac05d3c63339b13802886d53fc85",
    "0000000000000000000000000000000000000000000000000000000000000000",
    "c7176a703d4dd84fba3c0b760d10670f2a2053fa2c39ccc64ec7fd7792ac03fa"
  ],
  hashToPrivateScalar: (hash) => {
    hash = ensureBytes(hash);
    if (hash.length < 40 || hash.length > 1024)
      throw new Error("Expected 40-1024 bytes of private key as per FIPS 186");
    return mod2(bytesToNumberLE(hash), CURVE.l - _1n) + _1n;
  },
  randomBytes: (bytesLength = 32) => {
    if (crypto2.web) {
      return crypto2.web.getRandomValues(new Uint8Array(bytesLength));
    } else if (crypto2.node) {
      const { randomBytes: randomBytes2 } = crypto2.node;
      return new Uint8Array(randomBytes2(bytesLength).buffer);
    } else {
      throw new Error("The environment doesn't have randomBytes function");
    }
  },
  randomPrivateKey: () => {
    return utils.randomBytes(32);
  },
  sha512: async (...messages) => {
    const message = concatBytes(...messages);
    if (crypto2.web) {
      const buffer2 = await crypto2.web.subtle.digest("SHA-512", message.buffer);
      return new Uint8Array(buffer2);
    } else if (crypto2.node) {
      return Uint8Array.from(crypto2.node.createHash("sha512").update(message).digest());
    } else {
      throw new Error("The environment doesn't have sha512 function");
    }
  },
  precompute(windowSize = 8, point = Point.BASE) {
    const cached = point.equals(Point.BASE) ? point : new Point(point.x, point.y);
    cached._setWindowSize(windowSize);
    cached.multiply(_2n);
    return cached;
  },
  sha512Sync: void 0
};
Object.defineProperties(utils, {
  sha512Sync: {
    configurable: false,
    get() {
      return _sha512Sync;
    },
    set(val) {
      if (!_sha512Sync)
        _sha512Sync = val;
    }
  }
});

// node_modules/@ucanto/principal/src/ed25519/verifier.js
var verifier_exports = {};
__export(verifier_exports, {
  code: () => code8,
  decode: () => decode22,
  encode: () => encode20,
  format: () => format8,
  name: () => name6,
  or: () => or5,
  parse: () => parse6,
  signatureAlgorithm: () => signatureAlgorithm,
  signatureCode: () => signatureCode
});

// node_modules/@ucanto/principal/src/verifier.js
var API26 = __toESM(require_lib(), 1);
var parseWith = (did2, parsers) => {
  if (did2.startsWith("did:")) {
    for (const parser of parsers) {
      try {
        return parser.parse(did2);
      } catch (_) {
      }
    }
    throw new Error(`Unsupported did ${did2}`);
  } else {
    throw new Error(`Expected did instead got ${did2}`);
  }
};
var or4 = (left, right) => new Parser([left, right]);
var Parser = class _Parser {
  /**
   * @param {API.PrincipalParser[]} variants
   */
  constructor(variants) {
    this.variants = variants;
  }
  /**
   * @param {API.DID} did
   */
  parse(did2) {
    return parseWith(did2, this.variants);
  }
  /**
   * @param {API.PrincipalParser} parser
   */
  or(parser) {
    return new _Parser([...this.variants, parser]);
  }
};
var withDID = (key, id) => new VerifierWithDID(id, key);
var VerifierWithDID = class {
  /**
   * @param {ID} id
   * @param {API.VerifierKey<SigAlg>} key
   */
  constructor(id, key) {
    this.id = id;
    this.key = key;
  }
  did() {
    return this.id;
  }
  toDIDKey() {
    return this.key.toDIDKey();
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, SigAlg>} signature
   * @returns {API.Await<boolean>}
   */
  verify(payload, signature) {
    return this.key.verify(payload, signature);
  }
  /**
   * @template {API.DID} ID
   * @param {ID} id
   */
  withDID(id) {
    return withDID(this.key, id);
  }
};

// node_modules/@ucanto/principal/src/ed25519/verifier.js
var code8 = 237;
var name6 = "Ed25519";
var signatureCode = EdDSA;
var signatureAlgorithm = "EdDSA";
var PUBLIC_TAG_SIZE = varint_exports.encodingLength(code8);
var SIZE = 32 + PUBLIC_TAG_SIZE;
var parse6 = (did2) => decode22(parse2(did2));
var decode22 = (bytes3) => {
  const [algorithm2] = varint_exports.decode(bytes3);
  if (algorithm2 !== code8) {
    throw new RangeError(
      `Unsupported key algorithm with multicode 0x${code8.toString(16)}`
    );
  } else if (bytes3.byteLength !== SIZE) {
    throw new RangeError(
      `Expected Uint8Array with byteLength ${SIZE}, instead got Uint8Array with byteLength ${bytes3.byteLength}`
    );
  } else {
    return new Ed25519Verifier(bytes3.buffer, bytes3.byteOffset, bytes3.byteLength);
  }
};
var format8 = (principal2) => format2(principal2);
var encode20 = (principal2) => encode7(principal2);
var Ed25519Verifier = class extends Uint8Array {
  /** @type {typeof code} */
  get code() {
    return code8;
  }
  /** @type {typeof signatureCode} */
  get signatureCode() {
    return signatureCode;
  }
  /** @type {typeof signatureAlgorithm} */
  get signatureAlgorithm() {
    return signatureAlgorithm;
  }
  /**
   * Raw public key without a multiformat code.
   *
   * @readonly
   */
  get publicKey() {
    const key = new Uint8Array(this.buffer, this.byteOffset + PUBLIC_TAG_SIZE);
    Object.defineProperties(this, {
      publicKey: {
        value: key
      }
    });
    return key;
  }
  /**
   * DID of the Principal in `did:key` format.
   * @returns {API.DID<"key">}
   */
  did() {
    return `did:key:${base58btc.encode(this)}`;
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, Signature.EdDSA>} signature
   * @returns {API.Await<boolean>}
   */
  verify(payload, signature) {
    return signature.code === signatureCode && verify(signature.raw, payload, this.publicKey);
  }
  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Verifier<ID, typeof signatureCode>}
   */
  withDID(id) {
    return withDID(this, id);
  }
  toDIDKey() {
    return this.did();
  }
};
var or5 = (other) => or4({ parse: parse6 }, other);

// node_modules/@ucanto/principal/src/signer.js
var API27 = __toESM(require_lib(), 1);
var or6 = (left, right) => new Importer([left, right]);
var Importer = class _Importer {
  /**
   * @param {Importers} variants
   */
  constructor(variants) {
    this.variants = variants;
    this.from = create7(variants);
  }
  /**
   * @template {API.SignerImporter} Other
   * @param {Other} other
   * @returns {API.CompositeImporter<[Other, ...Importers]>}
   */
  or(other) {
    return new _Importer([other, ...this.variants]);
  }
};
var create7 = (importers) => {
  const from18 = (archive4) => {
    if (archive4.id.startsWith("did:key:")) {
      return (
        /** @type {API.Signer<ID, Alg>} */
        importWith(archive4, importers)
      );
    } else {
      for (const [name15, key] of Object.entries(archive4.keys)) {
        const id = (
          /** @type {API.DIDKey} */
          name15
        );
        const signer = (
          /** @type {API.Signer<API.DIDKey, Alg>} */
          importWith(
            {
              id,
              keys: { [id]: key }
            },
            importers
          )
        );
        return signer.withDID(archive4.id);
      }
      throw new Error(`Archive ${archive4.id} contains no keys`);
    }
  };
  return (
    /** @type {API.Intersection<Importers[number]['from']>} */
    from18
  );
};
var importWith = (archive4, importers) => {
  for (const importer of importers) {
    try {
      return importer.from(archive4);
    } catch (_) {
    }
  }
  throw new Error(`Unsupported signer`);
};
var withDID2 = ({ signer, verifier }, id) => new SignerWithDID(signer, verifier.withDID(id));
var SignerWithDID = class {
  /**
   * @param {API.Signer<API.DID<'key'>, Code>} key
   * @param {API.Verifier<ID, Code>} verifier
   */
  constructor(key, verifier) {
    this.key = key;
    this.verifier = verifier;
  }
  /** @type {API.Signer<ID, Code>} */
  get signer() {
    return this;
  }
  get signatureAlgorithm() {
    return this.key.signatureAlgorithm;
  }
  get signatureCode() {
    return this.key.signatureCode;
  }
  /**
   * @returns {ID}
   */
  did() {
    return this.verifier.did();
  }
  toDIDKey() {
    return this.verifier.toDIDKey();
  }
  /**
   * @template {API.DID} ID
   * @param {ID} id
   */
  withDID(id) {
    return withDID2(this.key, id);
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   */
  sign(payload) {
    return this.key.sign(payload);
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, Code>} signature
   */
  verify(payload, signature) {
    return this.verifier.verify(payload, signature);
  }
  toArchive() {
    const { keys: keys2 } = this.key.toArchive();
    return {
      id: this.did(),
      keys: keys2
    };
  }
};

// node_modules/@ucanto/principal/src/ed25519/signer.js
var code9 = 4864;
var name7 = name6;
var signatureAlgorithm2 = signatureAlgorithm;
var signatureCode2 = signatureCode;
var PRIVATE_TAG_SIZE = varint_exports.encodingLength(code9);
var PUBLIC_TAG_SIZE2 = varint_exports.encodingLength(code8);
var KEY_SIZE = 32;
var SIZE2 = PRIVATE_TAG_SIZE + KEY_SIZE + PUBLIC_TAG_SIZE2 + KEY_SIZE;
var PUB_KEY_OFFSET = PRIVATE_TAG_SIZE + KEY_SIZE;
var generate = () => derive2(utils.randomPrivateKey());
var derive2 = async (secret) => {
  if (secret.byteLength !== KEY_SIZE) {
    throw new Error(
      `Expected Uint8Array with byteLength of ${KEY_SIZE} instead not ${secret.byteLength}`
    );
  }
  const publicKey = await getPublicKey(secret);
  const signer = new Ed25519Signer(SIZE2);
  varint_exports.encodeTo(code9, signer, 0);
  signer.set(secret, PRIVATE_TAG_SIZE);
  varint_exports.encodeTo(code8, signer, PRIVATE_TAG_SIZE + KEY_SIZE);
  signer.set(publicKey, PRIVATE_TAG_SIZE + KEY_SIZE + PUBLIC_TAG_SIZE2);
  return signer;
};
var from9 = ({ id, keys: keys2 }) => {
  if (id.startsWith("did:key:")) {
    const key = keys2[
      /** @type {API.DIDKey} */
      id
    ];
    if (key instanceof Uint8Array) {
      return decode23(key);
    }
  }
  throw new TypeError(`Unsupported archive format`);
};
var or7 = (other) => or6({ from: from9 }, other);
var decode23 = (bytes3) => {
  if (bytes3.byteLength !== SIZE2) {
    throw new Error(
      `Expected Uint8Array with byteLength of ${SIZE2} instead not ${bytes3.byteLength}`
    );
  }
  {
    const [keyCode] = varint_exports.decode(bytes3);
    if (keyCode !== code9) {
      throw new Error(`Given bytes must be a multiformat with ${code9} tag`);
    }
  }
  {
    const [code19] = varint_exports.decode(bytes3.subarray(PUB_KEY_OFFSET));
    if (code19 !== code8) {
      throw new Error(
        `Given bytes must contain public key in multiformats with ${code8} tag`
      );
    }
  }
  return new Ed25519Signer(bytes3);
};
var encode21 = (signer) => signer.encode();
var format9 = (signer, encoder3) => (encoder3 || base64pad).encode(encode21(signer));
var parse7 = (principal2, decoder3) => decode23((decoder3 || base64pad).decode(principal2));
var Ed25519Signer = class extends Uint8Array {
  /** @type {typeof code} */
  get code() {
    return code9;
  }
  get signer() {
    return this;
  }
  /** @type {API.EdVerifier} */
  get verifier() {
    const bytes3 = new Uint8Array(this.buffer, PRIVATE_TAG_SIZE + KEY_SIZE);
    const verifier = decode22(bytes3);
    Object.defineProperties(this, {
      verifier: {
        value: verifier
      }
    });
    return verifier;
  }
  /**
   * Raw public key without multiformat code.
   */
  get secret() {
    const secret = new Uint8Array(this.buffer, PRIVATE_TAG_SIZE, KEY_SIZE);
    Object.defineProperties(this, {
      secret: {
        value: secret
      }
    });
    return secret;
  }
  /**
   * DID of this principal in `did:key` format.
   */
  did() {
    return this.verifier.did();
  }
  toDIDKey() {
    return this.verifier.toDIDKey();
  }
  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Signer<ID, typeof Signature.EdDSA>}
   */
  withDID(id) {
    return withDID2(this, id);
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @returns {Promise<API.SignatureView<T, typeof Signature.EdDSA>>}
   */
  async sign(payload) {
    const raw = await sign(payload, this.secret);
    return create3(this.signatureCode, raw);
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, typeof this.signatureCode>} signature
   */
  verify(payload, signature) {
    return this.verifier.verify(payload, signature);
  }
  get signatureAlgorithm() {
    return signatureAlgorithm2;
  }
  get signatureCode() {
    return EdDSA;
  }
  encode() {
    return this;
  }
  toArchive() {
    const id = this.did();
    return {
      id,
      keys: { [id]: this.encode() }
    };
  }
};

// node_modules/@scure/base/lib/esm/index.js
function assertNumber(n) {
  if (!Number.isSafeInteger(n))
    throw new Error(`Wrong integer: ${n}`);
}
function isBytes2(a) {
  return a instanceof Uint8Array || a != null && typeof a === "object" && a.constructor.name === "Uint8Array";
}
function chain(...args) {
  const id = (a) => a;
  const wrap = (a, b) => (c) => a(b(c));
  const encode32 = args.map((x) => x.encode).reduceRight(wrap, id);
  const decode36 = args.map((x) => x.decode).reduce(wrap, id);
  return { encode: encode32, decode: decode36 };
}
function alphabet2(alphabet3) {
  return {
    encode: (digits) => {
      if (!Array.isArray(digits) || digits.length && typeof digits[0] !== "number")
        throw new Error("alphabet.encode input should be an array of numbers");
      return digits.map((i) => {
        assertNumber(i);
        if (i < 0 || i >= alphabet3.length)
          throw new Error(`Digit index outside alphabet: ${i} (alphabet: ${alphabet3.length})`);
        return alphabet3[i];
      });
    },
    decode: (input11) => {
      if (!Array.isArray(input11) || input11.length && typeof input11[0] !== "string")
        throw new Error("alphabet.decode input should be array of strings");
      return input11.map((letter) => {
        if (typeof letter !== "string")
          throw new Error(`alphabet.decode: not string element=${letter}`);
        const index3 = alphabet3.indexOf(letter);
        if (index3 === -1)
          throw new Error(`Unknown letter: "${letter}". Allowed: ${alphabet3}`);
        return index3;
      });
    }
  };
}
function join(separator = "") {
  if (typeof separator !== "string")
    throw new Error("join separator should be string");
  return {
    encode: (from18) => {
      if (!Array.isArray(from18) || from18.length && typeof from18[0] !== "string")
        throw new Error("join.encode input should be array of strings");
      for (let i of from18)
        if (typeof i !== "string")
          throw new Error(`join.encode: non-string input=${i}`);
      return from18.join(separator);
    },
    decode: (to) => {
      if (typeof to !== "string")
        throw new Error("join.decode input should be string");
      return to.split(separator);
    }
  };
}
function padding(bits, chr = "=") {
  assertNumber(bits);
  if (typeof chr !== "string")
    throw new Error("padding chr should be string");
  return {
    encode(data) {
      if (!Array.isArray(data) || data.length && typeof data[0] !== "string")
        throw new Error("padding.encode input should be array of strings");
      for (let i of data)
        if (typeof i !== "string")
          throw new Error(`padding.encode: non-string input=${i}`);
      while (data.length * bits % 8)
        data.push(chr);
      return data;
    },
    decode(input11) {
      if (!Array.isArray(input11) || input11.length && typeof input11[0] !== "string")
        throw new Error("padding.encode input should be array of strings");
      for (let i of input11)
        if (typeof i !== "string")
          throw new Error(`padding.decode: non-string input=${i}`);
      let end = input11.length;
      if (end * bits % 8)
        throw new Error("Invalid padding: string should have whole number of bytes");
      for (; end > 0 && input11[end - 1] === chr; end--) {
        if (!((end - 1) * bits % 8))
          throw new Error("Invalid padding: string has too much padding");
      }
      return input11.slice(0, end);
    }
  };
}
function normalize(fn) {
  if (typeof fn !== "function")
    throw new Error("normalize fn should be function");
  return { encode: (from18) => from18, decode: (to) => fn(to) };
}
function convertRadix(data, from18, to) {
  if (from18 < 2)
    throw new Error(`convertRadix: wrong from=${from18}, base cannot be less than 2`);
  if (to < 2)
    throw new Error(`convertRadix: wrong to=${to}, base cannot be less than 2`);
  if (!Array.isArray(data))
    throw new Error("convertRadix: data should be array");
  if (!data.length)
    return [];
  let pos = 0;
  const res = [];
  const digits = Array.from(data);
  digits.forEach((d) => {
    assertNumber(d);
    if (d < 0 || d >= from18)
      throw new Error(`Wrong integer: ${d}`);
  });
  while (true) {
    let carry = 0;
    let done = true;
    for (let i = pos; i < digits.length; i++) {
      const digit = digits[i];
      const digitBase = from18 * carry + digit;
      if (!Number.isSafeInteger(digitBase) || from18 * carry / from18 !== carry || digitBase - digit !== from18 * carry) {
        throw new Error("convertRadix: carry overflow");
      }
      carry = digitBase % to;
      const rounded = Math.floor(digitBase / to);
      digits[i] = rounded;
      if (!Number.isSafeInteger(rounded) || rounded * to + carry !== digitBase)
        throw new Error("convertRadix: carry overflow");
      if (!done)
        continue;
      else if (!rounded)
        pos = i;
      else
        done = false;
    }
    res.push(carry);
    if (done)
      break;
  }
  for (let i = 0; i < data.length - 1 && data[i] === 0; i++)
    res.push(0);
  return res.reverse();
}
var gcd = (a, b) => !b ? a : gcd(b, a % b);
var radix2carry = (from18, to) => from18 + (to - gcd(from18, to));
function convertRadix2(data, from18, to, padding3) {
  if (!Array.isArray(data))
    throw new Error("convertRadix2: data should be array");
  if (from18 <= 0 || from18 > 32)
    throw new Error(`convertRadix2: wrong from=${from18}`);
  if (to <= 0 || to > 32)
    throw new Error(`convertRadix2: wrong to=${to}`);
  if (radix2carry(from18, to) > 32) {
    throw new Error(`convertRadix2: carry overflow from=${from18} to=${to} carryBits=${radix2carry(from18, to)}`);
  }
  let carry = 0;
  let pos = 0;
  const mask2 = 2 ** to - 1;
  const res = [];
  for (const n of data) {
    assertNumber(n);
    if (n >= 2 ** from18)
      throw new Error(`convertRadix2: invalid data word=${n} from=${from18}`);
    carry = carry << from18 | n;
    if (pos + from18 > 32)
      throw new Error(`convertRadix2: carry overflow pos=${pos} from=${from18}`);
    pos += from18;
    for (; pos >= to; pos -= to)
      res.push((carry >> pos - to & mask2) >>> 0);
    carry &= 2 ** pos - 1;
  }
  carry = carry << to - pos & mask2;
  if (!padding3 && pos >= from18)
    throw new Error("Excess padding");
  if (!padding3 && carry)
    throw new Error(`Non-zero padding: ${carry}`);
  if (padding3 && pos > 0)
    res.push(carry >>> 0);
  return res;
}
function radix(num) {
  assertNumber(num);
  return {
    encode: (bytes3) => {
      if (!isBytes2(bytes3))
        throw new Error("radix.encode input should be Uint8Array");
      return convertRadix(Array.from(bytes3), 2 ** 8, num);
    },
    decode: (digits) => {
      if (!Array.isArray(digits) || digits.length && typeof digits[0] !== "number")
        throw new Error("radix.decode input should be array of numbers");
      return Uint8Array.from(convertRadix(digits, num, 2 ** 8));
    }
  };
}
function radix2(bits, revPadding = false) {
  assertNumber(bits);
  if (bits <= 0 || bits > 32)
    throw new Error("radix2: bits should be in (0..32]");
  if (radix2carry(8, bits) > 32 || radix2carry(bits, 8) > 32)
    throw new Error("radix2: carry overflow");
  return {
    encode: (bytes3) => {
      if (!isBytes2(bytes3))
        throw new Error("radix2.encode input should be Uint8Array");
      return convertRadix2(Array.from(bytes3), 8, bits, !revPadding);
    },
    decode: (digits) => {
      if (!Array.isArray(digits) || digits.length && typeof digits[0] !== "number")
        throw new Error("radix2.decode input should be array of numbers");
      return Uint8Array.from(convertRadix2(digits, bits, 8, revPadding));
    }
  };
}
function unsafeWrapper(fn) {
  if (typeof fn !== "function")
    throw new Error("unsafeWrapper fn should be function");
  return function(...args) {
    try {
      return fn.apply(null, args);
    } catch (e) {
    }
  };
}
function checksum(len, fn) {
  assertNumber(len);
  if (typeof fn !== "function")
    throw new Error("checksum fn should be function");
  return {
    encode(data) {
      if (!isBytes2(data))
        throw new Error("checksum.encode: input should be Uint8Array");
      const checksum2 = fn(data).slice(0, len);
      const res = new Uint8Array(data.length + len);
      res.set(data);
      res.set(checksum2, data.length);
      return res;
    },
    decode(data) {
      if (!isBytes2(data))
        throw new Error("checksum.decode: input should be Uint8Array");
      const payload = data.slice(0, -len);
      const newChecksum = fn(payload).slice(0, len);
      const oldChecksum = data.slice(-len);
      for (let i = 0; i < len; i++)
        if (newChecksum[i] !== oldChecksum[i])
          throw new Error("Invalid checksum");
      return payload;
    }
  };
}
var utils2 = {
  alphabet: alphabet2,
  chain,
  checksum,
  convertRadix,
  convertRadix2,
  radix,
  radix2,
  join,
  padding
};
var base162 = chain(radix2(4), alphabet2("0123456789ABCDEF"), join(""));
var base322 = chain(radix2(5), alphabet2("ABCDEFGHIJKLMNOPQRSTUVWXYZ234567"), padding(5), join(""));
var base32nopad = chain(radix2(5), alphabet2("ABCDEFGHIJKLMNOPQRSTUVWXYZ234567"), join(""));
var base32hex2 = chain(radix2(5), alphabet2("0123456789ABCDEFGHIJKLMNOPQRSTUV"), padding(5), join(""));
var base32hexnopad = chain(radix2(5), alphabet2("0123456789ABCDEFGHIJKLMNOPQRSTUV"), join(""));
var base32crockford = chain(radix2(5), alphabet2("0123456789ABCDEFGHJKMNPQRSTVWXYZ"), join(""), normalize((s) => s.toUpperCase().replace(/O/g, "0").replace(/[IL]/g, "1")));
var base642 = chain(radix2(6), alphabet2("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"), padding(6), join(""));
var base64nopad = chain(radix2(6), alphabet2("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"), join(""));
var base64url2 = chain(radix2(6), alphabet2("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_"), padding(6), join(""));
var base64urlnopad = chain(radix2(6), alphabet2("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_"), join(""));
var genBase58 = (abc) => chain(radix(58), alphabet2(abc), join(""));
var base58 = genBase58("123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz");
var base58flickr2 = genBase58("123456789abcdefghijkmnopqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ");
var base58xrp = genBase58("rpshnaf39wBUDNEGHJKLM4PQRST7VWXYZ2bcdeCg65jkm8oFqi1tuvAxyz");
var BECH_ALPHABET = chain(alphabet2("qpzry9x8gf2tvdw0s3jn54khce6mua7l"), join(""));
var POLYMOD_GENERATORS = [996825010, 642813549, 513874426, 1027748829, 705979059];
function bech32Polymod(pre) {
  const b = pre >> 25;
  let chk = (pre & 33554431) << 5;
  for (let i = 0; i < POLYMOD_GENERATORS.length; i++) {
    if ((b >> i & 1) === 1)
      chk ^= POLYMOD_GENERATORS[i];
  }
  return chk;
}
function bechChecksum(prefix2, words, encodingConst = 1) {
  const len = prefix2.length;
  let chk = 1;
  for (let i = 0; i < len; i++) {
    const c = prefix2.charCodeAt(i);
    if (c < 33 || c > 126)
      throw new Error(`Invalid prefix (${prefix2})`);
    chk = bech32Polymod(chk) ^ c >> 5;
  }
  chk = bech32Polymod(chk);
  for (let i = 0; i < len; i++)
    chk = bech32Polymod(chk) ^ prefix2.charCodeAt(i) & 31;
  for (let v of words)
    chk = bech32Polymod(chk) ^ v;
  for (let i = 0; i < 6; i++)
    chk = bech32Polymod(chk);
  chk ^= encodingConst;
  return BECH_ALPHABET.encode(convertRadix2([chk % 2 ** 30], 30, 5, false));
}
function genBech32(encoding) {
  const ENCODING_CONST = encoding === "bech32" ? 1 : 734539939;
  const _words = radix2(5);
  const fromWords = _words.decode;
  const toWords = _words.encode;
  const fromWordsUnsafe = unsafeWrapper(fromWords);
  function encode32(prefix2, words, limit = 90) {
    if (typeof prefix2 !== "string")
      throw new Error(`bech32.encode prefix should be string, not ${typeof prefix2}`);
    if (words instanceof Uint8Array)
      words = Array.from(words);
    if (!Array.isArray(words) || words.length && typeof words[0] !== "number")
      throw new Error(`bech32.encode words should be array of numbers, not ${typeof words}`);
    if (prefix2.length === 0)
      throw new TypeError(`Invalid prefix length ${prefix2.length}`);
    const actualLength = prefix2.length + 7 + words.length;
    if (limit !== false && actualLength > limit)
      throw new TypeError(`Length ${actualLength} exceeds limit ${limit}`);
    const lowered = prefix2.toLowerCase();
    const sum = bechChecksum(lowered, words, ENCODING_CONST);
    return `${lowered}1${BECH_ALPHABET.encode(words)}${sum}`;
  }
  function decode36(str, limit = 90) {
    if (typeof str !== "string")
      throw new Error(`bech32.decode input should be string, not ${typeof str}`);
    if (str.length < 8 || limit !== false && str.length > limit)
      throw new TypeError(`Wrong string length: ${str.length} (${str}). Expected (8..${limit})`);
    const lowered = str.toLowerCase();
    if (str !== lowered && str !== str.toUpperCase())
      throw new Error(`String must be lowercase or uppercase`);
    const sepIndex = lowered.lastIndexOf("1");
    if (sepIndex === 0 || sepIndex === -1)
      throw new Error(`Letter "1" must be present between prefix and data only`);
    const prefix2 = lowered.slice(0, sepIndex);
    const data = lowered.slice(sepIndex + 1);
    if (data.length < 6)
      throw new Error("Data must be at least 6 characters long");
    const words = BECH_ALPHABET.decode(data).slice(0, -6);
    const sum = bechChecksum(prefix2, words, ENCODING_CONST);
    if (!data.endsWith(sum))
      throw new Error(`Invalid checksum in ${str}: expected "${sum}"`);
    return { prefix: prefix2, words };
  }
  const decodeUnsafe = unsafeWrapper(decode36);
  function decodeToBytes(str) {
    const { prefix: prefix2, words } = decode36(str, false);
    return { prefix: prefix2, words, bytes: fromWords(words) };
  }
  function encodeFromBytes(prefix2, bytes3) {
    return encode32(prefix2, toWords(bytes3));
  }
  return {
    encode: encode32,
    decode: decode36,
    encodeFromBytes,
    decodeToBytes,
    decodeUnsafe,
    fromWords,
    fromWordsUnsafe,
    toWords
  };
}
var bech32 = genBech32("bech32");
var bech32m = genBech32("bech32m");
var hex = chain(radix2(4), alphabet2("0123456789abcdef"), join(""), normalize((s) => {
  if (typeof s !== "string" || s.length % 2)
    throw new TypeError(`hex.decode: expected string, got ${typeof s} with length ${s.length}`);
  return s.toLowerCase();
}));

// node_modules/@scure/bip39/esm/index.js
var isJapanese = (wordlist2) => wordlist2[0] === "あいこくしん";
function nfkd(str) {
  if (typeof str !== "string")
    throw new TypeError(`Invalid mnemonic type: ${typeof str}`);
  return str.normalize("NFKD");
}
function normalize2(str) {
  const norm = nfkd(str);
  const words = norm.split(" ");
  if (![12, 15, 18, 21, 24].includes(words.length))
    throw new Error("Invalid mnemonic");
  return { nfkd: norm, words };
}
function assertEntropy(entropy) {
  bytes(entropy, 16, 20, 24, 28, 32);
}
var calcChecksum = (entropy) => {
  const bitsLeft = 8 - entropy.length / 4;
  return new Uint8Array([sha256(entropy)[0] >> bitsLeft << bitsLeft]);
};
function getCoder(wordlist2) {
  if (!Array.isArray(wordlist2) || wordlist2.length !== 2048 || typeof wordlist2[0] !== "string")
    throw new Error("Wordlist: expected array of 2048 strings");
  wordlist2.forEach((i) => {
    if (typeof i !== "string")
      throw new Error(`Wordlist: non-string element: ${i}`);
  });
  return utils2.chain(utils2.checksum(1, calcChecksum), utils2.radix2(11, true), utils2.alphabet(wordlist2));
}
function mnemonicToEntropy(mnemonic, wordlist2) {
  const { words } = normalize2(mnemonic);
  const entropy = getCoder(wordlist2).decode(words);
  assertEntropy(entropy);
  return entropy;
}
function entropyToMnemonic(entropy, wordlist2) {
  assertEntropy(entropy);
  const words = getCoder(wordlist2).encode(entropy);
  return words.join(isJapanese(wordlist2) ? "　" : " ");
}

// node_modules/@scure/bip39/esm/wordlists/english.js
var wordlist = `abandon
ability
able
about
above
absent
absorb
abstract
absurd
abuse
access
accident
account
accuse
achieve
acid
acoustic
acquire
across
act
action
actor
actress
actual
adapt
add
addict
address
adjust
admit
adult
advance
advice
aerobic
affair
afford
afraid
again
age
agent
agree
ahead
aim
air
airport
aisle
alarm
album
alcohol
alert
alien
all
alley
allow
almost
alone
alpha
already
also
alter
always
amateur
amazing
among
amount
amused
analyst
anchor
ancient
anger
angle
angry
animal
ankle
announce
annual
another
answer
antenna
antique
anxiety
any
apart
apology
appear
apple
approve
april
arch
arctic
area
arena
argue
arm
armed
armor
army
around
arrange
arrest
arrive
arrow
art
artefact
artist
artwork
ask
aspect
assault
asset
assist
assume
asthma
athlete
atom
attack
attend
attitude
attract
auction
audit
august
aunt
author
auto
autumn
average
avocado
avoid
awake
aware
away
awesome
awful
awkward
axis
baby
bachelor
bacon
badge
bag
balance
balcony
ball
bamboo
banana
banner
bar
barely
bargain
barrel
base
basic
basket
battle
beach
bean
beauty
because
become
beef
before
begin
behave
behind
believe
below
belt
bench
benefit
best
betray
better
between
beyond
bicycle
bid
bike
bind
biology
bird
birth
bitter
black
blade
blame
blanket
blast
bleak
bless
blind
blood
blossom
blouse
blue
blur
blush
board
boat
body
boil
bomb
bone
bonus
book
boost
border
boring
borrow
boss
bottom
bounce
box
boy
bracket
brain
brand
brass
brave
bread
breeze
brick
bridge
brief
bright
bring
brisk
broccoli
broken
bronze
broom
brother
brown
brush
bubble
buddy
budget
buffalo
build
bulb
bulk
bullet
bundle
bunker
burden
burger
burst
bus
business
busy
butter
buyer
buzz
cabbage
cabin
cable
cactus
cage
cake
call
calm
camera
camp
can
canal
cancel
candy
cannon
canoe
canvas
canyon
capable
capital
captain
car
carbon
card
cargo
carpet
carry
cart
case
cash
casino
castle
casual
cat
catalog
catch
category
cattle
caught
cause
caution
cave
ceiling
celery
cement
census
century
cereal
certain
chair
chalk
champion
change
chaos
chapter
charge
chase
chat
cheap
check
cheese
chef
cherry
chest
chicken
chief
child
chimney
choice
choose
chronic
chuckle
chunk
churn
cigar
cinnamon
circle
citizen
city
civil
claim
clap
clarify
claw
clay
clean
clerk
clever
click
client
cliff
climb
clinic
clip
clock
clog
close
cloth
cloud
clown
club
clump
cluster
clutch
coach
coast
coconut
code
coffee
coil
coin
collect
color
column
combine
come
comfort
comic
common
company
concert
conduct
confirm
congress
connect
consider
control
convince
cook
cool
copper
copy
coral
core
corn
correct
cost
cotton
couch
country
couple
course
cousin
cover
coyote
crack
cradle
craft
cram
crane
crash
crater
crawl
crazy
cream
credit
creek
crew
cricket
crime
crisp
critic
crop
cross
crouch
crowd
crucial
cruel
cruise
crumble
crunch
crush
cry
crystal
cube
culture
cup
cupboard
curious
current
curtain
curve
cushion
custom
cute
cycle
dad
damage
damp
dance
danger
daring
dash
daughter
dawn
day
deal
debate
debris
decade
december
decide
decline
decorate
decrease
deer
defense
define
defy
degree
delay
deliver
demand
demise
denial
dentist
deny
depart
depend
deposit
depth
deputy
derive
describe
desert
design
desk
despair
destroy
detail
detect
develop
device
devote
diagram
dial
diamond
diary
dice
diesel
diet
differ
digital
dignity
dilemma
dinner
dinosaur
direct
dirt
disagree
discover
disease
dish
dismiss
disorder
display
distance
divert
divide
divorce
dizzy
doctor
document
dog
doll
dolphin
domain
donate
donkey
donor
door
dose
double
dove
draft
dragon
drama
drastic
draw
dream
dress
drift
drill
drink
drip
drive
drop
drum
dry
duck
dumb
dune
during
dust
dutch
duty
dwarf
dynamic
eager
eagle
early
earn
earth
easily
east
easy
echo
ecology
economy
edge
edit
educate
effort
egg
eight
either
elbow
elder
electric
elegant
element
elephant
elevator
elite
else
embark
embody
embrace
emerge
emotion
employ
empower
empty
enable
enact
end
endless
endorse
enemy
energy
enforce
engage
engine
enhance
enjoy
enlist
enough
enrich
enroll
ensure
enter
entire
entry
envelope
episode
equal
equip
era
erase
erode
erosion
error
erupt
escape
essay
essence
estate
eternal
ethics
evidence
evil
evoke
evolve
exact
example
excess
exchange
excite
exclude
excuse
execute
exercise
exhaust
exhibit
exile
exist
exit
exotic
expand
expect
expire
explain
expose
express
extend
extra
eye
eyebrow
fabric
face
faculty
fade
faint
faith
fall
false
fame
family
famous
fan
fancy
fantasy
farm
fashion
fat
fatal
father
fatigue
fault
favorite
feature
february
federal
fee
feed
feel
female
fence
festival
fetch
fever
few
fiber
fiction
field
figure
file
film
filter
final
find
fine
finger
finish
fire
firm
first
fiscal
fish
fit
fitness
fix
flag
flame
flash
flat
flavor
flee
flight
flip
float
flock
floor
flower
fluid
flush
fly
foam
focus
fog
foil
fold
follow
food
foot
force
forest
forget
fork
fortune
forum
forward
fossil
foster
found
fox
fragile
frame
frequent
fresh
friend
fringe
frog
front
frost
frown
frozen
fruit
fuel
fun
funny
furnace
fury
future
gadget
gain
galaxy
gallery
game
gap
garage
garbage
garden
garlic
garment
gas
gasp
gate
gather
gauge
gaze
general
genius
genre
gentle
genuine
gesture
ghost
giant
gift
giggle
ginger
giraffe
girl
give
glad
glance
glare
glass
glide
glimpse
globe
gloom
glory
glove
glow
glue
goat
goddess
gold
good
goose
gorilla
gospel
gossip
govern
gown
grab
grace
grain
grant
grape
grass
gravity
great
green
grid
grief
grit
grocery
group
grow
grunt
guard
guess
guide
guilt
guitar
gun
gym
habit
hair
half
hammer
hamster
hand
happy
harbor
hard
harsh
harvest
hat
have
hawk
hazard
head
health
heart
heavy
hedgehog
height
hello
helmet
help
hen
hero
hidden
high
hill
hint
hip
hire
history
hobby
hockey
hold
hole
holiday
hollow
home
honey
hood
hope
horn
horror
horse
hospital
host
hotel
hour
hover
hub
huge
human
humble
humor
hundred
hungry
hunt
hurdle
hurry
hurt
husband
hybrid
ice
icon
idea
identify
idle
ignore
ill
illegal
illness
image
imitate
immense
immune
impact
impose
improve
impulse
inch
include
income
increase
index
indicate
indoor
industry
infant
inflict
inform
inhale
inherit
initial
inject
injury
inmate
inner
innocent
input
inquiry
insane
insect
inside
inspire
install
intact
interest
into
invest
invite
involve
iron
island
isolate
issue
item
ivory
jacket
jaguar
jar
jazz
jealous
jeans
jelly
jewel
job
join
joke
journey
joy
judge
juice
jump
jungle
junior
junk
just
kangaroo
keen
keep
ketchup
key
kick
kid
kidney
kind
kingdom
kiss
kit
kitchen
kite
kitten
kiwi
knee
knife
knock
know
lab
label
labor
ladder
lady
lake
lamp
language
laptop
large
later
latin
laugh
laundry
lava
law
lawn
lawsuit
layer
lazy
leader
leaf
learn
leave
lecture
left
leg
legal
legend
leisure
lemon
lend
length
lens
leopard
lesson
letter
level
liar
liberty
library
license
life
lift
light
like
limb
limit
link
lion
liquid
list
little
live
lizard
load
loan
lobster
local
lock
logic
lonely
long
loop
lottery
loud
lounge
love
loyal
lucky
luggage
lumber
lunar
lunch
luxury
lyrics
machine
mad
magic
magnet
maid
mail
main
major
make
mammal
man
manage
mandate
mango
mansion
manual
maple
marble
march
margin
marine
market
marriage
mask
mass
master
match
material
math
matrix
matter
maximum
maze
meadow
mean
measure
meat
mechanic
medal
media
melody
melt
member
memory
mention
menu
mercy
merge
merit
merry
mesh
message
metal
method
middle
midnight
milk
million
mimic
mind
minimum
minor
minute
miracle
mirror
misery
miss
mistake
mix
mixed
mixture
mobile
model
modify
mom
moment
monitor
monkey
monster
month
moon
moral
more
morning
mosquito
mother
motion
motor
mountain
mouse
move
movie
much
muffin
mule
multiply
muscle
museum
mushroom
music
must
mutual
myself
mystery
myth
naive
name
napkin
narrow
nasty
nation
nature
near
neck
need
negative
neglect
neither
nephew
nerve
nest
net
network
neutral
never
news
next
nice
night
noble
noise
nominee
noodle
normal
north
nose
notable
note
nothing
notice
novel
now
nuclear
number
nurse
nut
oak
obey
object
oblige
obscure
observe
obtain
obvious
occur
ocean
october
odor
off
offer
office
often
oil
okay
old
olive
olympic
omit
once
one
onion
online
only
open
opera
opinion
oppose
option
orange
orbit
orchard
order
ordinary
organ
orient
original
orphan
ostrich
other
outdoor
outer
output
outside
oval
oven
over
own
owner
oxygen
oyster
ozone
pact
paddle
page
pair
palace
palm
panda
panel
panic
panther
paper
parade
parent
park
parrot
party
pass
patch
path
patient
patrol
pattern
pause
pave
payment
peace
peanut
pear
peasant
pelican
pen
penalty
pencil
people
pepper
perfect
permit
person
pet
phone
photo
phrase
physical
piano
picnic
picture
piece
pig
pigeon
pill
pilot
pink
pioneer
pipe
pistol
pitch
pizza
place
planet
plastic
plate
play
please
pledge
pluck
plug
plunge
poem
poet
point
polar
pole
police
pond
pony
pool
popular
portion
position
possible
post
potato
pottery
poverty
powder
power
practice
praise
predict
prefer
prepare
present
pretty
prevent
price
pride
primary
print
priority
prison
private
prize
problem
process
produce
profit
program
project
promote
proof
property
prosper
protect
proud
provide
public
pudding
pull
pulp
pulse
pumpkin
punch
pupil
puppy
purchase
purity
purpose
purse
push
put
puzzle
pyramid
quality
quantum
quarter
question
quick
quit
quiz
quote
rabbit
raccoon
race
rack
radar
radio
rail
rain
raise
rally
ramp
ranch
random
range
rapid
rare
rate
rather
raven
raw
razor
ready
real
reason
rebel
rebuild
recall
receive
recipe
record
recycle
reduce
reflect
reform
refuse
region
regret
regular
reject
relax
release
relief
rely
remain
remember
remind
remove
render
renew
rent
reopen
repair
repeat
replace
report
require
rescue
resemble
resist
resource
response
result
retire
retreat
return
reunion
reveal
review
reward
rhythm
rib
ribbon
rice
rich
ride
ridge
rifle
right
rigid
ring
riot
ripple
risk
ritual
rival
river
road
roast
robot
robust
rocket
romance
roof
rookie
room
rose
rotate
rough
round
route
royal
rubber
rude
rug
rule
run
runway
rural
sad
saddle
sadness
safe
sail
salad
salmon
salon
salt
salute
same
sample
sand
satisfy
satoshi
sauce
sausage
save
say
scale
scan
scare
scatter
scene
scheme
school
science
scissors
scorpion
scout
scrap
screen
script
scrub
sea
search
season
seat
second
secret
section
security
seed
seek
segment
select
sell
seminar
senior
sense
sentence
series
service
session
settle
setup
seven
shadow
shaft
shallow
share
shed
shell
sheriff
shield
shift
shine
ship
shiver
shock
shoe
shoot
shop
short
shoulder
shove
shrimp
shrug
shuffle
shy
sibling
sick
side
siege
sight
sign
silent
silk
silly
silver
similar
simple
since
sing
siren
sister
situate
six
size
skate
sketch
ski
skill
skin
skirt
skull
slab
slam
sleep
slender
slice
slide
slight
slim
slogan
slot
slow
slush
small
smart
smile
smoke
smooth
snack
snake
snap
sniff
snow
soap
soccer
social
sock
soda
soft
solar
soldier
solid
solution
solve
someone
song
soon
sorry
sort
soul
sound
soup
source
south
space
spare
spatial
spawn
speak
special
speed
spell
spend
sphere
spice
spider
spike
spin
spirit
split
spoil
sponsor
spoon
sport
spot
spray
spread
spring
spy
square
squeeze
squirrel
stable
stadium
staff
stage
stairs
stamp
stand
start
state
stay
steak
steel
stem
step
stereo
stick
still
sting
stock
stomach
stone
stool
story
stove
strategy
street
strike
strong
struggle
student
stuff
stumble
style
subject
submit
subway
success
such
sudden
suffer
sugar
suggest
suit
summer
sun
sunny
sunset
super
supply
supreme
sure
surface
surge
surprise
surround
survey
suspect
sustain
swallow
swamp
swap
swarm
swear
sweet
swift
swim
swing
switch
sword
symbol
symptom
syrup
system
table
tackle
tag
tail
talent
talk
tank
tape
target
task
taste
tattoo
taxi
teach
team
tell
ten
tenant
tennis
tent
term
test
text
thank
that
theme
then
theory
there
they
thing
this
thought
three
thrive
throw
thumb
thunder
ticket
tide
tiger
tilt
timber
time
tiny
tip
tired
tissue
title
toast
tobacco
today
toddler
toe
together
toilet
token
tomato
tomorrow
tone
tongue
tonight
tool
tooth
top
topic
topple
torch
tornado
tortoise
toss
total
tourist
toward
tower
town
toy
track
trade
traffic
tragic
train
transfer
trap
trash
travel
tray
treat
tree
trend
trial
tribe
trick
trigger
trim
trip
trophy
trouble
truck
true
truly
trumpet
trust
truth
try
tube
tuition
tumble
tuna
tunnel
turkey
turn
turtle
twelve
twenty
twice
twin
twist
two
type
typical
ugly
umbrella
unable
unaware
uncle
uncover
under
undo
unfair
unfold
unhappy
uniform
unique
unit
universe
unknown
unlock
until
unusual
unveil
update
upgrade
uphold
upon
upper
upset
urban
urge
usage
use
used
useful
useless
usual
utility
vacant
vacuum
vague
valid
valley
valve
van
vanish
vapor
various
vast
vault
vehicle
velvet
vendor
venture
venue
verb
verify
version
very
vessel
veteran
viable
vibrant
vicious
victory
video
view
village
vintage
violin
virtual
virus
visa
visit
visual
vital
vivid
vocal
voice
void
volcano
volume
vote
voyage
wage
wagon
wait
walk
wall
walnut
want
warfare
warm
warrior
wash
wasp
waste
water
wave
way
wealth
weapon
wear
weasel
weather
web
wedding
weekend
weird
welcome
west
wet
whale
what
wheat
wheel
when
where
whip
whisper
wide
width
wife
wild
will
win
window
wine
wing
wink
winner
winter
wire
wisdom
wise
wish
witness
wolf
woman
wonder
wood
wool
word
work
world
worry
worth
wrap
wreck
wrestle
wrist
write
wrong
yard
year
yellow
you
young
youth
zebra
zero
zone
zoo`.split("\n");

// node_modules/@storacha/capabilities/dist/provider.js
var provider_exports = {};
__export(provider_exports, {
  AccountDID: () => AccountDID,
  Provider: () => Provider,
  add: () => add3
});
var Provider = did_exports2.match({ method: "web" });
var add3 = capability({
  can: "provider/add",
  with: AccountDID,
  nb: struct({
    provider: Provider,
    consumer: SpaceDID
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.provider, parent.nb.provider, "provider")) || and3(equal(child.nb.consumer, parent.nb.consumer, "consumer")) || ok({});
  }
});

// node_modules/@storacha/access/dist/provider.js
var { Provider: ProviderDID2, AccountDID: AccountDID2 } = provider_exports;
var add4 = async (agent, { account, consumer, provider = (
  /** @type {API.ProviderDID} */
  agent.connection.id.did()
), proofs: proofs2 }) => {
  if (!ProviderDID2.is(provider)) {
    throw new Error(`Unable to determine provider from agent.connection.id did ${provider}. expected a did:web:`);
  }
  const { out } = await agent.invokeAndExecute(add3, {
    with: account,
    nb: {
      provider,
      consumer
    },
    proofs: proofs2
  });
  return out;
};

// node_modules/@storacha/access/dist/space-access.js
var VALID_COMBINATIONS = {
  "google-kms": ["RSA_DECRYPT_OAEP_3072_SHA256"]
  // Add more providers and algorithms here as needed
};
var SpaceAccess = class {
  /**
   * Creates and validates a space access configuration
   *
   * @template {API.SpaceAccessType} T
   * @param {T} [access] - The access configuration to validate
   * @returns {T}
   * @throws {Error} When access configuration is invalid
   */
  static from(access2) {
    if (!access2 || access2.type === "public") {
      return (
        /** @type {T} */
        { type: "public" }
      );
    }
    if (access2.type === "private") {
      if (!access2.encryption) {
        throw new Error("Private access type requires encryption configuration");
      }
      const { provider, algorithm: algorithm2 } = access2.encryption;
      if (!VALID_COMBINATIONS[provider]) {
        throw new Error(`unknown encryption provider: ${provider}`);
      }
      if (!VALID_COMBINATIONS[provider].includes(algorithm2)) {
        throw new Error(`unknown encryption algorithm: ${algorithm2} for provider: ${provider}`);
      }
      return (
        /** @type {T} */
        access2
      );
    }
    throw new Error(`unknown access type: ${/** @type {any} */
    access2.type}`);
  }
  /**
   * Creates a public space access configuration
   *
   * @returns {API.PublicAccess}
   */
  static public() {
    return { type: "public" };
  }
  /**
   * Creates a private space access configuration with encryption provider
   *
   * @param {string} [provider] - The encryption provider. Defaults to `google-kms`.
   * @param {string} [algorithm] - The encryption algorithm. Defaults to `RSA_DECRYPT_OAEP_3072_SHA256`.
   * @returns {API.PrivateAccess<API.EncryptionProvider>}
   * @throws {Error} When provider/algorithm combination is invalid
   */
  static private(provider = "google-kms", algorithm2 = "RSA_DECRYPT_OAEP_3072_SHA256") {
    if (!VALID_COMBINATIONS[provider]) {
      throw new Error(`unknown encryption provider: ${provider}`);
    }
    if (!VALID_COMBINATIONS[provider].includes(algorithm2)) {
      throw new Error(`unknown encryption algorithm: ${algorithm2} for provider: ${provider}`);
    }
    return {
      type: "private",
      encryption: { provider, algorithm: algorithm2 }
    };
  }
};

// node_modules/@storacha/access/dist/space.js
var generate2 = async ({ name: name15, access: access2, agent }) => {
  const { signer } = await generate();
  const normalizedAccess = SpaceAccess.from(access2);
  return new OwnedSpace({ signer, name: name15, access: normalizedAccess, agent });
};
var fromMnemonic = async (mnemonic, { name: name15, access: access2, agent }) => {
  const secret = mnemonicToEntropy(mnemonic, wordlist);
  const signer = await derive2(secret);
  const normalizedAccess = SpaceAccess.from(access2);
  return new OwnedSpace({ signer, name: name15, access: normalizedAccess, agent });
};
var toMnemonic = ({ signer }) => {
  const secret = signer.secret;
  return entropyToMnemonic(secret, wordlist);
};
var createRecovery = (space2, account) => createAuthorization(space2, {
  audience: did_exports.parse(account),
  access: accountAccess,
  expiration: Infinity
});
var SESSION_LIFETIME = 60 * 60 * 24 * 365;
var createAuthorization = async ({ signer, name: name15, access: access2 }, { audience, access: spaceAccess3 = spaceAccess, expiration = lib_exports.now() + SESSION_LIFETIME }) => {
  const normalizedAccess = SpaceAccess.from(access2);
  const facts = [{ space: { name: name15, access: normalizedAccess } }];
  return await delegate({
    issuer: signer,
    audience,
    capabilities: toCapabilities2({
      [signer.did()]: spaceAccess3
    }),
    ...expiration ? { expiration } : {},
    facts
  });
};
var toCapabilities2 = (allow) => {
  const capabilities = [];
  for (const [subject, access2] of Object.entries(allow)) {
    const entries3 = (
      /** @type {[API.Ability, API.Unit][]} */
      Object.entries(access2)
    );
    for (const [can, details] of entries3) {
      if (details) {
        capabilities.push({ can, with: subject });
      }
    }
  }
  return (
    /** @type {API.Capabilities} */
    capabilities
  );
};
var OwnedSpace = class _OwnedSpace {
  /**
   * @param {Model<S>} model
   */
  constructor(model) {
    this.model = model;
  }
  get signer() {
    return this.model.signer;
  }
  get name() {
    return this.model.name;
  }
  get access() {
    return SpaceAccess.from(this.model.access);
  }
  did() {
    return this.signer.did();
  }
  /**
   * Creates a renamed version of this space.
   *
   * @param {string} name
   */
  withName(name15) {
    return new _OwnedSpace({
      signer: this.signer,
      name: name15,
      access: this.access
    });
  }
  /**
   * Saves account in the agent store so it can be accessed across sessions.
   *
   * @param {object} input
   * @param {API.Agent<S>} [input.agent]
   * @returns {Promise<API.Result<API.Unit, Error>>}
   */
  async save({ agent = this.model.agent } = {}) {
    if (!agent) {
      return fail2("Please provide an agent to save the space into");
    }
    const proof = await createAuthorization(this, { audience: agent });
    await agent.importSpaceFromDelegation(proof);
    await agent.setCurrentSpace(this.did());
    return { ok: {} };
  }
  /**
   * @param {Authorization} authorization
   * @param {object} options
   * @param {API.Agent<S>} [options.agent]
   */
  provision({ proofs: proofs2 }, { agent = this.model.agent } = {}) {
    if (!agent) {
      return fail2("Please provide an agent to save the space into");
    }
    return provision(this, { proofs: proofs2, agent });
  }
  /**
   * Creates a (UCAN) delegation that gives full access to the space to the
   * specified `account`. At the moment we only allow `did:mailto` principal
   * to be used as an `account`.
   *
   * @param {API.AccountDID} account
   */
  async createRecovery(account) {
    return createRecovery(this, account);
  }
  /**
   * Creates (UCAN) delegation that gives specified `agent` an access to
   * specified ability (passed as `access.can` field) on the this space.
   * Optionally, you can specify `access.expiration` field to set the
   *
   * @param {API.Principal} principal
   * @param {object} [input]
   * @param {API.Access} [input.access]
   * @param {API.UCAN.UTCUnixTimestamp} [input.expiration]
   */
  createAuthorization(principal2, input11) {
    return createAuthorization(this, { ...input11, audience: principal2 });
  }
  /**
   * Derives BIP39 mnemonic that can be used to recover the space.
   *
   * @returns {string}
   */
  toMnemonic() {
    return toMnemonic(this);
  }
};
var SpaceDID2 = schema_exports3.did({ method: "key" });
var fromDelegation = (delegation) => {
  var _a15;
  const result = SpaceDID2.read(delegation.capabilities[0].with);
  if (result.error) {
    throw Object.assign(new Error(`Invalid delegation, expected capabilities[0].with to be DID, ${result.error}`), {
      cause: result.error
    });
  }
  const meta = ((_a15 = delegation.facts[0]) == null ? void 0 : _a15.space) ?? {};
  meta.access = SpaceAccess.from(meta.access);
  return new SharedSpace({ id: result.ok, delegation, meta });
};
var provision = async (space2, { proofs: proofs2, agent }) => {
  const [capability2] = proofs2[0].capabilities;
  const { ok: account, error: reason } = AccountDID2.read(capability2.with);
  if (reason) {
    return error(reason);
  }
  return await add4(agent, {
    consumer: space2.did(),
    account,
    proofs: proofs2
  });
};
var SharedSpace = class _SharedSpace {
  /**
   * @typedef {object} SharedSpaceModel
   * @property {API.SpaceDID} id
   * @property {API.Delegation} delegation
   * @property {{name?:string, access?:API.SpaceAccessType}} meta
   * @property {API.Agent} [agent]
   *
   * @param {SharedSpaceModel} model
   */
  constructor(model) {
    this.model = model;
  }
  get delegation() {
    return this.model.delegation;
  }
  get meta() {
    return this.model.meta;
  }
  get name() {
    return this.meta.name ?? "";
  }
  get access() {
    return SpaceAccess.from(this.meta.access);
  }
  did() {
    return this.model.id;
  }
  /**
   * @param {string} name
   */
  withName(name15) {
    return new _SharedSpace({
      ...this.model,
      meta: { ...this.meta, name: name15, access: this.access }
    });
  }
};

// node_modules/@storacha/access/dist/delegations.js
function isExpired2(delegation) {
  if (delegation.expiration === void 0 || delegation.expiration <= Math.floor(Date.now() / 1e3)) {
    return true;
  }
  return false;
}
function isTooEarly2(delegation) {
  if (!delegation.notBefore) {
    return false;
  }
  return delegation.notBefore > Math.floor(Date.now() / 1e3);
}
function validate(delegation, opts) {
  const { checkAudience, checkIsExpired = true, checkIsTooEarly = true } = opts ?? {};
  if (checkAudience && delegation.audience.did() !== checkAudience.did()) {
    throw new Error(`Delegation audience ${delegation.audience.did()} does not match required DID ${checkAudience.did()}`);
  }
  if (checkIsExpired && isExpired2(delegation)) {
    throw new Error(`Delegation expired.`);
  }
  if (checkIsTooEarly && isTooEarly2(delegation)) {
    throw new Error(`Delegation is not active yet (too early).`);
  }
}
function canDelegateCapability(delegation, capability2) {
  const allowsCapabilities = delegation_exports.allows(delegation);
  for (const [uri2, abilities] of Object.entries(allowsCapabilities)) {
    if (matchResource(
      /** @type {API.Resource} */
      uri2,
      capability2.with
    )) {
      const cans = (
        /** @type {API.Ability[]} */
        Object.keys(abilities)
      );
      for (const can of cans) {
        if (canDelegateAbility(can, capability2.can)) {
          return true;
        }
      }
    }
  }
  return false;
}
var matchResource = (resource, query) => {
  if (query === "ucan:*") {
    return true;
  } else if (typeof query === "string") {
    return resource === query;
  } else {
    return query.test(resource);
  }
};

// node_modules/@ucanto/principal/src/rsa.js
var rsa_exports = {};
__export(rsa_exports, {
  Verifier: () => RSAVerifier,
  code: () => code10,
  decode: () => decode28,
  from: () => from10,
  generate: () => generate3,
  name: () => name8,
  or: () => or8,
  signatureAlgorithm: () => signatureAlgorithm3,
  signatureCode: () => signatureCode3
});

// node_modules/one-webcrypto/browser.mjs
var _globalReference = globalThis || window || self;
var webcrypto = _globalReference.crypto;

// node_modules/@ucanto/principal/src/multiformat.js
var tagWith = (code19, bytes3) => {
  const offset2 = varint_exports.encodingLength(code19);
  const multiformat = new Uint8Array(bytes3.byteLength + offset2);
  varint_exports.encodeTo(code19, multiformat, 0);
  multiformat.set(bytes3, offset2);
  return multiformat;
};
var untagWith = (code19, source, byteOffset = 0) => {
  const bytes3 = byteOffset !== 0 ? source.subarray(byteOffset) : source;
  const [tag2, size5] = varint_exports.decode(bytes3);
  if (tag2 !== code19) {
    throw new Error(
      `Expected multiformat with 0x${code19.toString(
        16
      )} tag instead got 0x${tag2.toString(16)}`
    );
  } else {
    return new Uint8Array(bytes3.buffer, bytes3.byteOffset + size5);
  }
};
var encodingLength2 = varint_exports.encodingLength;
var encodeTo2 = varint_exports.encodeTo;
var decode24 = varint_exports.decode;

// node_modules/@ucanto/principal/src/rsa/spki.js
var API28 = __toESM(require_lib(), 1);

// node_modules/@ucanto/principal/src/rsa/asn1.js
var TAG_SIZE = 1;
var INT_TAG = 2;
var BITSTRING_TAG = 3;
var OCTET_STRING_TAG = 4;
var SEQUENCE_TAG = 48;
var UNUSED_BIT_PAD = 0;
var encodeDERLength = (length2) => {
  if (length2 <= 127) {
    return new Uint8Array([length2]);
  }
  const octets = [];
  while (length2 !== 0) {
    octets.push(length2 & 255);
    length2 = length2 >>> 8;
  }
  octets.reverse();
  return new Uint8Array([128 | octets.length & 255, ...octets]);
};
var readDERLength = (bytes3, offset2 = 0) => {
  if ((bytes3[offset2] & 128) === 0) {
    return { number: bytes3[offset2], consumed: 1 };
  }
  const numberBytes = bytes3[offset2] & 127;
  if (bytes3.length < numberBytes + 1) {
    throw new Error(
      `ASN parsing error: Too few bytes. Expected encoded length's length to be at least ${numberBytes}`
    );
  }
  let length2 = 0;
  for (let i = 0; i < numberBytes; i++) {
    length2 = length2 << 8;
    length2 = length2 | bytes3[offset2 + i + 1];
  }
  return { number: length2, consumed: numberBytes + 1 };
};
var skip = (input11, expectedTag, position) => {
  const parsed = into(input11, expectedTag, position);
  return parsed.position + parsed.length;
};
var into = (input11, expectedTag, offset2) => {
  const actualTag = input11[offset2];
  if (actualTag !== expectedTag) {
    throw new Error(
      `ASN parsing error: Expected tag 0x${expectedTag.toString(
        16
      )} at position ${offset2}, but got 0x${actualTag.toString(16)}.`
    );
  }
  const length2 = readDERLength(input11, offset2 + TAG_SIZE);
  const position = offset2 + TAG_SIZE + length2.consumed;
  return { position, length: length2.number };
};
var encodeBitString = (input11) => {
  const length2 = encodeDERLength(input11.byteLength + 1);
  const bytes3 = new Uint8Array(
    TAG_SIZE + // ASN_BITSTRING_TAG
    length2.byteLength + 1 + // amount of unused bits at the end of our bitstring
    input11.byteLength
  );
  let byteOffset = 0;
  bytes3[byteOffset] = BITSTRING_TAG;
  byteOffset += TAG_SIZE;
  bytes3.set(length2, byteOffset);
  byteOffset += length2.byteLength;
  bytes3[byteOffset] = UNUSED_BIT_PAD;
  byteOffset += 1;
  bytes3.set(input11, byteOffset);
  return bytes3;
};
var encodeOctetString = (input11) => {
  const length2 = encodeDERLength(input11.byteLength);
  const bytes3 = new Uint8Array(TAG_SIZE + length2.byteLength + input11.byteLength);
  let byteOffset = 0;
  bytes3[byteOffset] = OCTET_STRING_TAG;
  byteOffset += TAG_SIZE;
  bytes3.set(length2, byteOffset);
  byteOffset += length2.byteLength;
  bytes3.set(input11, byteOffset);
  return bytes3;
};
var encodeSequence = (sequence) => {
  let byteLength = 0;
  for (const item of sequence) {
    byteLength += item.byteLength;
  }
  const length2 = encodeDERLength(byteLength);
  const bytes3 = new Uint8Array(TAG_SIZE + length2.byteLength + byteLength);
  let byteOffset = 0;
  bytes3[byteOffset] = SEQUENCE_TAG;
  byteOffset += TAG_SIZE;
  bytes3.set(length2, byteOffset);
  byteOffset += length2.byteLength;
  for (const item of sequence) {
    bytes3.set(item, byteOffset);
    byteOffset += item.byteLength;
  }
  return bytes3;
};
var readSequence = (bytes3, offset2 = 0) => {
  const { position, length: length2 } = into(bytes3, SEQUENCE_TAG, offset2);
  return new Uint8Array(bytes3.buffer, bytes3.byteOffset + position, length2);
};
var encodeInt = (input11) => {
  const extra = input11.byteLength === 0 || input11[0] & 128 ? 1 : 0;
  const length2 = encodeDERLength(input11.byteLength + extra);
  const bytes3 = new Uint8Array(
    TAG_SIZE + // INT_TAG
    length2.byteLength + input11.byteLength + extra
  );
  let byteOffset = 0;
  bytes3[byteOffset] = INT_TAG;
  byteOffset += TAG_SIZE;
  bytes3.set(length2, byteOffset);
  byteOffset += length2.byteLength;
  if (extra > 0) {
    bytes3[byteOffset] = UNUSED_BIT_PAD;
    byteOffset += extra;
  }
  bytes3.set(input11, byteOffset);
  return bytes3;
};
var enterSequence = (bytes3, offset2 = 0) => into(bytes3, SEQUENCE_TAG, offset2).position;
var skipSequence = (bytes3, offset2 = 0) => skip(bytes3, SEQUENCE_TAG, offset2);
var skipInt = (bytes3, offset2 = 0) => skip(bytes3, INT_TAG, offset2);
var readBitString = (bytes3, offset2 = 0) => {
  const { position, length: length2 } = into(bytes3, BITSTRING_TAG, offset2);
  const tag2 = bytes3[position];
  if (tag2 !== UNUSED_BIT_PAD) {
    throw new Error(
      `Can not read bitstring, expected length to be multiple of 8, but got ${tag2} unused bits in last byte.`
    );
  }
  return new Uint8Array(
    bytes3.buffer,
    bytes3.byteOffset + position + 1,
    length2 - 1
  );
};
var readInt2 = (bytes3, byteOffset = 0) => {
  const { position, length: length2 } = into(bytes3, INT_TAG, byteOffset);
  let delta = 0;
  while (bytes3[position + delta] === 0) {
    delta++;
  }
  return new Uint8Array(
    bytes3.buffer,
    bytes3.byteOffset + position + delta,
    length2 - delta
  );
};
var readOctetString = (bytes3, offset2 = 0) => {
  const { position, length: length2 } = into(bytes3, OCTET_STRING_TAG, offset2);
  return new Uint8Array(bytes3.buffer, bytes3.byteOffset + position, length2);
};
var readSequenceWith = (readers, source, byteOffset = 0) => {
  const results = [];
  const sequence = readSequence(source, byteOffset);
  let offset2 = 0;
  for (const read8 of readers) {
    const chunk = read8(sequence, offset2);
    results.push(chunk);
    offset2 = chunk.byteOffset + chunk.byteLength - sequence.byteOffset;
  }
  return results;
};

// node_modules/@ucanto/principal/src/rsa/spki.js
var SPKI_PARAMS_ENCODED = new Uint8Array([
  48,
  13,
  6,
  9,
  42,
  134,
  72,
  134,
  247,
  13,
  1,
  1,
  1,
  5,
  0
]);
var encode22 = (key) => encodeSequence([SPKI_PARAMS_ENCODED, encodeBitString(key)]);
var decode25 = (info2) => {
  const offset2 = enterSequence(info2, 0);
  const keyOffset = skipSequence(info2, offset2);
  return readBitString(info2, keyOffset);
};

// node_modules/@ucanto/principal/src/rsa/pkcs8.js
var API29 = __toESM(require_lib(), 1);
var PKSC8_HEADER = new Uint8Array([
  // version
  2,
  1,
  0,
  // privateKeyAlgorithm
  48,
  13,
  6,
  9,
  42,
  134,
  72,
  134,
  247,
  13,
  1,
  1,
  1,
  5,
  0
]);
var decode26 = (info2) => {
  let offset2 = 0;
  offset2 = enterSequence(info2, offset2);
  offset2 = skipInt(info2, offset2);
  offset2 = skipSequence(info2, offset2);
  return readOctetString(info2, offset2);
};
var encode23 = (key) => encodeSequence([PKSC8_HEADER, encodeOctetString(key)]);

// node_modules/@ucanto/principal/src/rsa/private-key.js
var API31 = __toESM(require_lib(), 1);

// node_modules/@ucanto/principal/src/rsa/public-key.js
var API30 = __toESM(require_lib(), 1);
var encode24 = ({ n, e }) => encodeSequence([encodeInt(n), encodeInt(e)]);

// node_modules/@ucanto/principal/src/rsa/private-key.js
var VERSION2 = new Uint8Array();
var decode27 = (source, byteOffset = 0) => {
  const [v, n, e, d, p, q, dp, dq, qi] = readSequenceWith(
    [
      readInt2,
      readInt2,
      readInt2,
      readInt2,
      readInt2,
      readInt2,
      readInt2,
      readInt2,
      readInt2
    ],
    source,
    byteOffset
  );
  return { v, n, e, d, p, q, dp, dq, qi };
};

// node_modules/@ucanto/principal/src/rsa.js
var name8 = "RSA";
var code10 = 4869;
var verifierCode = 4613;
var signatureCode3 = RS256;
var signatureAlgorithm3 = "RS256";
var ALG = "RSASSA-PKCS1-v1_5";
var HASH_ALG = "SHA-256";
var KEY_SIZE2 = 2048;
var SALT_LENGTH = 128;
var IMPORT_PARAMS = {
  name: ALG,
  hash: { name: HASH_ALG }
};
var generate3 = async ({
  size: size5 = KEY_SIZE2,
  extractable = false
} = {}) => {
  const { publicKey, privateKey } = await webcrypto.subtle.generateKey(
    {
      name: ALG,
      modulusLength: size5,
      publicExponent: new Uint8Array([1, 0, 1]),
      hash: { name: HASH_ALG }
    },
    extractable,
    ["sign", "verify"]
  );
  const spki = await webcrypto.subtle.exportKey("spki", publicKey);
  const publicBytes = tagWith(verifierCode, decode25(new Uint8Array(spki)));
  const verifier = new RSAVerifier({ bytes: publicBytes, publicKey });
  if (!extractable) {
    return new UnextractableRSASigner({
      privateKey,
      verifier
    });
  } else {
    const pkcs8 = await webcrypto.subtle.exportKey("pkcs8", privateKey);
    const bytes3 = tagWith(code10, decode26(new Uint8Array(pkcs8)));
    return new ExtractableRSASigner({
      privateKey,
      bytes: bytes3,
      verifier
    });
  }
};
var from10 = ({ id, keys: keys2 }) => {
  if (id.startsWith("did:key:")) {
    const did2 = (
      /** @type {API.DIDKey} */
      id
    );
    const key = keys2[did2];
    if (key instanceof Uint8Array) {
      return decode28(key);
    } else {
      return new UnextractableRSASigner({
        privateKey: key,
        verifier: RSAVerifier.parse(did2)
      });
    }
  } else {
    throw new TypeError(
      `RSA can not import from ${id} archive, try generic Signer instead`
    );
  }
};
var or8 = (other) => or6({ from: from10 }, other);
var decode28 = (bytes3) => {
  const rsa = decode27(untagWith(code10, bytes3));
  const publicBytes = tagWith(verifierCode, encode24(rsa));
  return new ExtractableRSASigner({
    bytes: bytes3,
    privateKey: webcrypto.subtle.importKey(
      "pkcs8",
      encode23(untagWith(code10, bytes3)),
      IMPORT_PARAMS,
      true,
      ["sign"]
    ),
    verifier: RSAVerifier.decode(publicBytes)
  });
};
var RSAVerifier = class _RSAVerifier {
  /**
   * @param {object} options
   * @param {API.Await<CryptoKey>} options.publicKey
   * @param {API.ByteView<API.RSAVerifier>} options.bytes
   */
  constructor({ publicKey, bytes: bytes3 }) {
    this.publicKey = publicKey;
    this.bytes = bytes3;
  }
  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Verifier<ID, typeof signatureCode>}
   */
  withDID(id) {
    return withDID(this, id);
  }
  toDIDKey() {
    return this.did();
  }
  /**
   * @param {API.ByteView<API.RSAVerifier>} bytes
   * @returns {API.RSAVerifier}
   */
  static decode(bytes3) {
    return new this({
      bytes: bytes3,
      publicKey: webcrypto.subtle.importKey(
        "spki",
        encode22(untagWith(verifierCode, bytes3)),
        IMPORT_PARAMS,
        true,
        ["verify"]
      )
    });
  }
  /**
   * @param {API.DIDKey} did
   * @returns {API.RSAVerifier}
   */
  static parse(did2) {
    return _RSAVerifier.decode(
      /** @type {Uint8Array} */
      parse2(did2)
    );
  }
  /**
   * @param {API.PrincipalParser} other
   */
  static or(other) {
    return or4(this, other);
  }
  /** @type {typeof verifierCode} */
  get code() {
    return verifierCode;
  }
  /**
   * @type {typeof signatureCode}
   */
  get signatureCode() {
    return signatureCode3;
  }
  /**
   * @type {typeof signatureAlgorithm}
   */
  get signatureAlgorithm() {
    return signatureAlgorithm3;
  }
  /**
   * DID of the Principal in `did:key` format.
   * @returns {API.DID<"key">}
   */
  did() {
    return `did:key:${base58btc.encode(this.bytes)}`;
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, typeof this.signatureCode>} signature
   * @returns {Promise<boolean>}
   */
  async verify(payload, signature) {
    if (signature.code !== signatureCode3) {
      return false;
    }
    return webcrypto.subtle.verify(
      { name: ALG, hash: { name: HASH_ALG } },
      await this.publicKey,
      signature.raw,
      payload
    );
  }
};
var RSASigner = class {
  /**
   * @param {object} options
   * @param {API.Await<CryptoKey>} options.privateKey
   * @param {API.RSAVerifier} options.verifier
   */
  constructor({ privateKey, verifier }) {
    this.verifier = verifier;
    this.privateKey = privateKey;
  }
  get signer() {
    return this;
  }
  /**
   * @type {typeof code}
   */
  get code() {
    return code10;
  }
  /**
   * @type {typeof signatureCode}
   */
  get signatureCode() {
    return signatureCode3;
  }
  /**
   * @type {typeof signatureAlgorithm}
   */
  get signatureAlgorithm() {
    return signatureAlgorithm3;
  }
  did() {
    return this.verifier.did();
  }
  toDIDKey() {
    return this.verifier.toDIDKey();
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, typeof this.signatureCode>} signature
   */
  verify(payload, signature) {
    return this.verifier.verify(payload, signature);
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @returns {Promise<API.SignatureView<T, typeof signatureCode>>}
   */
  async sign(payload) {
    const buffer2 = await webcrypto.subtle.sign(
      { name: ALG, saltLength: SALT_LENGTH },
      await this.privateKey,
      payload
    );
    return create3(signatureCode3, new Uint8Array(buffer2));
  }
};
var ExtractableRSASigner = class extends RSASigner {
  /**
   * @param {object} options
   * @param {API.Await<CryptoKey>} options.privateKey
   * @param {EncodedSigner} options.bytes
   * @param {API.RSAVerifier} options.verifier
   */
  constructor(options) {
    super(options);
    this.bytes = options.bytes;
  }
  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Signer<ID, typeof signatureCode>}
   */
  withDID(id) {
    return withDID2(this, id);
  }
  toArchive() {
    const id = this.did();
    return {
      id,
      keys: { [id]: this.bytes }
    };
  }
};
var UnextractableRSASigner = class extends RSASigner {
  /**
   * @param {object} options
   * @param {CryptoKey} options.privateKey
   * @param {API.RSAVerifier} options.verifier
   */
  constructor(options) {
    super(options);
    this.privateKey = options.privateKey;
  }
  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Signer<ID, typeof signatureCode>}
   */
  withDID(id) {
    return withDID2(this, id);
  }
  toArchive() {
    const id = this.did();
    return {
      id,
      keys: { [id]: this.privateKey }
    };
  }
};

// node_modules/@ucanto/principal/src/lib.js
var Verifier = verifier_exports.or(RSAVerifier);
var Signer = or7(rsa_exports);

// node_modules/@storacha/access/dist/agent-data.js
var Ucanto = __toESM(require_lib(), 1);

// node_modules/@storacha/capabilities/dist/assert.js
var linkOrDigest = () => schema_exports3.link().or(schema_exports3.struct({ digest: schema_exports3.bytes() }));
var assert = capability({
  can: "assert/*",
  with: uri_exports.match({ protocol: "did:" })
});
var location = capability({
  can: "assert/location",
  with: uri_exports.match({ protocol: "did:" }),
  nb: schema_exports3.struct({
    /** Blob CID or multihash */
    content: linkOrDigest(),
    location: schema_exports3.array(uri_exports),
    range: schema_exports3.struct({
      offset: schema_exports3.integer(),
      length: schema_exports3.integer().optional()
    }).optional(),
    space: schema_exports3.principal().optional()
  }),
  derives: (claimed, delegated) => {
    var _a15, _b12, _c5, _d4;
    return and3(equalWith(claimed, delegated)) || and3(equalLinkOrDigestContent(claimed, delegated)) || and3(equal(claimed.nb.location, delegated.nb.location, "location")) || and3(equal((_a15 = claimed.nb.range) == null ? void 0 : _a15.offset, (_b12 = delegated.nb.range) == null ? void 0 : _b12.offset, "offset")) || and3(equal((_c5 = claimed.nb.range) == null ? void 0 : _c5.length, (_d4 = delegated.nb.range) == null ? void 0 : _d4.length, "length")) || and3(equal(claimed.nb.space, delegated.nb.space, "space")) || ok({});
  }
});
var inclusion = capability({
  can: "assert/inclusion",
  with: uri_exports.match({ protocol: "did:" }),
  nb: schema_exports3.struct({
    /** CAR CID */
    content: linkOrDigest(),
    /** CARv2 index CID */
    includes: schema_exports3.link({ version: 1 }),
    proof: schema_exports3.link({ version: 1 }).optional()
  })
});
var index = capability({
  can: "assert/index",
  with: uri_exports.match({ protocol: "did:" }),
  nb: schema_exports3.struct({
    /** DAG root CID */
    content: linkOrDigest(),
    /**
     * Link to a Content Archive that contains the index.
     * e.g. `index/sharded/dag@0.1`
     *
     * @see https://github.com/storacha/specs/blob/main/w3-index.md
     */
    index: schema_exports3.link({ version: 1 })
  }),
  derives: (claimed, delegated) => and3(equalWith(claimed, delegated)) || and3(equal(claimed.nb.content, delegated.nb.content, "content")) || and3(equal(claimed.nb.index, delegated.nb.index, "index")) || ok({})
});
var partition = capability({
  can: "assert/partition",
  with: uri_exports.match({ protocol: "did:" }),
  nb: schema_exports3.struct({
    /** Content root CID */
    content: linkOrDigest(),
    /** CIDs CID */
    blocks: schema_exports3.link({ version: 1 }).optional(),
    parts: schema_exports3.array(schema_exports3.link({ version: 1 }))
  })
});
var relation = capability({
  can: "assert/relation",
  with: uri_exports.match({ protocol: "did:" }),
  nb: schema_exports3.struct({
    content: linkOrDigest(),
    /** CIDs this content links to directly. */
    children: schema_exports3.array(schema_exports3.link()),
    /** Parts this content and it's children can be read from. */
    parts: schema_exports3.array(schema_exports3.struct({
      content: schema_exports3.link({ version: 1 }),
      /** CID of contents (CARv2 index) included in this part. */
      includes: schema_exports3.struct({
        content: schema_exports3.link({ version: 1 }),
        /** CIDs of parts this index may be found in. */
        parts: schema_exports3.array(schema_exports3.link({ version: 1 })).optional()
      }).optional()
    }))
  })
});
var equals4 = capability({
  can: "assert/equals",
  with: uri_exports.match({ protocol: "did:" }),
  nb: schema_exports3.struct({
    content: linkOrDigest(),
    equals: schema_exports3.link()
  }),
  derives: (claimed, delegated) => and3(equalWith(claimed, delegated)) || and3(equalLinkOrDigestContent(claimed, delegated)) || and3(equal(claimed.nb.equals, delegated.nb.equals, "equals")) || ok({})
});

// node_modules/@storacha/capabilities/dist/claim.js
var multiaddr = schema_exports3.bytes();
var claim3 = capability({
  can: "claim/*",
  with: uri_exports.match({ protocol: "did:" })
});
var cache2 = capability({
  can: "claim/cache",
  with: uri_exports.match({ protocol: "did:" }),
  nb: schema_exports3.struct({
    claim: schema_exports3.link({ version: 1 }),
    provider: schema_exports3.struct({
      addresses: schema_exports3.array(multiaddr)
    })
  }),
  derives: (claimed, delegated) => and3(equalWith(claimed, delegated)) || and3(equal(claimed.nb.claim, delegated.nb.claim, "claim")) || and3(equalProviderAddresses(claimed, delegated)) || ok({})
});
var equalProviderAddresses = (claimed, delegated) => {
  var _a15, _b12, _c5, _d4;
  if ((_b12 = (_a15 = delegated.nb) == null ? void 0 : _a15.provider) == null ? void 0 : _b12.addresses) {
    const delegatedAddrs = delegated.nb.provider.addresses;
    const claimedAddrs = ((_d4 = (_c5 = claimed.nb) == null ? void 0 : _c5.provider) == null ? void 0 : _d4.addresses) ?? [];
    if (claimedAddrs.length !== delegatedAddrs.length) {
      return fail2(`Constraint violation: ${claimedAddrs.length} provider addresses violates imposed constraint ${delegatedAddrs.length} provider addresses`);
    }
    for (let i = 0; i < delegatedAddrs.length; i++) {
      const addr = delegatedAddrs[i];
      const found = claimedAddrs.some((a) => equals(addr, a));
      if (!found) {
        return fail2(`Constraint violation: provider address ${i} is not an allowed provider address`);
      }
    }
  }
  return ok({});
};

// node_modules/@storacha/capabilities/dist/consumer.js
var ProviderDID3 = did_exports2.match({ method: "web" });
var has = capability({
  can: "consumer/has",
  with: ProviderDID3,
  nb: struct({
    consumer: SpaceDID
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.consumer, parent.nb.consumer, "consumer")) || ok({});
  }
});
var get4 = capability({
  can: "consumer/get",
  with: ProviderDID3,
  nb: struct({
    consumer: SpaceDID
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.consumer, parent.nb.consumer, "consumer")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/customer.js
var ProviderDID4 = did_exports2.match({ method: "web" });
var get5 = capability({
  can: "customer/get",
  with: ProviderDID4,
  nb: struct({
    customer: AccountDID
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.customer, parent.nb.customer, "customer")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/console.js
var console2 = capability({
  can: "console/*",
  with: schema_exports3.did(),
  derives: equalWith
});
var log = capability({
  can: "console/log",
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    value: schema_exports3.unknown()
  }),
  derives: equalWith
});
var error3 = capability({
  can: "console/error",
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    error: schema_exports3.unknown()
  }),
  derives: equalWith
});

// node_modules/@storacha/capabilities/dist/rate-limit.js
var Provider2 = did_exports2;
var add5 = capability({
  can: "rate-limit/add",
  with: Provider2,
  nb: struct({
    subject: schema_exports3.string(),
    rate: schema_exports3.number()
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.subject, parent.nb.subject, "subject")) || and3(equal(child.nb.rate, parent.nb.rate, "rate")) || ok({});
  }
});
var remove3 = capability({
  can: "rate-limit/remove",
  with: Provider2,
  nb: struct({
    id: schema_exports3.string()
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.id, parent.nb.id, "id")) || ok({});
  }
});
var list3 = capability({
  can: "rate-limit/list",
  with: Provider2,
  nb: struct({
    subject: schema_exports3.string()
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.subject, parent.nb.subject, "subject")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/admin.js
var admin = capability({
  can: "admin/*",
  with: ProviderDID,
  derives: equalWith
});
var upload2 = {
  /**
   * Capability can be invoked by a provider to get information about a content CID.
   */
  inspect: capability({
    can: "admin/upload/inspect",
    with: ProviderDID,
    nb: struct({
      root: link_exports2
    }),
    derives: (child, parent) => {
      return and3(equalWith(child, parent)) || and3(equal(child.nb.root, parent.nb.root, "root")) || ok({});
    }
  })
};
var store2 = {
  /**
   * Capability can be invoked by a provider to get information an upload shard CID.
   */
  inspect: capability({
    can: "admin/store/inspect",
    with: ProviderDID,
    nb: struct({
      link: link_exports2
    }),
    derives: (child, parent) => {
      return and3(equalWith(child, parent)) || and3(equal(child.nb.link, parent.nb.link, "link")) || ok({});
    }
  })
};

// node_modules/@storacha/capabilities/dist/subscription.js
var subscription_exports = {};
__export(subscription_exports, {
  ProviderDID: () => ProviderDID5,
  get: () => get6,
  list: () => list4
});
var ProviderDID5 = did_exports2.match({ method: "web" });
var get6 = capability({
  can: "subscription/get",
  with: ProviderDID5,
  nb: struct({
    subscription: schema_exports3.string()
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.subscription, parent.nb.subscription, "consumer")) || ok({});
  }
});
var list4 = capability({
  can: "subscription/list",
  with: AccountDID,
  derives: equalWith
});

// node_modules/@storacha/capabilities/dist/filecoin/index.js
var filecoin_exports = {};
__export(filecoin_exports, {
  accept: () => filecoinAccept,
  filecoin: () => filecoin,
  info: () => filecoinInfo,
  offer: () => filecoinOffer,
  submit: () => filecoinSubmit
});

// node_modules/@storacha/capabilities/dist/filecoin/lib.js
var FR32_SHA2_256_TRUNC254_PADDED_BINARY_TREE = (
  /** @type {const} */
  4113
);
var RAW_CODE = (
  /** @type {const} */
  85
);
var PieceLink = (
  /** @type {import('../types.js').PieceLinkSchema} */
  schema_exports3.link({
    code: RAW_CODE,
    version: 1,
    multihash: {
      code: FR32_SHA2_256_TRUNC254_PADDED_BINARY_TREE
    }
  })
);

// node_modules/@storacha/capabilities/dist/filecoin/storefront.js
var filecoin = capability({
  can: "filecoin/*",
  /**
   * DID of the space the content is stored in.
   */
  with: schema_exports3.did(),
  derives: equalWith
});
var filecoinOffer = capability({
  can: "filecoin/offer",
  /**
   * DID of the space the content is stored in.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * CID of the content that resulted in Filecoin piece.
     */
    content: schema_exports3.link(),
    /**
     * CID of the piece.
     */
    piece: PieceLink
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.content, from18.nb.content, "nb.content")) || and3(checkLink(claim5.nb.piece, from18.nb.piece, "nb.piece")) || ok({});
  }
});
var filecoinSubmit = capability({
  can: "filecoin/submit",
  /**
   * DID of the Storefront.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * CID of the content that resulted in Filecoin piece.
     */
    content: schema_exports3.link(),
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.content, from18.nb.content, "nb.content")) || and3(checkLink(claim5.nb.piece, from18.nb.piece, "nb.piece")) || ok({});
  }
});
var filecoinAccept = capability({
  can: "filecoin/accept",
  /**
   * DID of the Storefront.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * CID of the content that resulted in Filecoin piece.
     */
    content: schema_exports3.link(),
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.content, from18.nb.content, "nb.content")) || and3(checkLink(claim5.nb.piece, from18.nb.piece, "nb.piece")) || ok({});
  }
});
var filecoinInfo = capability({
  can: "filecoin/info",
  /**
   * DID of the space the content is stored in.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.piece, from18.nb.piece, "nb.piece")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/filecoin/aggregator.js
var pieceOffer = capability({
  can: "piece/offer",
  /**
   * DID of an authorized Storefront.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * CID of the piece.
     */
    piece: PieceLink,
    /**
     * Grouping of joining segments into an aggregate.
     */
    group: schema_exports3.text()
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.piece, from18.nb.piece, "nb.piece")) || and3(equal(claim5.nb.group, from18.nb.group, "nb.group")) || ok({});
  }
});
var pieceAccept = capability({
  can: "piece/accept",
  /**
   * DID of the Aggregator.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink,
    /**
     * Grouping of joining segments into an aggregate.
     */
    group: schema_exports3.text()
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.piece, from18.nb.piece, "nb.piece")) || and3(equal(claim5.nb.group, from18.nb.group, "nb.group")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/filecoin/dealer.js
var aggregateOffer = capability({
  can: "aggregate/offer",
  /**
   * DID of an authorized Storefront.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * Commitment proof for the aggregate being offered.
     */
    aggregate: PieceLink,
    /**
     * CID of the DAG-CBOR encoded block with offer details.
     * Service will queue given offer to be validated and handled.
     */
    pieces: schema_exports3.link({ version: 1 })
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.aggregate, from18.nb.aggregate, "nb.aggregate")) || and3(checkLink(claim5.nb.pieces, from18.nb.pieces, "nb.pieces")) || ok({});
  }
});
var aggregateAccept = capability({
  can: "aggregate/accept",
  /**
   * did:key identifier of the broker authority where offer is made available.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * Commitment proof for the aggregate being offered.
     */
    aggregate: PieceLink,
    /**
     * CID of the DAG-CBOR encoded block with offer details.
     * Service will queue given offer to be validated and handled.
     */
    pieces: schema_exports3.link()
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.aggregate, from18.nb.aggregate, "nb.aggregate")) || and3(checkLink(claim5.nb.pieces, from18.nb.pieces, "nb.pieces")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/filecoin/deal-tracker.js
var dealInfo = capability({
  can: "deal/info",
  /**
   * DID of the Storefront.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(checkLink(claim5.nb.piece, from18.nb.piece, "nb.piece")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/space/index.js
var space_exports3 = {};
__export(space_exports3, {
  Schema: () => schema_exports3,
  add: () => add6,
  index: () => index2
});
var index2 = capability({
  can: "space/index/*",
  /** DID of the space where indexed data is stored. */
  with: SpaceDID,
  derives: equalWith
});
var add6 = capability({
  can: "space/index/add",
  /** DID of the space where indexed data is stored. */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /** Content Archive (CAR) containing the `Index`. */
    index: schema_exports3.link({ code: car_exports.code, version: 1 })
  }),
  derives: (claimed, delegated) => and3(equalWith(claimed, delegated)) || and3(equal(claimed.nb.index, delegated.nb.index, "index")) || ok({})
});

// node_modules/@storacha/capabilities/dist/plan.js
var plan_exports = {};
__export(plan_exports, {
  createAdminSession: () => createAdminSession,
  get: () => get7,
  set: () => set
});
var get7 = capability({
  can: "plan/get",
  with: AccountDID,
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || ok({});
  }
});
var set = capability({
  can: "plan/set",
  with: AccountDID,
  nb: struct({
    product: did_exports2
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.product, parent.nb.product, "product")) || ok({});
  }
});
var createAdminSession = capability({
  can: "plan/create-admin-session",
  with: AccountDID,
  nb: struct({
    returnURL: schema_exports3.string()
  }),
  derives: (child, parent) => {
    return and3(equalWith(child, parent)) || and3(equal(child.nb.returnURL, parent.nb.returnURL, "returnURL")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/usage.js
var usage_exports = {};
__export(usage_exports, {
  report: () => report,
  usage: () => usage
});
var usage = capability({
  can: "usage/*",
  /** DID of the (memory) space where usage is derived. */
  with: SpaceDID,
  derives: equalWith
});
var report = capability({
  can: "usage/report",
  with: SpaceDID,
  nb: schema_exports3.struct({
    /** Period to retrieve events between. */
    period: schema_exports3.struct({
      /** Time in seconds after Unix epoch (inclusive). */
      from: schema_exports3.integer().greaterThan(-1),
      /** Time in seconds after Unix epoch (exclusive). */
      to: schema_exports3.integer().greaterThan(-1)
    })
  }),
  derives: (child, parent) => {
    var _a15, _b12, _c5, _d4;
    return and3(equalWith(child, parent)) || and3(equal((_a15 = child.nb.period) == null ? void 0 : _a15.from, (_b12 = parent.nb.period) == null ? void 0 : _b12.from, "period.from")) || and3(equal((_c5 = child.nb.period) == null ? void 0 : _c5.to, (_d4 = parent.nb.period) == null ? void 0 : _d4.to, "period.to")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/space/blob.js
var blob_exports = {};
__export(blob_exports, {
  Schema: () => schema_exports3,
  add: () => add7,
  blob: () => blob,
  content: () => content,
  get: () => get8,
  list: () => list5,
  remove: () => remove4,
  replicate: () => replicate
});
var blob = capability({
  can: "space/blob/*",
  /**
   * DID of the (memory) space where Blob is intended to
   * be stored.
   */
  with: SpaceDID,
  derives: equalWith
});
var content = schema_exports3.struct({
  /**
   * A multihash digest of the blob payload bytes, uniquely identifying blob.
   */
  digest: schema_exports3.bytes(),
  /**
   * Number of bytes contained by this blob. Service will provision write target
   * for this exact size. Attempt to write a larger Blob file will fail.
   */
  size: schema_exports3.integer()
});
var add7 = capability({
  can: "space/blob/add",
  /**
   * DID of the (memory) space where Blob is intended to
   * be stored.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * Blob to be added on the space.
     */
    blob: content
  }),
  derives: equalBlob
});
var remove4 = capability({
  can: "space/blob/remove",
  /**
   * DID of the (memory) space where Blob is stored.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * A multihash digest of the blob payload bytes, uniquely identifying blob.
     */
    digest: schema_exports3.bytes()
  }),
  derives: (claimed, delegated) => {
    if (claimed.with !== delegated.with) {
      return fail2(`Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`);
    } else if (delegated.nb.digest && !equals(delegated.nb.digest, claimed.nb.digest)) {
      return fail2(`Link ${claimed.nb.digest ? `${claimed.nb.digest}` : ""} violates imposed ${delegated.nb.digest} constraint.`);
    }
    return ok({});
  }
});
var list5 = capability({
  can: "space/blob/list",
  /**
   * DID of the (memory) space where Blobs to be listed are stored.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * A pointer that can be moved back and forth on the list.
     * It can be used to paginate a list for instance.
     */
    cursor: schema_exports3.string().optional(),
    /**
     * Maximum number of items per page.
     */
    size: schema_exports3.integer().optional()
  }),
  derives: (claimed, delegated) => {
    if (claimed.with !== delegated.with) {
      return fail2(`Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`);
    }
    return ok({});
  }
});
var get8 = capability({
  can: "space/blob/get/0/1",
  /**
   * DID of the (memory) space where Blob is stored.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * A multihash digest of the blob payload bytes, uniquely identifying blob.
     */
    digest: schema_exports3.bytes()
  }),
  derives: (claimed, delegated) => {
    if (claimed.with !== delegated.with) {
      return fail2(`Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`);
    } else if (delegated.nb.digest && !equals(delegated.nb.digest, claimed.nb.digest)) {
      return fail2(`Link ${claimed.nb.digest ? `${claimed.nb.digest}` : ""} violates imposed ${delegated.nb.digest} constraint.`);
    }
    return ok({});
  }
});
var replicate = capability({
  can: "space/blob/replicate",
  with: SpaceDID,
  nb: schema_exports3.struct({
    /** Blob to replicate. */
    blob: content,
    /**
     * The number of replicas to ensure. e.g. `replicas: 2` will ensure 2 copies
     * of the data exist in the network.
     */
    replicas: schema_exports3.integer().greaterThan(0),
    /** Link to a location commitment indicating where the Blob must be fetched from. */
    site: schema_exports3.link({ version: 1 })
  }),
  derives: (claimed, delegated) => and3(equalWith(claimed, delegated)) || and3(equalBlob(claimed, delegated)) || and3(equal(claimed.nb.replicas, delegated.nb.replicas, "replicas")) || and3(checkLink(claimed.nb.site, delegated.nb.site, "site")) || ok({})
});

// node_modules/@storacha/capabilities/dist/blob/index.js
var blob2 = capability({
  can: "blob/*",
  /** Storage provider DID. */
  with: schema_exports3.did(),
  derives: equalWith
});
var allocate2 = capability({
  can: "blob/allocate",
  /** Storage provider DID. */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /** Blob to allocate. */
    blob: content,
    /** Link to the add blob task that initiated the allocation. */
    cause: schema_exports3.link({ version: 1 }),
    /** DID of the user space where the allocation takes place. */
    space: schema_exports3.principal({ method: "key" })
  }),
  derives: (claimed, delegated) => {
    var _a15, _b12;
    return and3(equalWith(claimed, delegated)) || and3(equalBlob(claimed, delegated)) || and3(checkLink(claimed.nb.cause, delegated.nb.cause, "cause")) || and3(equal((_a15 = claimed.nb.space) == null ? void 0 : _a15.did(), (_b12 = delegated.nb.space) == null ? void 0 : _b12.did(), "space")) || ok({});
  }
});
var accept = capability({
  can: "blob/accept",
  /** Storage provider DID. */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /** Blob to accept. */
    blob: content,
    /** DID of the user space where allocation took place. */
    space: schema_exports3.principal({ method: "key" }),
    /** This task is blocked on `http/put` receipt available */
    _put: Await
  }),
  derives: (claimed, delegated) => {
    var _a15, _b12;
    return and3(equalWith(claimed, delegated)) || and3(equalBlob(claimed, delegated)) || and3(equal((_a15 = claimed.nb.space) == null ? void 0 : _a15.did(), (_b12 = delegated.nb.space) == null ? void 0 : _b12.did(), "space")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/web3.storage/blob.js
var blob3 = capability({
  can: "web3.storage/blob/*",
  /**
   * DID of the (memory) space where Blob is intended to
   * be stored.
   */
  with: SpaceDID,
  derives: equalWith
});
var allocate3 = capability({
  can: "web3.storage/blob/allocate",
  /**
   * Provider DID.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * Blob to allocate on the space.
     */
    blob: content,
    /**
     * The Link for an Add Blob task, that caused an allocation
     */
    cause: link_exports2,
    /**
     * DID of the user space where allocation takes place
     */
    space: SpaceDID
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(equalBlob(claim5, from18)) || and3(checkLink(claim5.nb.cause, from18.nb.cause, "cause")) || and3(equal(claim5.nb.space, from18.nb.space, "space")) || ok({});
  }
});
var accept2 = capability({
  can: "web3.storage/blob/accept",
  /**
   * Provider DID.
   */
  with: schema_exports3.did(),
  nb: schema_exports3.struct({
    /**
     * Blob to accept.
     */
    blob: content,
    /**
     * Content location commitment time to live, which will be encoded as expiry of the issued location claim.
     */
    ttl: schema_exports3.integer().optional(),
    /**
     * DID of the user space where allocation took place
     */
    space: SpaceDID,
    /**
     * This task is blocked on `http/put` receipt available
     */
    _put: Await
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(equalBlob(claim5, from18)) || and3(equal(claim5.nb.ttl, from18.nb.ttl, "ttl")) || and3(equal(claim5.nb.space, from18.nb.space, "space")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/http.js
var put = capability({
  can: "http/put",
  /**
   * DID of the (memory) space where Blob is intended to
   * be stored.
   */
  with: SpaceDID,
  nb: schema_exports3.struct({
    /**
     * Description of body to send (digest/size).
     */
    body: content,
    /**
     * HTTP(S) location that can receive blob content via HTTP PUT request.
     */
    url: schema_exports3.string().or(Await),
    /**
     * HTTP headers.
     */
    headers: schema_exports3.dictionary({ value: schema_exports3.string() }).or(Await)
  }),
  derives: (claim5, from18) => {
    return and3(equalWith(claim5, from18)) || and3(equalBody(claim5, from18)) || and3(equal(claim5.nb.url, from18.nb, "url")) || and3(equal(claim5.nb.headers, from18.nb, "headers")) || ok({});
  }
});

// node_modules/@storacha/capabilities/dist/index.js
var abilitiesAsStrings = [
  top.can,
  assert.can,
  equals4.can,
  inclusion.can,
  index.can,
  location.can,
  partition.can,
  relation.can,
  claim3.can,
  cache2.can,
  add3.can,
  space.can,
  info.can,
  EncryptionSetup.can,
  EncryptionKeyDecrypt.can,
  upload.can,
  add2.can,
  get3.can,
  remove2.can,
  list2.can,
  store.can,
  add.can,
  get2.can,
  remove.can,
  list.can,
  access.can,
  authorize.can,
  attest.can,
  conclude.can,
  get5.can,
  has.can,
  get4.can,
  get6.can,
  list4.can,
  add5.can,
  remove3.can,
  list3.can,
  filecoinOffer.can,
  filecoinSubmit.can,
  filecoinAccept.can,
  filecoinInfo.can,
  pieceOffer.can,
  pieceAccept.can,
  aggregateOffer.can,
  aggregateAccept.can,
  dealInfo.can,
  admin.can,
  upload2.inspect.can,
  store2.inspect.can,
  get7.can,
  set.can,
  createAdminSession.can,
  usage.can,
  report.can,
  blob2.can,
  allocate2.can,
  accept.can,
  blob.can,
  add7.can,
  remove4.can,
  list5.can,
  blob3.can,
  allocate3.can,
  accept2.can,
  put.can,
  index2.can,
  add6.can
];

// node_modules/@storacha/access/dist/utils/buffers.js
function uint8ArrayToArrayBuffer(array2) {
  if (array2.byteOffset === 0 && array2.byteLength === array2.buffer.byteLength) {
    return array2.buffer;
  } else {
    return array2.buffer.slice(array2.byteOffset, array2.byteLength + array2.byteOffset);
  }
}

// node_modules/@storacha/access/dist/agent-data.js
var _save;
var _AgentData = class _AgentData {
  /**
   * @param {import('./types.js').AgentDataModel} data
   * @param {import('./types.js').AgentDataOptions} [options]
   */
  constructor(data, options = {}) {
    /** @type {(data: import('./types.js').AgentDataExport) => Promise<void> | void} */
    __privateAdd(this, _save);
    this.meta = data.meta;
    this.principal = data.principal;
    this.spaces = data.spaces;
    this.delegations = data.delegations;
    this.currentSpace = data.currentSpace;
    __privateSet(this, _save, (data2) => options.store ? options.store.save(data2) : void 0);
  }
  /**
   * Create a new AgentData instance from the passed initialization data.
   *
   * @param {Partial<import('./types.js').AgentDataModel>} [init]
   * @param {import('./types.js').AgentDataOptions} [options]
   */
  static async create(init2 = {}, options = {}) {
    const agentData = new _AgentData({
      meta: { name: "agent", type: "device", ...init2.meta },
      principal: init2.principal ?? await signer_exports.generate(),
      spaces: init2.spaces ?? /* @__PURE__ */ new Map(),
      delegations: init2.delegations ?? /* @__PURE__ */ new Map(),
      currentSpace: init2.currentSpace
    }, options);
    if (options.store) {
      await options.store.save(agentData.export());
    }
    return agentData;
  }
  /**
   * Instantiate AgentData from previously exported data.
   *
   * @param {import('./types.js').AgentDataExport} raw
   * @param {import('./types.js').AgentDataOptions} [options]
   */
  static fromExport(raw, options) {
    const dels = /* @__PURE__ */ new Map();
    for (const [key, value] of raw.delegations) {
      dels.set(key, {
        delegation: importDAG(value.delegation.map((d) => ({
          cid: CID.parse(d.cid).toV1(),
          bytes: d.bytes instanceof Uint8Array ? d.bytes : new Uint8Array(d.bytes)
        }))),
        meta: value.meta
      });
    }
    return new _AgentData({
      meta: raw.meta,
      // @ts-expect-error for some reason TS thinks this is a EdSigner
      principal: Signer.from(raw.principal),
      currentSpace: raw.currentSpace,
      spaces: raw.spaces,
      delegations: dels
    }, options);
  }
  /**
   * Export data in a format safe to pass to `structuredClone()`.
   */
  export() {
    const raw = {
      meta: this.meta,
      principal: this.principal.toArchive(),
      currentSpace: this.currentSpace,
      spaces: this.spaces,
      delegations: /* @__PURE__ */ new Map()
    };
    for (const [key, value] of this.delegations) {
      raw.delegations.set(key, {
        meta: value.meta,
        delegation: [...value.delegation.export()].map((b) => ({
          cid: b.cid.toString(),
          bytes: uint8ArrayToArrayBuffer(b.bytes)
        }))
      });
    }
    return raw;
  }
  /**
   * @param {import('@ucanto/interface').DID} did
   * @param {import('./types.js').SpaceMeta} meta
   * @param {import('@ucanto/interface').Delegation} [proof]
   */
  async addSpace(did2, meta, proof) {
    this.spaces.set(did2, meta);
    await (proof ? this.addDelegation(proof) : __privateGet(this, _save).call(this, this.export()));
  }
  /**
   * @deprecated
   * @param {import('@ucanto/interface').DID<'key'>} did
   */
  async setCurrentSpace(did2) {
    this.currentSpace = did2;
    await __privateGet(this, _save).call(this, this.export());
  }
  /**
   * @param {import('@ucanto/interface').Delegation} delegation
   * @param {import('./types.js').DelegationMeta} [meta]
   */
  async addDelegation(delegation, meta) {
    this.delegations.set(delegation.cid.toString(), {
      delegation,
      meta: meta ?? {}
    });
    await __privateGet(this, _save).call(this, this.export());
  }
  /**
   * @param {import('@ucanto/interface').UCANLink} cid
   */
  async removeDelegation(cid) {
    this.delegations.delete(cid.toString());
    await __privateGet(this, _save).call(this, this.export());
  }
};
_save = new WeakMap();
var AgentData = _AgentData;
var isSessionCapability = (cap) => cap.can === ucan_exports2.attest.can;
var isSessionProof = (delegation) => delegation.capabilities.some((cap) => isSessionCapability(cap));
function getSessionProofs(data) {
  const proofs2 = {};
  for (const { delegation } of data.delegations.values()) {
    if (isSessionProof(delegation)) {
      const cap = delegation.capabilities[0];
      if (cap && !isExpired2(delegation)) {
        const proof = cap.nb.proof;
        if (proof) {
          const proofCid = proof.toString();
          const issuerDid = delegation.issuer.did();
          proofs2[proofCid] = proofs2[proofCid] ?? {};
          proofs2[proofCid][issuerDid] = proofs2[proofCid][issuerDid] ?? [];
          proofs2[proofCid][issuerDid].push(delegation);
        }
      }
    }
  }
  return proofs2;
}

// node_modules/@storacha/access/dist/agent.js
__reExport(agent_exports, types_exports);

// node_modules/@storacha/access/dist/agent-use-cases.js
var DIDWeb = schema_exports3.DID.match({ method: "web" });
async function requestAccess(access2, account, capabilities) {
  const res = await access2.invokeAndExecute(authorize, {
    audience: access2.connection.id,
    with: access2.issuer.did(),
    nb: {
      iss: account.did(),
      att: [...capabilities]
    }
  });
  if (res == null ? void 0 : res.out.error) {
    throw res.out.error;
  }
}
async function claimAccess(access2, audienceOfClaimedDelegations = access2.connection.id.did(), { addProofs = false, nonce } = {}) {
  const res = await access2.invokeAndExecute(claim, {
    audience: access2.connection.id,
    with: audienceOfClaimedDelegations,
    nonce
  });
  if (res.out.error) {
    throw res.out.error;
  }
  const delegations = Object.values(res.out.ok.delegations).flatMap((bytes3) => bytesToDelegations(bytes3));
  if (addProofs) {
    for (const d of delegations) {
      await access2.addProof(d);
    }
    await addSpacesFromDelegations(access2, delegations);
  }
  return delegations;
}
async function addProvider({ access: access2, space: space2, account, provider }) {
  const result = await access2.invokeAndExecute(provider_exports.add, {
    audience: access2.connection.id,
    with: account.did(),
    nb: {
      provider,
      consumer: space2
    }
  });
  if (result.out.error) {
    throw result.out.error;
  }
}
function delegationsIncludeSessionProof(delegations) {
  return delegations.some((d) => isSessionProof(d));
}
async function pollAccessClaimUntil(delegationsMatch, access2, delegee, opts) {
  var _a15;
  const interval = (opts == null ? void 0 : opts.interval) || 250;
  while (true) {
    if ((_a15 = opts == null ? void 0 : opts.signal) == null ? void 0 : _a15.aborted)
      throw opts.signal.reason ?? new Error("operation aborted");
    const res = await access2.invokeAndExecute(access_exports.claim, {
      with: delegee
    });
    if (res.out.error)
      throw res.out.error;
    const claims = Object.values(res.out.ok.delegations).flatMap((d) => bytesToDelegations(d));
    if (delegationsMatch(claims))
      return claims;
    await new Promise((resolve) => setTimeout(resolve, interval));
  }
}
async function waitForAuthorizationByPolling(access2, opts = {}) {
  const claimed = await pollAccessClaimUntil(delegationsIncludeSessionProof, access2, access2.issuer.did(), {
    signal: opts == null ? void 0 : opts.signal,
    interval: opts == null ? void 0 : opts.interval
  });
  return [...claimed];
}
async function authorizeAndWait(access2, email2, opts = {}) {
  const expectAuthorization = opts.expectAuthorization || waitForAuthorizationByPolling;
  const account = { did: () => fromEmail(email2) };
  await requestAccess(access2, account, (opts == null ? void 0 : opts.capabilities) || [
    { can: "space/*" },
    { can: "store/*" },
    { can: "provider/add" },
    { can: "subscription/list" },
    { can: "upload/*" },
    { can: "ucan/*" },
    { can: "plan/*" },
    { can: "usage/*" },
    { can: "w3up/*" }
  ]);
  const sessionDelegations = [...await expectAuthorization(access2, opts)];
  if (!(opts == null ? void 0 : opts.dontAddProofs)) {
    await Promise.all(sessionDelegations.map(async (d) => access2.addProof(d)));
  }
}
async function authorizeWaitAndClaim(accessAgent, email2, opts) {
  await authorizeAndWait(accessAgent, email2, opts);
  await claimAccess(accessAgent, accessAgent.issuer.did(), {
    addProofs: (opts == null ? void 0 : opts.addProofs) ?? true
  });
}
async function addProviderAndDelegateToAccount(access2, agentData, email2, opts) {
  const space2 = (opts == null ? void 0 : opts.space) || access2.currentSpace();
  const spaceMeta = space2 ? agentData.spaces.get(space2) : void 0;
  const provider = (opts == null ? void 0 : opts.provider) || (() => {
    const service = access2.connection.id.did();
    if (DIDWeb.is(service)) {
      return service;
    }
    throw new Error(`unable to determine provider to use to addProviderAndDelegateToAccount using access.connection.id did ${service}. expected a did:web:`);
  })();
  if (!space2 || !spaceMeta) {
    throw new Error("No space selected");
  }
  if (spaceMeta) {
    throw new Error("Space already registered with storacha.network.");
  }
  const account = { did: () => fromEmail(email(email2)) };
  await addProvider({ access: access2, space: space2, account, provider });
  const delegateSpaceAccessResult = await delegateSpaceAccessToAccount(access2, space2, account);
  if (delegateSpaceAccessResult.out.error) {
    throw delegateSpaceAccessResult.out.error;
  }
  await agentData.addSpace(space2, spaceMeta);
}
async function delegateSpaceAccessToAccount(access2, space2, account) {
  const issuerSaysAccountCanAdminSpace = await createIssuerSaysAccountCanAdminSpace(
    access2.issuer,
    space2,
    account,
    void 0,
    access2.proofs([{ with: space2, can: "*" }]),
    // we want to sign over control of this space forever
    Infinity
  );
  return access2.invokeAndExecute(delegate3, {
    audience: access2.connection.id,
    with: space2,
    expiration: Infinity,
    nb: {
      delegations: {
        [issuerSaysAccountCanAdminSpace.cid.toString()]: issuerSaysAccountCanAdminSpace.cid
      }
    },
    proofs: [
      // must be embedded here because it's referenced by cid in .nb.delegations
      issuerSaysAccountCanAdminSpace
    ]
  });
}
async function createIssuerSaysAccountCanAdminSpace(issuer, space2, account, capabilities = [
  {
    can: "*",
    with: space2
  }
], proofs2 = [], expiration) {
  return delegate({
    issuer,
    audience: account,
    capabilities,
    proofs: proofs2,
    expiration
  });
}
async function getAccountPlan(agent, account) {
  const receipt = await agent.invokeAndExecute(plan_exports.get, {
    with: account
  });
  return receipt.out;
}

// node_modules/@storacha/access/dist/agent.js
var HOST = "https://up.storacha.network";
var PRINCIPAL = did_exports.parse("did:web:up.storacha.network");
var IDENTITIES = [PRINCIPAL.did(), "did:web:web3.storage"];
var agentToData = /* @__PURE__ */ new WeakMap();
function connection(options = {}) {
  return connect({
    id: options.principal ?? PRINCIPAL,
    codec: outbound2,
    channel: options.channel ?? open({
      url: options.url ?? new URL(HOST),
      method: "POST",
      fetch: options.fetch ?? globalThis.fetch.bind(globalThis)
    })
  });
}
var _data, _serviceIdentities, _Agent_instances, delegations_fn;
var _Agent = class _Agent {
  /**
   * @param {import('./agent-data.js').AgentData} data - Agent data
   * @param {import('./types.js').AgentOptions<S>} [options]
   */
  constructor(data, options = {}) {
    __privateAdd(this, _Agent_instances);
    /** @type {import('./agent-data.js').AgentData} */
    __privateAdd(this, _data);
    /**
     * DIDs the service is known by, including the primary. They are used by
     * `invoke` and `invokeAndExecute` to scope session proofs to just the service
     * DID or it's aliases.
     *
     * @type {Set<API.DID>}
     */
    __privateAdd(this, _serviceIdentities);
    var _a15;
    const channel = (_a15 = options.connection) == null ? void 0 : _a15.channel;
    this.url = options.url ?? (channel == null ? void 0 : channel.url) ?? new URL(HOST);
    this.connection = options.connection ?? connection({
      principal: options.servicePrincipal,
      url: this.url
    });
    __privateSet(this, _data, data);
    agentToData.set(this, __privateGet(this, _data));
    const primary = this.connection.id;
    const serviceIdentities = new Set(options.serviceIdentities);
    if (IDENTITIES.includes(primary.did())) {
      for (const id of IDENTITIES) {
        serviceIdentities.add(id);
      }
    }
    if (!serviceIdentities.has(primary.did())) {
      serviceIdentities.add(primary.did());
    }
    __privateSet(this, _serviceIdentities, serviceIdentities);
  }
  /**
   * Create a new Agent instance, optionally with the passed initialization data.
   *
   * @template {Record<string, any>} [R=Service]
   * @param {Partial<import('./types.js').AgentDataModel>} [init]
   * @param {import('./types.js').AgentOptions<R> & import('./types.js').AgentDataOptions} [options]
   */
  static async create(init2, options = {}) {
    const data = await AgentData.create(init2, options);
    return new _Agent(data, options);
  }
  /**
   * Instantiate an Agent from pre-exported agent data.
   *
   * @template {Record<string, any>} [R=Service]
   * @param {import('./types.js').AgentDataExport} raw
   * @param {import('./types.js').AgentOptions<R> & import('./types.js').AgentDataOptions} [options]
   */
  static from(raw, options = {}) {
    const data = AgentData.fromExport(raw, options);
    return new _Agent(data, options);
  }
  get issuer() {
    return __privateGet(this, _data).principal;
  }
  get meta() {
    return __privateGet(this, _data).meta;
  }
  get spaces() {
    return __privateGet(this, _data).spaces;
  }
  did() {
    return __privateGet(this, _data).principal.did();
  }
  /**
   * Add a proof to the agent store.
   *
   * @param {API.Delegation} delegation
   */
  async addProof(delegation) {
    return await this.addProofs([delegation]);
  }
  /**
   * Adds set of proofs to the agent store.
   *
   * @param {Iterable<API.Delegation>} delegations
   */
  async addProofs(delegations) {
    for (const proof of delegations) {
      await __privateGet(this, _data).addDelegation(proof, { audience: this.meta });
    }
    await this.removeExpiredDelegations();
    return {};
  }
  /**
   * Clean up any expired delegations.
   */
  async removeExpiredDelegations() {
    for (const [, value] of __privateGet(this, _data).delegations) {
      if (isExpired2(value.delegation)) {
        await __privateGet(this, _data).removeDelegation(value.delegation.cid);
      }
    }
  }
  /**
   * Revoke a delegation by CID.
   *
   * If the delegation was issued by this agent (and therefore is stored in the
   * delegation store) you can just pass the CID. If not, or if the current agent's
   * delegation store no longer contains the delegation, you MUST pass a chain of
   * proofs that proves your authority to revoke this delegation as `options.proofs`.
   *
   * @param {API.UCANLink} delegationCID
   * @param {object} [options]
   * @param {API.Delegation[]} [options.proofs]
   */
  async revoke(delegationCID, options = {}) {
    const additionalProofs = options.proofs ?? [];
    const delegation = [...this.delegations(), ...additionalProofs].find((delegation2) => delegation2.cid.equals(delegationCID));
    if (!delegation) {
      return {
        error: new Error(`could not find delegation ${delegationCID.toString()} - please include the delegation in options.proofs`)
      };
    }
    const receipt = await this.invokeAndExecute(ucan_exports2.revoke, {
      // per https://github.com/storacha/upload-service/blob/main/packages/capabilities/src/ucan.js#L38C6-L38C6 the resource here should be
      // the current issuer - using the space DID here works for simple cases but falls apart when a delegee tries to revoke a delegation
      // they have re-delegated, since they don't have "ucan/revoke" capabilities on the space
      with: this.issuer.did(),
      nb: {
        ucan: delegation.cid
      },
      proofs: [delegation, ...additionalProofs]
    });
    return receipt.out;
  }
  /**
   * Get all the proofs matching the capabilities.
   *
   * Proofs are delegations with an audience matching agent DID, or with an
   * audience matching the session DID.
   *
   * Proof of session will also be included in the returned proofs if any
   * proofs matching the passed capabilities require it.
   *
   * @param {API.CapabilityQuery[]} [caps] - Capabilities to filter by. Empty or undefined caps with return all the proofs.
   * @param {object} [options]
   * @param {API.DID|API.DID[]} [options.sessionProofIssuer] - only include session proofs for these issuer(s)
   */
  proofs(caps, options) {
    const authorizations = /* @__PURE__ */ new Map();
    for (const { delegation } of __privateMethod(this, _Agent_instances, delegations_fn).call(this, caps)) {
      if (delegation.audience.did() === this.issuer.did()) {
        authorizations.set(delegation.cid.toString(), delegation);
      }
    }
    const sessionProofIssuers = /* @__PURE__ */ new Set();
    if (options == null ? void 0 : options.sessionProofIssuer) {
      const ids = Array.isArray(options.sessionProofIssuer) ? options.sessionProofIssuer : [options.sessionProofIssuer];
      for (const id of ids) {
        sessionProofIssuers.add(id);
      }
    }
    const sessions = getSessionProofs(__privateGet(this, _data));
    for (const proof of [...authorizations.values()]) {
      const proofsByIssuer = sessions[proof.asCID.toString()] ?? {};
      const sessionProofs = [];
      if (sessionProofIssuers.size > 0) {
        for (const id of sessionProofIssuers) {
          const proofs2 = proofsByIssuer[id];
          if (proofs2)
            sessionProofs.push(...proofs2);
        }
      } else {
        sessionProofs.push(...Object.values(proofsByIssuer).flat());
      }
      for (const sessionProof of sessionProofs) {
        authorizations.set(sessionProof.cid.toString(), sessionProof);
      }
    }
    return [...authorizations.values()];
  }
  /**
   * Get delegations created by the agent for others.
   *
   * @param {API.CapabilityQuery[]} [caps] - Capabilities to filter by. Empty or undefined caps with return all the delegations.
   */
  delegations(caps) {
    const arr = [];
    for (const { delegation } of this.delegationsWithMeta(caps)) {
      arr.push(delegation);
    }
    return arr;
  }
  /**
   * Get delegations created by the agent for others and their metadata.
   *
   * @param {API.CapabilityQuery[]} [caps] - Capabilities to filter by. Empty or undefined caps with return all the delegations.
   */
  delegationsWithMeta(caps) {
    const arr = [];
    for (const value of __privateMethod(this, _Agent_instances, delegations_fn).call(this, caps)) {
      const { delegation } = value;
      const isSession = delegation.capabilities.some((c) => c.can === attest.can);
      if (!isSession && delegation.audience.did() !== this.issuer.did()) {
        arr.push(value);
      }
    }
    return arr;
  }
  /**
   * Creates a space signer and a delegation to the agent
   *
   * @param {string} name
   * @param {object} [options]
   * @param {API.SpaceAccessType} [options.access] - The access type for the space. Defaults to { type: 'public' }.
   */
  async createSpace(name15, { access: access2 } = {}) {
    return await generate2({ name: name15, access: access2, agent: this });
  }
  /**
   * @param {string} secret
   * @param {object} options
   * @param {string} options.name - The name of the space.
   * @param {API.SpaceAccessType} [options.access] - The access type for the space. Defaults to { type: 'public' }.
   */
  async recoverSpace(secret, { name: name15, access: access2 }) {
    return await fromMnemonic(secret, { name: name15, access: access2, agent: this });
  }
  /**
   * Import a space from a delegation.
   *
   * @param {API.Delegation} delegation
   * @param {object} options
   * @param {string} [options.name]
   */
  async importSpaceFromDelegation(delegation, { name: name15 = "" } = {}) {
    const space2 = name15 === "" ? fromDelegation(delegation) : fromDelegation(delegation).withName(name15);
    __privateGet(this, _data).spaces.set(space2.did(), {
      ...space2.meta,
      name: space2.name,
      access: space2.access
    });
    await this.addProof(space2.delegation);
    if (!this.currentSpace()) {
      await this.setCurrentSpace(space2.did());
    }
    return space2;
  }
  /**
   * Sets the current selected space
   *
   * Other methods will default to use the current space if no resource is defined
   *
   * @param {API.SpaceDID} space
   */
  async setCurrentSpace(space2) {
    if (!__privateGet(this, _data).spaces.has(space2)) {
      throw new Error(`Agent has no proofs for ${space2}.`);
    }
    await __privateGet(this, _data).setCurrentSpace(space2);
    return space2;
  }
  /**
   * Get current space DID
   */
  currentSpace() {
    return __privateGet(this, _data).currentSpace;
  }
  /**
   * Get current space DID, proofs and abilities
   */
  currentSpaceWithMeta() {
    if (!__privateGet(this, _data).currentSpace) {
      return;
    }
    const proofs2 = this.proofs([
      {
        can: "space/info",
        with: __privateGet(this, _data).currentSpace
      }
    ]);
    const caps = /* @__PURE__ */ new Set();
    for (const p of proofs2) {
      for (const cap of p.capabilities) {
        caps.add(cap.can);
      }
    }
    return {
      did: __privateGet(this, _data).currentSpace,
      proofs: proofs2,
      capabilities: [...caps],
      meta: __privateGet(this, _data).spaces.get(__privateGet(this, _data).currentSpace)
    };
  }
  /**
   *
   * @param {import('./types.js').DelegationOptions} options
   */
  async delegate(options) {
    const space2 = this.currentSpaceWithMeta();
    if (!space2) {
      throw new Error("no space selected.");
    }
    const caps = (
      /** @type {API.Capabilities} */
      options.abilities.map((a) => {
        return {
          with: space2.did,
          can: a
        };
      })
    );
    for (const cap of caps) {
      if (!this.proofs([cap]).length) {
        throw new Error(`cannot delegate capability ${cap.can} with ${cap.with}`);
      }
    }
    const delegation = await delegate({
      issuer: this.issuer,
      capabilities: caps,
      proofs: this.proofs(caps),
      facts: [{ space: space2.meta ?? {} }],
      ...options
    });
    await __privateGet(this, _data).addDelegation(delegation, {
      audience: options.audienceMeta
    });
    await this.removeExpiredDelegations();
    return delegation;
  }
  /**
   * Invoke and execute the given capability on the Access service connection
   *
   * ```js
   *
   * await agent.invokeAndExecute(Space.recover, {
   *   nb: {
   *     identity: 'mailto: email@gmail.com',
   *   },
   * })
   *
   * // sugar for
   * const recoverInvocation = await agent.invoke(Space.recover, {
   *   nb: {
   *     identity: 'mailto: email@gmail.com',
   *   },
   * })
   *
   * await recoverInvocation.execute(agent.connection)
   * ```
   *
   * @template {API.Ability} A
   * @template {API.URI} R
   * @template {API.Caveats} C
   * @param {API.TheCapabilityParser<API.CapabilityMatch<A, R, C>>} cap
   * @param {API.InvokeOptions<A, R, API.TheCapabilityParser<API.CapabilityMatch<A, R, C>>>} options
   * @returns {Promise<API.InferReceipt<API.Capability<A, R, C>, S>>}
   */
  async invokeAndExecute(cap, options) {
    const inv = await this.invoke(cap, options);
    const out = inv.execute(
      /** @type {*} */
      this.connection
    );
    return (
      /** @type {*} */
      out
    );
  }
  /**
   * Execute invocations on the agent's connection
   *
   * @example
   * ```js
   * const i1 = await agent.invoke(Space.info, {})
   * const i2 = await agent.invoke(Space.recover, {
   *   nb: {
   *     identity: 'mailto:hello@storacha.network',
   *   },
   * })
   *
   * const results = await agent.execute2(i1, i2)
   *
   * ```
   * @template {API.Capability} C
   * @template {API.Tuple<API.ServiceInvocation<C, S>>} I
   * @param {I} invocations
   */
  execute(...invocations) {
    return this.connection.execute(...invocations);
  }
  /**
   * Creates an invocation for the given capability with Agent's proofs, service, issuer and space.
   *
   * @example
   * ```js
   * const recoverInvocation = await agent.invoke(Space.recover, {
   *   nb: {
   *     identity: 'mailto: email@gmail.com',
   *   },
   * })
   *
   * await recoverInvocation.execute(agent.connection)
   * // or
   * await agent.execute(recoverInvocation)
   * ```
   *
   * @template {API.Ability} A
   * @template {API.URI} R
   * @template {API.TheCapabilityParser<API.CapabilityMatch<A, R, C>>} CAP
   * @template {API.Caveats} [C={}]
   * @param {CAP} cap
   * @param {import('./types.js').InvokeOptions<A, R, CAP>} options
   */
  async invoke(cap, options) {
    const audience = options.audience || this.connection.id;
    const space2 = options.with || this.currentSpace();
    if (!space2) {
      throw new Error("No space or resource selected, you need pass a resource.");
    }
    const proofs2 = [
      ...options.proofs || [],
      ...this.proofs([
        {
          with: space2,
          can: cap.can
        }
      ], {
        sessionProofIssuer: __privateGet(this, _serviceIdentities).has(audience.did()) ? [...__privateGet(this, _serviceIdentities)] : audience.did()
      })
    ];
    if (proofs2.length === 0 && options.with !== this.did()) {
      throw new Error(`no proofs available for resource ${space2} and ability ${cap.can}`);
    }
    const inv = invoke({
      ...options,
      audience,
      // @ts-ignore
      capability: cap.create({
        with: space2,
        nb: "nb" in options ? options.nb : void 0
      }),
      issuer: this.issuer,
      proofs: [...proofs2],
      nonce: options.nonce
    });
    return (
      /** @type {API.IssuedInvocationView<API.InferInvokedCapability<CAP>>} */
      inv
    );
  }
  /**
   * Get Space information from Access service
   *
   * @param {API.URI<"did:">} [space]
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async getSpaceInfo(space2, options) {
    const _space = space2 || this.currentSpace();
    if (!_space) {
      throw new Error("No space selected, you need pass a resource.");
    }
    const inv = await this.invokeAndExecute(info, {
      ...options,
      with: _space
    });
    if (inv.out.error) {
      throw inv.out.error;
    }
    return (
      /** @type {import('./types.js').SpaceInfoResult} */
      inv.out.ok
    );
  }
};
_data = new WeakMap();
_serviceIdentities = new WeakMap();
_Agent_instances = new WeakSet();
/**
 * Query the delegations store for all the delegations matching the capabilities provided.
 *
 * @param {API.CapabilityQuery[]} [caps]
 */
delegations_fn = function(caps) {
  const _caps = new Set(caps);
  const values2 = [];
  for (const [, value] of __privateGet(this, _data).delegations) {
    if (!isExpired2(value.delegation) && // check if delegation can be used
    !isTooEarly2(value.delegation)) {
      if (Array.isArray(caps) && caps.length > 0) {
        for (const cap of _caps) {
          if (canDelegateCapability(value.delegation, cap)) {
            values2.push(value);
          }
        }
      } else {
        values2.push(value);
      }
    }
  }
  return values2;
};
var Agent = _Agent;
async function addSpacesFromDelegations(agent, delegations) {
  const data = agentToData.get(agent);
  if (!data) {
    throw Object.assign(new Error(`cannot determine AgentData for Agent`), {
      agent
    });
  }
  const spaces = /* @__PURE__ */ new Map();
  const ours = delegations.filter((x) => x.audience.did() === agent.did());
  const ucanStars = ours.filter((x) => x.capabilities[0].can === "*" && x.capabilities[0].with === "ucan:*");
  for (const delegation of ucanStars) {
    for (const proof of delegation.proofs) {
      if (!isDelegation(proof) || !proof.capabilities[0].with.startsWith("did:key")) {
        continue;
      }
      const space2 = fromDelegation(proof);
      spaces.set(space2.did(), space2.meta);
    }
  }
  for (const delegation of ours) {
    const allows2 = delegation_exports.allows(delegation);
    for (const [resource, value] of Object.entries(allows2)) {
      if (resource.startsWith("did:key") && Object.keys(value).length > 0) {
        if (!spaces.has(resource)) {
          spaces.set(resource, {});
        }
      }
    }
  }
  for (const [did2, meta] of spaces) {
    await data.addSpace(did2, meta);
  }
}
var importAuthorization = async (agent, { proofs: proofs2 }) => {
  try {
    await agent.addProofs(proofs2);
    await addSpacesFromDelegations(agent, proofs2);
    return { ok: {} };
  } catch (error4) {
    return (
      /** @type {{error:Error}} */
      { error: error4 }
    );
  }
};

// node_modules/p-defer/index.js
function pDefer() {
  const deferred = {};
  deferred.promise = new Promise((resolve, reject) => {
    deferred.resolve = resolve;
    deferred.reject = reject;
  });
  return deferred;
}

// node_modules/@storacha/access/dist/drivers/indexeddb.js
var STORE_NAME = "AccessStore";
var DATA_ID = 1;
var _dbName, _dbVersion, _dbStoreName, _db, _autoOpen, _IndexedDBDriver_instances, getOpenDB_fn;
var IndexedDBDriver = class {
  /**
   * @param {string} dbName
   * @param {object} [options]
   * @param {number} [options.dbVersion]
   * @param {string} [options.dbStoreName]
   * @param {boolean} [options.autoOpen]
   */
  constructor(dbName, options = {}) {
    __privateAdd(this, _IndexedDBDriver_instances);
    /** @type {string} */
    __privateAdd(this, _dbName);
    /** @type {number|undefined} */
    __privateAdd(this, _dbVersion);
    /** @type {string} */
    __privateAdd(this, _dbStoreName);
    /** @type {IDBDatabase|undefined} */
    __privateAdd(this, _db);
    /** @type {boolean} */
    __privateAdd(this, _autoOpen);
    __privateSet(this, _dbName, dbName);
    __privateSet(this, _dbVersion, options.dbVersion);
    __privateSet(this, _dbStoreName, options.dbStoreName ?? STORE_NAME);
    __privateSet(this, _autoOpen, options.autoOpen ?? true);
  }
  async open() {
    const db = __privateGet(this, _db);
    if (db)
      return;
    const { resolve, reject, promise } = pDefer();
    const openReq = indexedDB.open(__privateGet(this, _dbName), __privateGet(this, _dbVersion));
    openReq.addEventListener("upgradeneeded", () => {
      const db2 = openReq.result;
      db2.createObjectStore(__privateGet(this, _dbStoreName), { keyPath: "id" });
    });
    openReq.addEventListener("success", () => {
      __privateSet(this, _db, openReq.result);
      resolve();
    });
    openReq.addEventListener("error", () => reject(openReq.error));
    return promise;
  }
  async close() {
    const db = __privateGet(this, _db);
    if (!db)
      throw new Error("Store is not open");
    db.close();
    __privateSet(this, _db, void 0);
  }
  /** @param {T} data */
  async save(data) {
    const db = await __privateMethod(this, _IndexedDBDriver_instances, getOpenDB_fn).call(this);
    const putData = withObjectStore(db, "readwrite", __privateGet(this, _dbStoreName), async (store3) => {
      const { resolve, reject, promise } = pDefer();
      const putReq = store3.put({ id: DATA_ID, ...data });
      putReq.addEventListener("success", () => resolve());
      putReq.addEventListener("error", () => reject(new Error("failed to query DB", { cause: putReq.error })));
      return promise;
    });
    return await putData();
  }
  async load() {
    const db = await __privateMethod(this, _IndexedDBDriver_instances, getOpenDB_fn).call(this);
    const getData = withObjectStore(db, "readonly", __privateGet(this, _dbStoreName), async (store3) => {
      const { resolve, reject, promise } = pDefer();
      const getReq = store3.get(DATA_ID);
      getReq.addEventListener("success", () => resolve(getReq.result));
      getReq.addEventListener("error", () => reject(new Error("failed to query DB", { cause: getReq.error })));
      return promise;
    });
    return await getData();
  }
  async reset() {
    const db = await __privateMethod(this, _IndexedDBDriver_instances, getOpenDB_fn).call(this);
    const clear = withObjectStore(db, "readwrite", __privateGet(this, _dbStoreName), (s) => {
      const { resolve, reject, promise } = pDefer();
      const req = s.clear();
      req.addEventListener("success", () => {
        resolve();
      });
      req.addEventListener("error", () => reject(new Error("failed to query DB", { cause: req.error })));
      return promise;
    });
    await clear();
  }
};
_dbName = new WeakMap();
_dbVersion = new WeakMap();
_dbStoreName = new WeakMap();
_db = new WeakMap();
_autoOpen = new WeakMap();
_IndexedDBDriver_instances = new WeakSet();
getOpenDB_fn = async function() {
  if (!__privateGet(this, _db)) {
    if (!__privateGet(this, _autoOpen))
      throw new Error("Store is not open");
    await this.open();
  }
  return __privateGet(this, _db);
};
function withObjectStore(db, txnMode, storeName, fn) {
  return async () => {
    const tx = db.transaction(storeName, txnMode);
    const { resolve, reject, promise } = pDefer();
    let result;
    tx.addEventListener("complete", () => resolve(result));
    tx.addEventListener("abort", () => reject(tx.error || new Error("transaction aborted")));
    tx.addEventListener("error", () => reject(new Error("transaction error", { cause: tx.error })));
    try {
      result = await fn(tx.objectStore(storeName));
      tx.commit();
    } catch (error4) {
      reject(error4);
      tx.abort();
    }
    return promise;
  };
}

// node_modules/@storacha/access/dist/stores/store-indexeddb.js
var StoreIndexedDB = class extends IndexedDBDriver {
};

// node_modules/@web3-storage/data-segment/src/multihash.js
var multihash_exports = {};
__export(multihash_exports, {
  Digest: () => digest_exports2,
  MAX_HEIGHT: () => MAX_HEIGHT2,
  MAX_PAYLOAD_SIZE: () => MAX_PAYLOAD_SIZE2,
  code: () => code15,
  create: () => create8,
  digest: () => digest3,
  name: () => name12
});

// node_modules/@web3-storage/data-segment/src/constant.js
var BITS_PER_BYTE = 8;
var FRS_PER_QUAD = 4;
var LEAFS_PER_QUAD = (
  /** @type {4n} */
  BigInt(FRS_PER_QUAD)
);
var IN_BITS_FR = 254;
var OUT_BITS_FR = 256;
var IN_BYTES_PER_QUAD = (
  /** @type {127} */
  FRS_PER_QUAD * IN_BITS_FR / BITS_PER_BYTE
);
var OUT_BYTES_PER_QUAD = (
  /** @type {128} */
  FRS_PER_QUAD * OUT_BITS_FR / BITS_PER_BYTE
);
var PADDED_BYTES_PER_QUAD = (
  /** @type {127n} */
  BigInt(IN_BYTES_PER_QUAD)
);
var EXPANDED_BYTES_PER_QUAD = (
  /** @type {128n} */
  BigInt(OUT_BYTES_PER_QUAD)
);
var BYTES_PER_FR = (
  /** @type {32} */
  OUT_BYTES_PER_QUAD / FRS_PER_QUAD
);
var FR_RATIO = IN_BITS_FR / OUT_BITS_FR;
var NODE_SIZE = (
  /** @type {32} */
  OUT_BYTES_PER_QUAD / FRS_PER_QUAD
);
var EXPANDED_BYTES_PER_NODE = (
  /** @type {32n} */
  BigInt(NODE_SIZE)
);
var MIN_PAYLOAD_SIZE = 2 * NODE_SIZE + 1;

// node_modules/@web3-storage/data-segment/src/node.js
var from11 = (bytes3) => {
  if (bytes3 instanceof Uint8Array) {
    if (bytes3.length > NODE_SIZE) {
      return bytes3.subarray(0, NODE_SIZE);
    } else if (bytes3.length == NODE_SIZE) {
      return bytes3;
    }
  }
  const node = new Uint8Array(NODE_SIZE);
  node.set([...bytes3]);
  return node;
};
var empty2 = () => EMPTY;
var EMPTY = from11(new Uint8Array(NODE_SIZE).fill(0));
Object.freeze(EMPTY.buffer);

// node_modules/@web3-storage/data-segment/src/ipld/sha256.js
var sha256_exports = {};
__export(sha256_exports, {
  code: () => code12,
  digest: () => digest2,
  name: () => name9,
  size: () => size2
});

// node_modules/sync-multihash-sha2/src/sha256/digest.js
var name9 = "sha2-256";
var code11 = 18;
var size2 = 32;
var prefix = new Uint8Array([18, 32]);
var Digest2 = class {
  /**
   * @param {Uint8Array} bytes
   */
  constructor(bytes3) {
    this.code = code11;
    this.name = name9;
    this.bytes = bytes3;
    this.size = size2;
    this.digest = bytes3.subarray(2);
  }
};

// node_modules/sync-multihash-sha2/src/sha256/web.js
var digest2 = (payload) => {
  const digest4 = new Uint8Array(prefix.length + size2);
  digest4.set(prefix, 0);
  digest4.set(sha256(payload), prefix.length);
  return new Digest2(digest4);
};

// node_modules/@web3-storage/data-segment/src/ipld/sha256.js
var code12 = code11;

// node_modules/@web3-storage/data-segment/src/ipld/cbor.js
var cbor_exports3 = {};
__export(cbor_exports3, {
  code: () => code13,
  decode: () => decode29,
  encode: () => encode25,
  name: () => name10
});
var code13 = code;
var name10 = name;
var encode25 = encode4;
var decode29 = decode6;

// node_modules/@web3-storage/data-segment/src/proof.js
function truncatedHash(payload, options = {}) {
  const hasher = options.hasher || sha256_exports;
  const { digest: digest4 } = hasher.digest(payload);
  return truncate(digest4);
}
var computeNode = (left, right, options) => {
  const payload = new Uint8Array(left.length + right.length);
  payload.set(left, 0);
  payload.set(right, left.length);
  return truncatedHash(payload, options);
};
function truncate(node) {
  node[NODE_SIZE - 1] &= 63;
  return node;
}

// node_modules/@web3-storage/data-segment/src/zero-comm.js
var MAX_LEVEL = 64;
var ZeroComm = class {
  constructor() {
    this.bytes = new Uint8Array(MAX_LEVEL * NODE_SIZE);
    this.bytes.set(empty2(), 0);
    this.node = empty2();
    this.length = NODE_SIZE;
  }
  /**
   * @param {number} start
   * @param {number} end
   */
  slice(start, end) {
    while (this.length < end) {
      this.node = computeNode(this.node, this.node);
      this.bytes.set(this.node, this.length);
      this.length += NODE_SIZE;
    }
    return this.bytes.subarray(start, end);
  }
};
var ZERO_COMM = new ZeroComm();
var fromLevel = (level) => {
  if (level < 0 || level >= MAX_LEVEL) {
    throw new Error(
      `Only levels between 0 and ${MAX_LEVEL - 1} inclusive are available`
    );
  }
  return ZERO_COMM.slice(NODE_SIZE * level, NODE_SIZE * (level + 1));
};

// node_modules/@web3-storage/data-segment/src/piece/tree.js
var MAX_LEAF_COUNT = 2 ** 32 - 1;
var split = (source) => {
  const count = source.length / NODE_SIZE;
  const chunks = new Array(count);
  for (let n = 0; n < count; n++) {
    const offset2 = n * NODE_SIZE;
    const chunk = source.subarray(offset2, offset2 + NODE_SIZE);
    chunks[n] = chunk;
  }
  return chunks;
};

// node_modules/@web3-storage/data-segment/src/fr32.js
function toZeroPaddedSize(payloadSize) {
  const size5 = Math.max(payloadSize, MIN_PAYLOAD_SIZE);
  const highestBit = Math.floor(Math.log2(size5));
  const bound = Math.ceil(FR_RATIO * 2 ** (highestBit + 1));
  return size5 <= bound ? bound : Math.ceil(FR_RATIO * 2 ** (highestBit + 2));
}
var toPieceSize = (size5) => toZeroPaddedSize(size5) / FR_RATIO;
var pad = (source, output = new Uint8Array(toPieceSize(source.length))) => {
  const size5 = toZeroPaddedSize(source.byteLength);
  const quadCount = size5 / IN_BYTES_PER_QUAD;
  for (let n = 0; n < quadCount; n++) {
    const readOffset = n * IN_BYTES_PER_QUAD;
    const writeOffset = n * OUT_BYTES_PER_QUAD;
    output.set(source.subarray(readOffset, readOffset + 32), writeOffset);
    output[writeOffset + 31] &= 63;
    for (let i = 32; i < 64; i++) {
      output[writeOffset + i] = source[readOffset + i] << 2 | source[readOffset + i - 1] >> 6;
    }
    output[writeOffset + 63] &= 63;
    for (let i = 64; i < 96; i++) {
      output[writeOffset + i] = source[readOffset + i] << 4 | source[readOffset + i - 1] >> 4;
    }
    output[writeOffset + 95] &= 63;
    for (let i = 96; i < 127; i++) {
      output[writeOffset + i] = source[readOffset + i] << 6 | source[readOffset + i - 1] >> 2;
    }
    output[writeOffset + 127] = source[readOffset + 126] >> 2;
  }
  return output;
};

// node_modules/@web3-storage/data-segment/src/piece/size/expanded.js
var expanded_exports = {};
__export(expanded_exports, {
  from: () => from13,
  fromHeight: () => fromHeight2,
  fromPadded: () => toExpanded,
  fromUnpadded: () => toExpanded2,
  fromWidth: () => fromWidth2,
  toHeight: () => toHeight3,
  toPadded: () => fromExpanded,
  toWidth: () => toWidth3,
  tryFrom: () => tryFrom2
});

// node_modules/@web3-storage/data-segment/src/uint64.js
var log2Floor = (n) => {
  let result = 0n;
  while (n >>= 1n) result++;
  return Number(result);
};
var log2Ceil = (n) => n <= 1n ? 0 : log2Floor(BigInt(n) - 1n) + 1;
var trailingZeros64 = (n) => {
  if (n === 0n) {
    return 64;
  }
  let count = 0;
  while ((n & 1n) === 0n) {
    n >>= 1n;
    count++;
  }
  return count;
};
var onesCount64 = (value) => {
  let count = 0;
  const mask2 = 1n;
  for (let i = 0n; i < 64n; i++) {
    if ((value & mask2 << i) !== 0n) {
      count++;
    }
  }
  return count;
};

// node_modules/@web3-storage/data-segment/src/piece/size/padded.js
var padded_exports = {};
__export(padded_exports, {
  from: () => from12,
  fromExpanded: () => fromExpanded,
  fromHeight: () => fromHeight,
  fromWidth: () => fromWidth,
  toExpanded: () => toExpanded,
  toHeight: () => toHeight,
  toWidth: () => toWidth,
  tryFrom: () => tryFrom
});
var from12 = (size5) => {
  const result = tryFrom(size5);
  if (result.error) {
    throw result.error;
  } else {
    return result.ok;
  }
};
var tryFrom = (input11) => {
  const size5 = BigInt(input11);
  if (size5 < PADDED_BYTES_PER_QUAD) {
    return {
      error: new RangeError(
        `Padded payload must contain at least ${PADDED_BYTES_PER_QUAD} bytes`
      )
    };
  }
  if (size5 >> BigInt(trailingZeros64(size5)) !== PADDED_BYTES_PER_QUAD) {
    return {
      error: new RangeError(
        `Padded payload size must be (2ⁿ * ${PADDED_BYTES_PER_QUAD})`
      )
    };
  }
  return { ok: size5 };
};
var fromExpanded = (size5) => fromQuads(size5 / EXPANDED_BYTES_PER_QUAD);
var toExpanded = (size5) => toQauds(size5) * EXPANDED_BYTES_PER_QUAD;
var fromHeight = (height2) => {
  const quads = 2n ** BigInt(height2 - 2);
  return quads * PADDED_BYTES_PER_QUAD;
};
var toHeight = (size5) => log2Ceil(toWidth(size5));
var toWidth = (size5) => toQauds(size5) * LEAFS_PER_QUAD;
var fromWidth = (width) => fromQuads(width / LEAFS_PER_QUAD);
var toQauds = (size5) => size5 / PADDED_BYTES_PER_QUAD;
var fromQuads = (count) => count * PADDED_BYTES_PER_QUAD;

// node_modules/@web3-storage/data-segment/src/piece/size/unpadded.js
var unpadded_exports = {};
__export(unpadded_exports, {
  fromPiece: () => fromPiece,
  toExpanded: () => toExpanded2,
  toHeight: () => toHeight2,
  toPadded: () => toPadded,
  toPadding: () => toPadding,
  toWidth: () => toWidth2
});
var fromPiece = ({ height: height2, padding: padding3 }) => fromHeight(height2) - padding3;
var toPadding = (size5) => toPadded(size5) - size5;
var toPadded = (size5) => toQauds2(size5) * PADDED_BYTES_PER_QUAD;
var toExpanded2 = (size5) => toQauds2(size5) * EXPANDED_BYTES_PER_QUAD;
var toWidth2 = (size5) => toQauds2(size5) * LEAFS_PER_QUAD;
var toHeight2 = (size5) => log2Ceil(toWidth2(size5));
var toQauds2 = (size5) => {
  const quadCount = (size5 + PADDED_BYTES_PER_QUAD - 1n) / PADDED_BYTES_PER_QUAD;
  return 2n ** BigInt(log2Ceil(quadCount));
};

// node_modules/@web3-storage/data-segment/src/piece/size/expanded.js
var tryFrom2 = (input11) => {
  const size5 = BigInt(input11);
  if (size5 < EXPANDED_BYTES_PER_QUAD) {
    return {
      error: RangeError(
        `Minimum piece size is ${EXPANDED_BYTES_PER_QUAD} bytes`
      )
    };
  }
  if (onesCount64(size5) !== 1) {
    return { error: RangeError("Piece size must be a power of 2") };
  }
  return { ok: size5 };
};
var from13 = (size5) => {
  const result = tryFrom2(size5);
  if (result.error) {
    throw result.error;
  } else {
    return result.ok;
  }
};
var fromHeight2 = (height2) => fromWidth2(2n ** BigInt(height2));
var toHeight3 = (size5) => log2Ceil(toWidth3(size5));
var fromWidth2 = (width) => width * EXPANDED_BYTES_PER_NODE;
var toWidth3 = (size5) => size5 / EXPANDED_BYTES_PER_NODE;

// node_modules/@web3-storage/data-segment/src/digest.js
var digest_exports2 = {};
__export(digest_exports2, {
  HEIGHT_SIZE: () => HEIGHT_SIZE,
  MAX_DIGEST_SIZE: () => MAX_DIGEST_SIZE,
  MAX_HEIGHT: () => MAX_HEIGHT,
  MAX_PAYLOAD_SIZE: () => MAX_PAYLOAD_SIZE,
  MAX_SIZE: () => MAX_SIZE,
  ROOT_SIZE: () => ROOT_SIZE,
  TAG_SIZE: () => TAG_SIZE2,
  code: () => code14,
  fromBytes: () => fromBytes3,
  fromPiece: () => fromPiece2,
  height: () => height,
  name: () => name11,
  padding: () => padding2,
  root: () => root,
  toBytes: () => toBytes
});
var name11 = (
  /** @type {const} */
  "fr32-sha2-256-trunc254-padded-binary-tree"
);
var code14 = 4113;
var MAX_PADDING_SIZE = 9;
var HEIGHT_SIZE = 1;
var ROOT_SIZE = sha256_exports.size;
var MAX_DIGEST_SIZE = MAX_PADDING_SIZE + HEIGHT_SIZE + sha256_exports.size;
var TAG_SIZE2 = varint_exports.encodingLength(code14);
var MAX_SIZE = TAG_SIZE2 + varint_exports.encodingLength(MAX_DIGEST_SIZE) + MAX_DIGEST_SIZE;
var MAX_HEIGHT = 255;
var MAX_PAYLOAD_SIZE = fromHeight2(MAX_HEIGHT) * BigInt(IN_BITS_FR) / BigInt(OUT_BITS_FR);
var fromPiece2 = ({ padding: padding3, height: height2, root: root2 }) => {
  const paddingLength = varint_exports.encodingLength(Number(padding3));
  const size5 = paddingLength + HEIGHT_SIZE + ROOT_SIZE;
  const sizeLength = varint_exports.encodingLength(size5);
  const multihashLength = TAG_SIZE2 + sizeLength + size5;
  let offset2 = 0;
  const bytes3 = new Uint8Array(multihashLength);
  varint_exports.encodeTo(code14, bytes3, offset2);
  offset2 += TAG_SIZE2;
  varint_exports.encodeTo(size5, bytes3, offset2);
  offset2 += sizeLength;
  varint_exports.encodeTo(Number(padding3), bytes3, offset2);
  offset2 += paddingLength;
  bytes3[offset2] = height2;
  offset2 += HEIGHT_SIZE;
  bytes3.set(root2, offset2);
  return new Digest3(bytes3);
};
var fromBytes3 = (bytes3) => new Digest3(bytes3);
var toBytes = ({ digest: digest4 }) => {
  const SIZE_BYTE_LENGTH = varint_exports.encodingLength(digest4.length);
  const prefixByteLength = SIZE_BYTE_LENGTH + TAG_SIZE2;
  if (digest4.byteOffset >= prefixByteLength) {
    const bytes4 = new Uint8Array(
      digest4.buffer,
      digest4.byteOffset - prefixByteLength,
      digest4.byteOffset + digest4.length
    );
    const [tag2, offset2] = varint_exports.decode(bytes4);
    if (tag2 === code14 && varint_exports.decode(bytes4, offset2)[0] === digest4.length) {
      return bytes4;
    }
  }
  const bytes3 = new Uint8Array(digest4.length + prefixByteLength);
  varint_exports.encodeTo(code14, bytes3);
  varint_exports.encodeTo(digest4.length, bytes3, TAG_SIZE2);
  bytes3.set(digest4, prefixByteLength);
  return bytes3;
};
var height = ({ digest: digest4 }) => {
  const [, offset2] = varint_exports.decode(digest4);
  return digest4[offset2];
};
var padding2 = ({ digest: digest4 }) => {
  const [padding3] = varint_exports.decode(digest4);
  return BigInt(padding3);
};
var root = ({ digest: digest4 }) => {
  const [, offset2] = varint_exports.decode(digest4);
  return digest4.subarray(
    offset2 + HEIGHT_SIZE,
    offset2 + HEIGHT_SIZE + sha256_exports.size
  );
};
var Digest3 = class {
  /**
   * @param {Uint8Array} bytes
   */
  constructor(bytes3) {
    this.bytes = bytes3;
    const [tag2] = varint_exports.decode(bytes3);
    if (tag2 !== code14) {
      throw new RangeError(`Expected multihash with code ${code14}`);
    }
    let offset2 = TAG_SIZE2;
    const [size5, length2] = varint_exports.decode(bytes3, offset2);
    offset2 += length2;
    const digest4 = bytes3.subarray(offset2);
    if (digest4.length !== size5) {
      throw new RangeError(
        `Invalid multihash size expected ${offset2 + size5} bytes, got ${bytes3.length} bytes`
      );
    }
    this.digest = digest4;
  }
  get name() {
    return name11;
  }
  get code() {
    return code14;
  }
  get size() {
    return this.digest.length;
  }
  get padding() {
    return padding2(this);
  }
  get height() {
    return height(this);
  }
  get root() {
    return root(this);
  }
};

// node_modules/@web3-storage/data-segment/src/multihash.js
var name12 = (
  /** @type {const} */
  "fr32-sha2-256-trunc254-padded-binary-tree"
);
var code15 = 4113;
var MAX_HEIGHT2 = 255;
var MAX_PAYLOAD_SIZE2 = fromHeight2(MAX_HEIGHT2) * BigInt(IN_BITS_FR) / BigInt(OUT_BITS_FR);
var digest3 = (payload) => {
  const hasher = new Hasher2();
  hasher.write(payload);
  return hasher.digest();
};
var create8 = () => new Hasher2();
var Hasher2 = class {
  constructor() {
    this.bytesWritten = 0n;
    this.buffer = new Uint8Array(IN_BYTES_PER_QUAD);
    this.offset = 0;
    this.layers = [[]];
  }
  /**
   * Return the total number of bytes written into the hasher. Calling
   * {@link reset} will reset the hasher and the count will be reset to 0.
   *
   * @returns {bigint}
   */
  count() {
    return this.bytesWritten;
  }
  /**
   * Computes the digest of all the data that has been written into this hasher.
   * This method does not have side-effects, meaning that you can continue
   * writing and call this method again to compute digest of all the data
   * written from the very beginning.
   */
  digest() {
    const bytes3 = new Uint8Array(MAX_SIZE);
    const count = this.digestInto(bytes3, 0, true);
    return fromBytes3(bytes3.subarray(0, count));
  }
  /**
   * Computes the digest and writes into the given buffer. You can provide
   * optional `byteOffset` to write digest at that offset in the buffer. By
   * default the multihash prefix will be written into the buffer, but you can
   * opt-out by passing `false` as the `asMultihash` argument.
   *
   * @param {Uint8Array} output
   * @param {number} [byteOffset]
   * @param {boolean} asMultihash
   */
  digestInto(output, byteOffset = 0, asMultihash = true) {
    const { buffer: buffer2, layers, offset: offset2, bytesWritten } = this;
    let [leaves, ...nodes] = layers;
    if (offset2 > 0 || bytesWritten === 0n) {
      leaves = [...leaves, ...split(pad(buffer2.fill(0, offset2)))];
    }
    const tree2 = build2([leaves, ...nodes]);
    const height2 = tree2.length - 1;
    const [root2] = tree2[height2];
    const padding3 = Number(unpadded_exports.toPadding(this.bytesWritten));
    const paddingLength = varint_exports.encodingLength(
      /** @type {number & bigint} */
      padding3
    );
    let endOffset = byteOffset;
    if (asMultihash) {
      varint_exports.encodeTo(code15, output, endOffset);
      endOffset += TAG_SIZE2;
      const size5 = paddingLength + HEIGHT_SIZE + ROOT_SIZE;
      const sizeLength = varint_exports.encodingLength(size5);
      varint_exports.encodeTo(size5, output, endOffset);
      endOffset += sizeLength;
    }
    varint_exports.encodeTo(padding3, output, endOffset);
    endOffset += paddingLength;
    output[endOffset] = height2;
    endOffset += 1;
    output.set(root2, endOffset);
    endOffset += root2.length;
    return endOffset - byteOffset;
  }
  /**
   * @param {Uint8Array} bytes
   */
  write(bytes3) {
    const { buffer: buffer2, offset: offset2, layers } = this;
    const leaves = layers[0];
    const { length: length2 } = bytes3;
    if (length2 === 0) {
      return this;
    } else if (this.bytesWritten + BigInt(length2) > MAX_PAYLOAD_SIZE2) {
      throw new RangeError(
        `Writing ${length2} bytes exceeds max payload size of ${MAX_PAYLOAD_SIZE2}`
      );
    } else if (offset2 + length2 < buffer2.length) {
      buffer2.set(bytes3, offset2);
      this.offset += length2;
      this.bytesWritten += BigInt(length2);
      return this;
    } else {
      const bytesRequired = buffer2.length - offset2;
      buffer2.set(bytes3.subarray(0, bytesRequired), offset2);
      leaves.push(...split(pad(buffer2)));
      let readOffset = bytesRequired;
      while (readOffset + IN_BYTES_PER_QUAD < length2) {
        const quad = bytes3.subarray(readOffset, readOffset + IN_BYTES_PER_QUAD);
        leaves.push(...split(pad(quad)));
        readOffset += IN_BYTES_PER_QUAD;
      }
      this.buffer.set(bytes3.subarray(readOffset), 0);
      this.offset = length2 - readOffset;
      this.bytesWritten += BigInt(length2);
      prune(this.layers);
      return this;
    }
  }
  /**
   * Resets this hasher to its initial state so it could be recycled as new
   * instance.
   */
  reset() {
    this.offset = 0;
    this.bytesWritten = 0n;
    this.layers.length = 1;
    this.layers[0].length = 0;
    return this;
  }
  /* c8 ignore next 3 */
  dispose() {
    this.reset();
  }
  get code() {
    return code15;
  }
  get name() {
    return name12;
  }
};
var prune = (layers) => flush(layers, false);
var build2 = (layers) => flush([...layers], true);
var flush = (layers, build3) => {
  let level = 0;
  while (level < layers.length) {
    let next = layers[level + 1];
    const layer = layers[level];
    if (build3 && layer.length % 2 > 0 && next) {
      layer.push(fromLevel(level));
    }
    level += 1;
    next = next ? build3 ? [...next] : next : [];
    let index3 = 0;
    while (index3 + 1 < layer.length) {
      const node = computeNode(layer[index3], layer[index3 + 1]);
      delete layer[index3];
      delete layer[index3 + 1];
      next.push(node);
      index3 += 2;
    }
    if (next.length) {
      layers[level] = next;
    }
    layer.splice(0, index3);
  }
  return layers;
};

// node_modules/@storacha/filecoin-client/dist/storefront.js
var storefront_exports2 = {};
__export(storefront_exports2, {
  connection: () => connection2,
  filecoinAccept: () => filecoinAccept2,
  filecoinInfo: () => filecoinInfo2,
  filecoinOffer: () => filecoinOffer2,
  filecoinSubmit: () => filecoinSubmit2
});

// node_modules/@ucanto/transport/src/utf8.js
var utf8_exports2 = {};
__export(utf8_exports2, {
  decode: () => decode30,
  decoder: () => decoder2,
  encode: () => encode26,
  encoder: () => encoder2
});
var encoder2 = new TextEncoder();
var decoder2 = new TextDecoder();
var encode26 = (text2) => encoder2.encode(text2);
var decode30 = (bytes3) => decoder2.decode(bytes3);

// node_modules/@ucanto/transport/src/legacy.js
var legacy_exports = {};
__export(legacy_exports, {
  contentType: () => contentType8,
  inbound: () => inbound3,
  request: () => request_exports2,
  response: () => response_exports2
});

// node_modules/@ucanto/transport/src/legacy/response.js
var response_exports2 = {};
__export(response_exports2, {
  contentType: () => contentType6,
  encode: () => encode27
});
var API32 = __toESM(require_lib(), 1);
var contentType6 = "application/cbor";
var HEADERS3 = Object.freeze({
  "content-type": contentType6
});
var encode27 = (message, options) => {
  const legacyResults = [];
  for (const receipt of message.receipts.values()) {
    const result = receipt.out;
    if (result.ok) {
      legacyResults.push(result.ok);
    } else {
      legacyResults.push({
        ...result.error,
        error: true
      });
    }
  }
  const body = encode15(legacyResults);
  return (
    /** @type {API.HTTPResponse<Message>} */
    {
      headers: HEADERS3,
      body
    }
  );
};

// node_modules/@ucanto/transport/src/legacy/request.js
var request_exports2 = {};
__export(request_exports2, {
  contentType: () => contentType7,
  decode: () => decode31
});
var API33 = __toESM(require_lib(), 1);
var contentType7 = "application/car";
var decode31 = async ({ body }) => {
  const { roots, blocks } = decode16(
    /** @type {Uint8Array} */
    body
  );
  const run = [];
  for (const { cid } of roots) {
    const invocation = invocation_exports.view({
      root: (
        /** @type {API.Link} */
        cid
      ),
      blocks
    });
    run.push(invocation);
  }
  const message = await message_exports.build({
    invocations: (
      /** @type {API.Tuple<API.IssuedInvocation>} */
      run
    )
  });
  return (
    /** @type {Message} */
    message
  );
};

// node_modules/@ucanto/transport/src/legacy.js
var { contentType: contentType8 } = request_exports2;
var inbound3 = inbound({
  decoders: {
    [contentType8]: request_exports2,
    [contentType5]: request_exports
  },
  encoders: {
    // Here we configure encoders such that if accept header is `*/*` (which is
    // the default if omitted) we will encode the response in CBOR. If
    // `application/vnd.ipld.car` is set we will encode the response in current
    // format.
    // Here we exploit the fact that legacy clients do not send an accept header
    // and therefore will get response in legacy format. New clients on the other
    // hand will send `application/vnd.ipld.car` and consequently get response
    // in current format.
    "*/*;q=0.1": response_exports2,
    [contentType5]: response_exports
  }
});

// node_modules/@storacha/filecoin-client/dist/service.js
var services = {
  STOREFRONT: {
    url: new URL("https://up.storacha.network"),
    principal: parse2("did:web:up.storacha.network")
  },
  AGGREGATOR: {
    url: new URL("https://aggregator.web3.storage"),
    principal: parse2("did:web:web3.storage")
  },
  DEALER: {
    url: new URL("https://dealer.web3.storage"),
    principal: parse2("did:web:web3.storage")
  },
  DEAL_TRACKER: {
    url: new URL("https://tracker.web3.storage"),
    principal: parse2("did:web:web3.storage")
  }
};

// node_modules/@storacha/filecoin-client/dist/storefront.js
var connection2 = connect({
  id: services.STOREFRONT.principal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: services.STOREFRONT.url,
    method: "POST"
  })
});
async function filecoinOffer2({ issuer, with: resource, proofs: proofs2, audience }, content2, piece, options = {}) {
  const conn = options.connection ?? connection2;
  const invocation = filecoinOffer.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.STOREFRONT.principal,
    with: resource,
    nb: {
      content: content2,
      piece
    },
    proofs: proofs2,
    expiration: Infinity
  });
  return await invocation.execute(conn);
}
async function filecoinSubmit2({ issuer, with: resource, proofs: proofs2, audience }, content2, piece, options = {}) {
  const conn = options.connection ?? connection2;
  const invocation = filecoinSubmit.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.STOREFRONT.principal,
    with: resource,
    nb: {
      content: content2,
      piece
    },
    proofs: proofs2,
    expiration: Infinity
  });
  return await invocation.execute(conn);
}
async function filecoinAccept2({ issuer, with: resource, proofs: proofs2, audience }, content2, piece, options = {}) {
  const conn = options.connection ?? connection2;
  const invocation = filecoinAccept.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.STOREFRONT.principal,
    with: resource,
    nb: {
      content: content2,
      piece
    },
    proofs: proofs2,
    expiration: Infinity
  });
  return await invocation.execute(conn);
}
async function filecoinInfo2({ issuer, with: resource, proofs: proofs2, audience }, piece, options = {}) {
  const conn = options.connection ?? connection2;
  const invocation = filecoinInfo.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.STOREFRONT.principal,
    with: resource,
    nb: {
      piece
    },
    proofs: proofs2
  });
  return await invocation.execute(conn);
}

// node_modules/@storacha/filecoin-client/dist/aggregator.js
var aggregator_exports2 = {};
__export(aggregator_exports2, {
  connection: () => connection3,
  pieceAccept: () => pieceAccept2,
  pieceOffer: () => pieceOffer2
});
var connection3 = connect({
  id: services.AGGREGATOR.principal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: services.AGGREGATOR.url,
    method: "POST"
  })
});
async function pieceOffer2({ issuer, with: resource, proofs: proofs2, audience }, piece, group2, options = {}) {
  const conn = options.connection ?? connection3;
  const invocation = pieceOffer.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.AGGREGATOR.principal,
    with: resource,
    nb: {
      piece,
      group: group2
    },
    proofs: proofs2,
    expiration: Infinity
  });
  return await invocation.execute(conn);
}
async function pieceAccept2({ issuer, with: resource, proofs: proofs2, audience }, piece, group2, options = {}) {
  const conn = options.connection ?? connection3;
  const invocation = pieceAccept.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.AGGREGATOR.principal,
    with: resource,
    nb: {
      piece,
      group: group2
    },
    proofs: proofs2,
    expiration: Infinity
  });
  return await invocation.execute(conn);
}

// node_modules/@storacha/filecoin-client/dist/dealer.js
var dealer_exports2 = {};
__export(dealer_exports2, {
  aggregateAccept: () => aggregateAccept2,
  aggregateOffer: () => aggregateOffer2,
  connection: () => connection4
});
var connection4 = connect({
  id: services.DEALER.principal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: services.DEALER.url,
    method: "POST"
  })
});
async function aggregateOffer2({ issuer, with: resource, proofs: proofs2, audience }, aggregate, pieces, options = {}) {
  const conn = options.connection ?? connection4;
  const block = await cbor_exports2.write(pieces);
  const invocation = aggregateOffer.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.AGGREGATOR.principal,
    with: resource,
    nb: {
      aggregate,
      pieces: block.cid
    },
    proofs: proofs2,
    expiration: Infinity
  });
  invocation.attach(block);
  return await invocation.execute(conn);
}
async function aggregateAccept2({ issuer, with: resource, proofs: proofs2, audience }, aggregate, pieces, options = {}) {
  const conn = options.connection ?? connection4;
  const invocation = aggregateAccept.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.AGGREGATOR.principal,
    with: resource,
    nb: {
      aggregate,
      pieces
    },
    proofs: proofs2,
    expiration: Infinity
  });
  return await invocation.execute(conn);
}

// node_modules/@storacha/filecoin-client/dist/deal-tracker.js
var deal_tracker_exports2 = {};
__export(deal_tracker_exports2, {
  connection: () => connection5,
  dealInfo: () => dealInfo2
});
var connection5 = connect({
  id: services.DEAL_TRACKER.principal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: services.DEAL_TRACKER.url,
    method: "POST"
  })
});
async function dealInfo2({ issuer, with: resource, proofs: proofs2, audience }, piece, options = {}) {
  const conn = options.connection ?? connection5;
  const invocation = dealInfo.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.DEAL_TRACKER.principal,
    with: resource,
    nb: {
      piece
    },
    proofs: proofs2
  });
  return await invocation.execute(conn);
}

// node_modules/@storacha/upload-client/dist/blob/index.js
var blob_exports4 = {};
__export(blob_exports4, {
  add: () => add8,
  get: () => get10,
  list: () => list6,
  remove: () => remove5,
  replicate: () => replicate2
});

// node_modules/p-retry/index.js
var import_retry = __toESM(require_retry2());
var networkErrorMsgs = /* @__PURE__ */ new Set([
  "Failed to fetch",
  // Chrome
  "NetworkError when attempting to fetch resource.",
  // Firefox
  "The Internet connection appears to be offline.",
  // Safari
  "Network request failed",
  // `cross-fetch`
  "fetch failed"
  // Undici (Node.js)
]);
var AbortError = class extends Error {
  constructor(message) {
    super();
    if (message instanceof Error) {
      this.originalError = message;
      ({ message } = message);
    } else {
      this.originalError = new Error(message);
      this.originalError.stack = this.stack;
    }
    this.name = "AbortError";
    this.message = message;
  }
};
var decorateErrorWithCounts = (error4, attemptNumber, options) => {
  const retriesLeft = options.retries - (attemptNumber - 1);
  error4.attemptNumber = attemptNumber;
  error4.retriesLeft = retriesLeft;
  return error4;
};
var isNetworkError = (errorMessage) => networkErrorMsgs.has(errorMessage);
var getDOMException = (errorMessage) => globalThis.DOMException === void 0 ? new Error(errorMessage) : new DOMException(errorMessage);
async function pRetry(input11, options) {
  return new Promise((resolve, reject) => {
    options = {
      onFailedAttempt() {
      },
      retries: 10,
      ...options
    };
    const operation = import_retry.default.operation(options);
    operation.attempt(async (attemptNumber) => {
      try {
        resolve(await input11(attemptNumber));
      } catch (error4) {
        if (!(error4 instanceof Error)) {
          reject(new TypeError(`Non-error was thrown: "${error4}". You should only throw errors.`));
          return;
        }
        if (error4 instanceof AbortError) {
          operation.stop();
          reject(error4.originalError);
        } else if (error4 instanceof TypeError && !isNetworkError(error4.message)) {
          operation.stop();
          reject(error4);
        } else {
          decorateErrorWithCounts(error4, attemptNumber, options);
          try {
            await options.onFailedAttempt(error4);
          } catch (error5) {
            reject(error5);
            return;
          }
          if (!operation.retry(error4)) {
            reject(operation.mainError());
          }
        }
      }
    });
    if (options.signal && !options.signal.aborted) {
      options.signal.addEventListener("abort", () => {
        operation.stop();
        const reason = options.signal.reason === void 0 ? getDOMException("The operation was aborted.") : options.signal.reason;
        reject(reason instanceof Error ? reason : getDOMException(reason));
      }, {
        once: true
      });
    }
  });
}

// node_modules/@storacha/upload-client/dist/service.js
var serviceURL = new URL("https://up.storacha.network");
var servicePrincipal = parse2("did:web:up.storacha.network");
var receiptsEndpoint = "https://up.storacha.network/receipt/";
var connection6 = connect({
  id: servicePrincipal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: serviceURL,
    method: "POST"
  })
});

// node_modules/@storacha/upload-client/dist/constants.js
var REQUEST_RETRIES = 3;

// node_modules/@storacha/upload-client/dist/receipts.js
var receipts_exports = {};
__export(receipts_exports, {
  ReceiptMissing: () => ReceiptMissing,
  ReceiptNotFound: () => ReceiptNotFound,
  get: () => get9,
  poll: () => poll
});
var ReceiptNotFound = class extends Error {
  /**
   * @param {API.UnknownLink} taskCid
   */
  constructor(taskCid) {
    super();
    __publicField(
      this,
      "name",
      /** @type {const} */
      "ReceiptNotFound"
    );
    this.taskCid = taskCid;
  }
  /* c8 ignore start */
  get reason() {
    return `receipt not found for task ${this.taskCid} in the indexed workflow`;
  }
};
var ReceiptMissing = class extends Error {
  /**
   * @param {API.UnknownLink} taskCid
   */
  constructor(taskCid) {
    super();
    __publicField(
      this,
      "name",
      /** @type {const} */
      "ReceiptMissing"
    );
    this.taskCid = taskCid;
  }
  /* c8 ignore start */
  get reason() {
    return `receipt missing for task ${this.taskCid}`;
  }
};
async function poll(taskCid, options) {
  return await pRetry(async () => {
    const res = await get9(taskCid, options);
    if (res.error) {
      if (res.error.name === "ReceiptNotFound") {
        throw res.error;
      } else {
        throw new AbortError(new Error(`failed to fetch receipt for task: ${taskCid}`, {
          cause: res.error
        }));
      }
    }
    return res.ok;
  }, {
    signal: options == null ? void 0 : options.signal,
    onFailedAttempt: console.warn,
    /* c8 ignore next */
    retries: (options == null ? void 0 : options.retries) ?? REQUEST_RETRIES
  });
}
function receiptEndpointFromChannel(channel) {
  if ("url" in channel && channel.url instanceof URL) {
    const url = channel.url;
    return new URL("/receipt/", url.toString());
  } else {
    return null;
  }
}
async function get9(taskCid, options) {
  var _a15, _b12;
  const channel = (_a15 = options == null ? void 0 : options.connection) == null ? void 0 : _a15.channel;
  const endpoint = (options == null ? void 0 : options.endpoint) ?? (channel && receiptEndpointFromChannel(channel)) ?? receiptsEndpoint;
  const url = new URL(taskCid.toString(), endpoint);
  const fetchReceipt = (options == null ? void 0 : options.fetch) ?? globalThis.fetch.bind(globalThis);
  const workflowResponse = await fetchReceipt(url, { signal: options == null ? void 0 : options.signal });
  if (workflowResponse.status === 404) {
    return {
      error: new ReceiptNotFound(taskCid)
    };
  }
  const agentMessageBytes = new Uint8Array(await workflowResponse.arrayBuffer());
  const agentMessage = await car_exports2.request.decode({
    body: agentMessageBytes,
    headers: {}
  });
  const receipt = (
    /** @type {API.InferReceipt<C, S>|undefined} */
    agentMessage.receipts.get(`${taskCid}`)
  );
  if (!receipt) {
    const blocks = /* @__PURE__ */ new Map();
    for (const b of agentMessage.iterateIPLDBlocks()) {
      blocks.set(b.cid.toString(), b);
    }
    const invocations = [...agentMessage.invocations];
    for (const receipt2 of agentMessage.receipts.values()) {
      if (isDelegation(receipt2.ran)) {
        invocations.push(receipt2.ran);
      }
    }
    for (const inv of invocations) {
      if (((_b12 = inv.capabilities[0]) == null ? void 0 : _b12.can) !== "ucan/conclude")
        continue;
      const root2 = Object(inv.capabilities[0].nb).receipt;
      const receipt2 = root2 ? (
        /** @type {API.InferReceipt<C, S>|null} */
        receipt_exports.view({ root: root2, blocks }, null)
      ) : null;
      if (!receipt2)
        continue;
      const ran = isDelegation(receipt2.ran) ? receipt2.ran.cid : receipt2.ran;
      if (ran.toString() === taskCid.toString()) {
        return { ok: receipt2 };
      }
    }
    return {
      error: new ReceiptMissing(taskCid)
    };
  }
  return {
    ok: receipt
  };
}

// node_modules/@storacha/upload-client/dist/runtime.js
var isCloudflareWorkers = typeof navigator !== "undefined" && (navigator == null ? void 0 : navigator.userAgent) === "Cloudflare-Workers";

// node_modules/@storacha/upload-client/dist/blob/add.js
function createUploadProgressHandler(url, handler) {
  const onUploadProgress = ({ total, loaded, lengthComputable }) => {
    return handler({ total, loaded, lengthComputable, url });
  };
  return onUploadProgress;
}
function getConcludeReceipt(concludeFx) {
  const receiptBlocks = /* @__PURE__ */ new Map();
  for (const block of concludeFx.iterateIPLDBlocks()) {
    receiptBlocks.set(`${block.cid}`, block);
  }
  return receipt_exports.view({
    // @ts-expect-error object of type unknown
    root: concludeFx.capabilities[0].nb.receipt,
    blocks: receiptBlocks
  });
}
function parseBlobAddReceiptNext(receipt) {
  const forkInvocations = receipt.fx.fork;
  const allocateTask = forkInvocations.find(
    (fork5) => fork5.capabilities[0].can === allocate2.can
    /* c8 ignore next 4 */
    // tested by legacy integration test in w3up-client
  ) ?? forkInvocations.find((fork5) => fork5.capabilities[0].can === allocate3.can);
  const concludefxs = forkInvocations.filter((fork5) => fork5.capabilities[0].can === conclude.can);
  const putTask = forkInvocations.find((fork5) => fork5.capabilities[0].can === put.can);
  const acceptTask = (
    /** @type {API.Invocation<API.BlobAccept>|undefined} */
    forkInvocations.find(
      (fork5) => fork5.capabilities[0].can === accept.can
      /* c8 ignore next 4 */
      // tested by legacy integration test in w3up-client
    ) ?? forkInvocations.find((fork5) => fork5.capabilities[0].can === accept2.can)
  );
  if (!allocateTask || !concludefxs.length || !putTask || !acceptTask) {
    throw new Error("mandatory effects not received");
  }
  const nextReceipts = concludefxs.map((fx) => getConcludeReceipt(fx));
  const allocateReceipt = nextReceipts.find((receipt2) => receipt2.ran.link().equals(allocateTask.cid));
  const putReceipt = nextReceipts.find((receipt2) => receipt2.ran.link().equals(putTask.cid));
  const acceptReceipt = nextReceipts.find((receipt2) => receipt2.ran.link().equals(acceptTask.cid));
  if (!allocateReceipt) {
    throw new Error("mandatory effects not received");
  }
  return {
    allocate: {
      task: allocateTask,
      receipt: allocateReceipt
    },
    put: {
      task: putTask,
      receipt: putReceipt
    },
    accept: {
      task: acceptTask,
      receipt: acceptReceipt
    }
  };
}
function createConcludeInvocation(id, serviceDid, receipt) {
  const receiptBlocks = [];
  const receiptCids = [];
  for (const block of receipt.iterateIPLDBlocks()) {
    receiptBlocks.push(block);
    receiptCids.push(block.cid);
  }
  const concludeAllocatefx = conclude.invoke({
    issuer: id,
    audience: serviceDid,
    with: id.toDIDKey(),
    nb: {
      receipt: receipt.link()
    },
    expiration: Infinity,
    facts: [
      {
        ...receiptCids
      }
    ]
  });
  for (const block of receiptBlocks) {
    concludeAllocatefx.attach(block);
  }
  return concludeAllocatefx;
}
async function add8({ issuer, with: resource, proofs: proofs2, audience }, digest4, data, options = {}) {
  var _a15;
  const bytes3 = data instanceof Uint8Array ? data : new Uint8Array(await data.arrayBuffer());
  const size5 = bytes3.length;
  const conn = options.connection ?? connection6;
  const result = await pRetry(async () => {
    return await add7.invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID.from(resource),
      nb: input(digest4, size5),
      proofs: proofs2,
      nonce: options.nonce
    }).execute(conn);
  }, {
    onFailedAttempt: console.warn,
    retries: options.retries ?? REQUEST_RETRIES
  });
  if (!result.out.ok) {
    throw new Error(`failed ${add7.can} invocation`, {
      cause: result.out.error
    });
  }
  const nextTasks = parseBlobAddReceiptNext(result);
  const { receipt: allocateReceipt } = nextTasks.allocate;
  if (!allocateReceipt.out.ok) {
    throw new Error(`failed ${add7.can} invocation`, {
      cause: allocateReceipt.out.error
    });
  }
  const { address } = allocateReceipt.out.ok;
  if (address) {
    const fetchWithUploadProgress = options.fetchWithUploadProgress || options.fetch || globalThis.fetch.bind(globalThis);
    let fetchDidCallUploadProgressCb = false;
    await pRetry(async () => {
      var _a16;
      try {
        const res = await fetchWithUploadProgress(address.url, {
          method: "PUT",
          ...!isCloudflareWorkers && { mode: "cors" },
          body: bytes3,
          headers: address.headers,
          signal: options.signal,
          onUploadProgress: (status) => {
            fetchDidCallUploadProgressCb = true;
            if (options.onUploadProgress)
              createUploadProgressHandler(address.url, options.onUploadProgress)(status);
          },
          // @ts-expect-error - this is needed by recent versions of node - see https://github.com/bluesky-social/atproto/pull/470 for more info
          duplex: "half"
        });
        if (res.status >= 400 && res.status < 500) {
          throw new AbortError(`upload failed: ${res.status}`);
        }
        if (!res.ok) {
          throw new Error(`upload failed: ${res.status}`);
        }
        await res.arrayBuffer();
      } catch (err) {
        if (((_a16 = options.signal) == null ? void 0 : _a16.aborted) === true) {
          throw new AbortError("upload aborted");
        }
        throw err;
      }
    }, {
      retries: options.retries ?? REQUEST_RETRIES
    });
    if (!fetchDidCallUploadProgressCb && options.onUploadProgress) {
      const blob4 = new Blob([bytes3]);
      options.onUploadProgress({
        total: blob4.size,
        loaded: blob4.size,
        lengthComputable: false
      });
    }
  }
  let { receipt: httpPutReceipt } = nextTasks.put;
  if (!(httpPutReceipt == null ? void 0 : httpPutReceipt.out.ok)) {
    const derivedSigner = ed25519_exports.from(
      /** @type {API.SignerArchive<API.DID, typeof ed25519.signatureCode>} */
      nextTasks.put.task.facts[0]["keys"]
    );
    httpPutReceipt = await receipt_exports.issue({
      issuer: derivedSigner,
      ran: nextTasks.put.task.cid,
      result: { ok: {} }
    });
    const httpPutConcludeInvocation = createConcludeInvocation(
      issuer,
      /* c8 ignore next */
      audience ?? servicePrincipal,
      httpPutReceipt
    );
    const ucanConclude = await httpPutConcludeInvocation.execute(conn);
    if (!ucanConclude.out.ok) {
      throw new Error(`failed ${conclude.can} for ${put.can} invocation`, {
        cause: ucanConclude.out.error
      });
    }
  }
  let { receipt: acceptReceipt } = nextTasks.accept;
  if (!acceptReceipt || !acceptReceipt.out.ok) {
    acceptReceipt = await poll(nextTasks.accept.task.link(), {
      ...options,
      /* c8 ignore next 3 */
      endpoint: options.receiptsEndpoint ? new URL(options.receiptsEndpoint) : void 0,
      // The connection we have is for the upload service, which does not
      // actually implement blob/accept. However, it does provide receipts for
      // blob accept invocations it has made to storage nodes. Hence we type
      // assert that this connection is a connecton to a service that
      // implements blob/accept so that we can get a typed receipt back.
      connection: (
        /** @type {API.Connection<API.BlobService>} */
        Object(conn)
      )
    });
    if (acceptReceipt.out.error) {
      throw new Error(`${accept.can} failure`, {
        cause: acceptReceipt.out.error
      });
    }
  }
  const blocks = new Map([...acceptReceipt.iterateIPLDBlocks()].map((block) => [
    `${block.cid}`,
    block
  ]));
  const site = delegation_exports.view({
    root: (
      /** @type {API.UCANLink<[import('@web3-storage/content-claims/capability/api').AssertLocation]>} */
      (_a15 = acceptReceipt.out.ok) == null ? void 0 : _a15.site
    ),
    blocks
  });
  return { site };
}
var ability = add7.can;
var input = (digest4, size5) => ({
  blob: {
    digest: digest4.bytes,
    size: size5
  }
});

// node_modules/@storacha/upload-client/dist/blob/get.js
async function get10({ issuer, with: resource, proofs: proofs2, audience }, multihash, options = {}) {
  const conn = options.connection ?? connection6;
  const result = await get8.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? servicePrincipal,
    with: SpaceDID.from(resource),
    nb: input2(multihash),
    proofs: proofs2,
    nonce: options.nonce
  }).execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${get8.can} invocation`, {
      cause: result.out.error
    });
  }
  return result.out;
}
var ability2 = get8.can;
var input2 = (digest4) => ({ digest: digest4.bytes });

// node_modules/@storacha/upload-client/dist/blob/list.js
async function list6({ issuer, with: resource, proofs: proofs2, audience }, options = {}) {
  const conn = options.connection ?? connection6;
  const result = await list5.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? servicePrincipal,
    with: SpaceDID.from(resource),
    proofs: proofs2,
    nb: input3(options.cursor, options.size),
    nonce: options.nonce
  }).execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${list5.can} invocation`, {
      cause: result.out.error
    });
  }
  return result.out.ok;
}
var ability3 = list5.can;
var input3 = (cursor, size5) => ({ cursor, size: size5 });

// node_modules/@storacha/upload-client/dist/blob/remove.js
async function remove5({ issuer, with: resource, proofs: proofs2, audience }, multihash, options = {}) {
  const conn = options.connection ?? connection6;
  const result = await remove4.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? servicePrincipal,
    with: SpaceDID.from(resource),
    nb: input4(multihash),
    proofs: proofs2,
    nonce: options.nonce
  }).execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${remove4.can} invocation`, {
      cause: result.out.error
    });
  }
  return result.out;
}
var ability4 = remove4.can;
var input4 = (digest4) => ({ digest: digest4.bytes });

// node_modules/@storacha/upload-client/dist/blob/replicate.js
async function replicate2({ issuer, with: resource, proofs: proofs2, audience }, blob4, site, replicas, options = {}) {
  const conn = options.connection ?? connection6;
  const receipt = await replicate.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? servicePrincipal,
    with: SpaceDID.from(resource),
    nb: input5(blob4, site, replicas),
    proofs: [...proofs2, site],
    nonce: options.nonce
  }).execute(conn);
  if (!receipt.out.ok) {
    throw new Error(`failed ${replicate.can} invocation`, {
      cause: receipt.out.error
    });
  }
  return receipt.out.ok;
}
var ability5 = replicate.can;
var input5 = (blob4, site, replicas) => ({
  blob: {
    digest: blob4.digest.bytes,
    size: blob4.size
  },
  site: site.cid,
  replicas
});

// node_modules/@storacha/upload-client/dist/index/index.js
var index_exports = {};
__export(index_exports, {
  add: () => add9
});

// node_modules/@storacha/upload-client/dist/index/add.js
async function add9({ issuer, with: resource, proofs: proofs2, audience }, index3, options = {}) {
  const conn = options.connection ?? connection6;
  const result = await pRetry(async () => {
    return await add6.invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID.from(resource),
      nb: input6(index3),
      proofs: proofs2
    }).execute(conn);
  }, {
    onFailedAttempt: console.warn,
    retries: options.retries ?? REQUEST_RETRIES
  });
  if (!result.out.ok) {
    throw new Error(`failed ${add6.can} invocation`, {
      cause: result.out.error
    });
  }
  return result.out.ok;
}
var ability6 = add6.can;
var input6 = (index3) => ({ index: index3 });

// node_modules/@storacha/upload-client/dist/upload/index.js
var upload_exports2 = {};
__export(upload_exports2, {
  add: () => add10,
  get: () => get11,
  list: () => list7,
  remove: () => remove6
});

// node_modules/@storacha/upload-client/dist/upload/add.js
async function add10({ issuer, with: resource, proofs: proofs2, audience }, root2, shards, options = {}) {
  const conn = options.connection ?? connection6;
  const result = await pRetry(async () => {
    return await add2.invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID.from(resource),
      nb: input7(root2, shards),
      proofs: proofs2,
      nonce: options.nonce
    }).execute(conn);
  }, {
    onFailedAttempt: console.warn,
    retries: options.retries ?? REQUEST_RETRIES
  });
  if (!result.out.ok) {
    throw new Error(`failed ${add2.can} invocation`, {
      cause: result.out.error
    });
  }
  return result.out.ok;
}
var ability7 = add2.can;
var input7 = (root2, shards) => ({ root: root2, shards });

// node_modules/@storacha/upload-client/dist/upload/get.js
async function get11({ issuer, with: resource, proofs: proofs2, audience }, root2, options = {}) {
  const conn = options.connection ?? connection6;
  const result = await pRetry(async () => {
    return await get3.invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID.from(resource),
      nb: input8(root2),
      proofs: proofs2,
      nonce: options.nonce
    }).execute(conn);
  }, {
    onFailedAttempt: console.warn,
    retries: options.retries ?? REQUEST_RETRIES
  });
  if (!result.out.ok) {
    throw new Error(`failed ${get3.can} invocation`, {
      cause: result.out.error
    });
  }
  return result.out.ok;
}
var ability8 = get3.can;
var input8 = (root2) => ({ root: root2 });

// node_modules/@storacha/upload-client/dist/upload/list.js
async function list7({ issuer, with: resource, proofs: proofs2, audience }, options = {}) {
  const conn = options.connection ?? connection6;
  const result = await list2.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? servicePrincipal,
    with: SpaceDID.from(resource),
    proofs: proofs2,
    nb: input9(options.cursor, options.size, options.pre),
    nonce: options.nonce
  }).execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${list2.can} invocation`, {
      cause: result.out.error
    });
  }
  return result.out.ok;
}
var ability9 = list2.can;
var input9 = (cursor, size5, pre) => ({ cursor, size: size5, pre });

// node_modules/@storacha/upload-client/dist/upload/remove.js
async function remove6({ issuer, with: resource, proofs: proofs2, audience }, root2, options = {}) {
  const conn = options.connection ?? connection6;
  const result = await remove2.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? servicePrincipal,
    with: SpaceDID.from(resource),
    nb: input10(root2),
    proofs: proofs2,
    nonce: options.nonce
  }).execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${remove2.can} invocation`, {
      cause: result.out.error
    });
  }
  return result.out.ok;
}
var ability10 = remove2.can;
var input10 = (root2) => ({ root: root2 });

// node_modules/@ipld/unixfs/src/codec.js
var codec_exports2 = {};
__export(codec_exports2, {
  DEFAULT_DIRECTORY_MODE: () => DEFAULT_DIRECTORY_MODE,
  DEFAULT_FILE_MODE: () => DEFAULT_FILE_MODE,
  NodeType: () => NodeType,
  code: () => code17,
  createAdvancedFile: () => createAdvancedFile,
  createComplexFile: () => createComplexFile,
  createDirectoryShard: () => createDirectoryShard,
  createEmptyFile: () => createEmptyFile,
  createFileChunk: () => createFileChunk,
  createFileShard: () => createFileShard,
  createFlatDirectory: () => createFlatDirectory,
  createRaw: () => createRaw,
  createShardedDirectory: () => createShardedDirectory,
  createSimpleFile: () => createSimpleFile,
  createSymlink: () => createSymlink,
  cumulativeContentByteLength: () => cumulativeContentByteLength,
  cumulativeDagByteLength: () => cumulativeDagByteLength,
  decode: () => decode33,
  decodeMetadata: () => decodeMetadata,
  encode: () => encode29,
  encodeAdvancedFile: () => encodeAdvancedFile,
  encodeComplexFile: () => encodeComplexFile,
  encodeDirectory: () => encodeDirectory,
  encodeDirectoryMetadata: () => encodeDirectoryMetadata,
  encodeFile: () => encodeFile,
  encodeFileChunk: () => encodeFileChunk,
  encodeFileShard: () => encodeFileShard,
  encodeHAMTShard: () => encodeHAMTShard,
  encodeLink: () => encodeLink2,
  encodeMetadata: () => encodeMetadata,
  encodeMode: () => encodeMode,
  encodeRaw: () => encodeRaw,
  encodeSimpleFile: () => encodeSimpleFile,
  encodeSymlink: () => encodeSymlink,
  filesize: () => filesize,
  matchFile: () => matchFile,
  name: () => name13
});

// node_modules/@ipld/dag-pb/src/pb-decode.js
var textDecoder3 = new TextDecoder();
function decodeVarint2(bytes3, offset2) {
  let v = 0;
  for (let shift = 0; ; shift += 7) {
    if (shift >= 64) {
      throw new Error("protobuf: varint overflow");
    }
    if (offset2 >= bytes3.length) {
      throw new Error("protobuf: unexpected end of data");
    }
    const b = bytes3[offset2++];
    v += shift < 28 ? (b & 127) << shift : (b & 127) * 2 ** shift;
    if (b < 128) {
      break;
    }
  }
  return [v, offset2];
}
function decodeBytes(bytes3, offset2) {
  let byteLen;
  [byteLen, offset2] = decodeVarint2(bytes3, offset2);
  const postOffset = offset2 + byteLen;
  if (byteLen < 0 || postOffset < 0) {
    throw new Error("protobuf: invalid length");
  }
  if (postOffset > bytes3.length) {
    throw new Error("protobuf: unexpected end of data");
  }
  return [bytes3.subarray(offset2, postOffset), postOffset];
}
function decodeKey(bytes3, index3) {
  let wire;
  [wire, index3] = decodeVarint2(bytes3, index3);
  return [wire & 7, wire >> 3, index3];
}
function decodeLink(bytes3) {
  const link6 = {};
  const l = bytes3.length;
  let index3 = 0;
  while (index3 < l) {
    let wireType, fieldNum;
    [wireType, fieldNum, index3] = decodeKey(bytes3, index3);
    if (fieldNum === 1) {
      if (link6.Hash) {
        throw new Error("protobuf: (PBLink) duplicate Hash section");
      }
      if (wireType !== 2) {
        throw new Error(`protobuf: (PBLink) wrong wireType (${wireType}) for Hash`);
      }
      if (link6.Name !== void 0) {
        throw new Error("protobuf: (PBLink) invalid order, found Name before Hash");
      }
      if (link6.Tsize !== void 0) {
        throw new Error("protobuf: (PBLink) invalid order, found Tsize before Hash");
      }
      [link6.Hash, index3] = decodeBytes(bytes3, index3);
    } else if (fieldNum === 2) {
      if (link6.Name !== void 0) {
        throw new Error("protobuf: (PBLink) duplicate Name section");
      }
      if (wireType !== 2) {
        throw new Error(`protobuf: (PBLink) wrong wireType (${wireType}) for Name`);
      }
      if (link6.Tsize !== void 0) {
        throw new Error("protobuf: (PBLink) invalid order, found Tsize before Name");
      }
      let byts;
      [byts, index3] = decodeBytes(bytes3, index3);
      link6.Name = textDecoder3.decode(byts);
    } else if (fieldNum === 3) {
      if (link6.Tsize !== void 0) {
        throw new Error("protobuf: (PBLink) duplicate Tsize section");
      }
      if (wireType !== 0) {
        throw new Error(`protobuf: (PBLink) wrong wireType (${wireType}) for Tsize`);
      }
      [link6.Tsize, index3] = decodeVarint2(bytes3, index3);
    } else {
      throw new Error(`protobuf: (PBLink) invalid fieldNumber, expected 1, 2 or 3, got ${fieldNum}`);
    }
  }
  if (index3 > l) {
    throw new Error("protobuf: (PBLink) unexpected end of data");
  }
  return link6;
}
function decodeNode(bytes3) {
  const l = bytes3.length;
  let index3 = 0;
  let links3 = void 0;
  let linksBeforeData = false;
  let data = void 0;
  while (index3 < l) {
    let wireType, fieldNum;
    [wireType, fieldNum, index3] = decodeKey(bytes3, index3);
    if (wireType !== 2) {
      throw new Error(`protobuf: (PBNode) invalid wireType, expected 2, got ${wireType}`);
    }
    if (fieldNum === 1) {
      if (data) {
        throw new Error("protobuf: (PBNode) duplicate Data section");
      }
      [data, index3] = decodeBytes(bytes3, index3);
      if (links3) {
        linksBeforeData = true;
      }
    } else if (fieldNum === 2) {
      if (linksBeforeData) {
        throw new Error("protobuf: (PBNode) duplicate Links section");
      } else if (!links3) {
        links3 = [];
      }
      let byts;
      [byts, index3] = decodeBytes(bytes3, index3);
      links3.push(decodeLink(byts));
    } else {
      throw new Error(`protobuf: (PBNode) invalid fieldNumber, expected 1 or 2, got ${fieldNum}`);
    }
  }
  if (index3 > l) {
    throw new Error("protobuf: (PBNode) unexpected end of data");
  }
  const node = {};
  if (data) {
    node.Data = data;
  }
  node.Links = links3 || [];
  return node;
}

// node_modules/@ipld/dag-pb/src/pb-encode.js
var textEncoder3 = new TextEncoder();
var maxInt32 = 2 ** 32;
var maxUInt32 = 2 ** 31;
function encodeLink(link6, bytes3) {
  let i = bytes3.length;
  if (typeof link6.Tsize === "number") {
    if (link6.Tsize < 0) {
      throw new Error("Tsize cannot be negative");
    }
    if (!Number.isSafeInteger(link6.Tsize)) {
      throw new Error("Tsize too large for encoding");
    }
    i = encodeVarint(bytes3, i, link6.Tsize) - 1;
    bytes3[i] = 24;
  }
  if (typeof link6.Name === "string") {
    const nameBytes = textEncoder3.encode(link6.Name);
    i -= nameBytes.length;
    bytes3.set(nameBytes, i);
    i = encodeVarint(bytes3, i, nameBytes.length) - 1;
    bytes3[i] = 18;
  }
  if (link6.Hash) {
    i -= link6.Hash.length;
    bytes3.set(link6.Hash, i);
    i = encodeVarint(bytes3, i, link6.Hash.length) - 1;
    bytes3[i] = 10;
  }
  return bytes3.length - i;
}
function encodeNode(node) {
  const size5 = sizeNode(node);
  const bytes3 = new Uint8Array(size5);
  let i = size5;
  if (node.Data) {
    i -= node.Data.length;
    bytes3.set(node.Data, i);
    i = encodeVarint(bytes3, i, node.Data.length) - 1;
    bytes3[i] = 10;
  }
  if (node.Links) {
    for (let index3 = node.Links.length - 1; index3 >= 0; index3--) {
      const size6 = encodeLink(node.Links[index3], bytes3.subarray(0, i));
      i -= size6;
      i = encodeVarint(bytes3, i, size6) - 1;
      bytes3[i] = 18;
    }
  }
  return bytes3;
}
function sizeLink(link6) {
  let n = 0;
  if (link6.Hash) {
    const l = link6.Hash.length;
    n += 1 + l + sov(l);
  }
  if (typeof link6.Name === "string") {
    const l = textEncoder3.encode(link6.Name).length;
    n += 1 + l + sov(l);
  }
  if (typeof link6.Tsize === "number") {
    n += 1 + sov(link6.Tsize);
  }
  return n;
}
function sizeNode(node) {
  let n = 0;
  if (node.Data) {
    const l = node.Data.length;
    n += 1 + l + sov(l);
  }
  if (node.Links) {
    for (const link6 of node.Links) {
      const l = sizeLink(link6);
      n += 1 + l + sov(l);
    }
  }
  return n;
}
function encodeVarint(bytes3, offset2, v) {
  offset2 -= sov(v);
  const base3 = offset2;
  while (v >= maxUInt32) {
    bytes3[offset2++] = v & 127 | 128;
    v /= 128;
  }
  while (v >= 128) {
    bytes3[offset2++] = v & 127 | 128;
    v >>>= 7;
  }
  bytes3[offset2] = v;
  return base3;
}
function sov(x) {
  if (x % 2 === 0) {
    x++;
  }
  return Math.floor((len64(x) + 6) / 7);
}
function len64(x) {
  let n = 0;
  if (x >= maxInt32) {
    x = Math.floor(x / maxInt32);
    n = 32;
  }
  if (x >= 1 << 16) {
    x >>>= 16;
    n += 16;
  }
  if (x >= 1 << 8) {
    x >>>= 8;
    n += 8;
  }
  return n + len8tab[x];
}
var len8tab = [
  0,
  1,
  2,
  2,
  3,
  3,
  3,
  3,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  4,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  5,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  6,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  7,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8,
  8
];

// node_modules/@ipld/dag-pb/src/util.js
var pbNodeProperties = ["Data", "Links"];
var pbLinkProperties = ["Hash", "Name", "Tsize"];
var textEncoder4 = new TextEncoder();
function linkComparator(a, b) {
  if (a === b) {
    return 0;
  }
  const abuf = a.Name ? textEncoder4.encode(a.Name) : [];
  const bbuf = b.Name ? textEncoder4.encode(b.Name) : [];
  let x = abuf.length;
  let y = bbuf.length;
  for (let i = 0, len = Math.min(x, y); i < len; ++i) {
    if (abuf[i] !== bbuf[i]) {
      x = abuf[i];
      y = bbuf[i];
      break;
    }
  }
  return x < y ? -1 : y < x ? 1 : 0;
}
function hasOnlyProperties(node, properties) {
  return !Object.keys(node).some((p) => !properties.includes(p));
}
function asLink(link6) {
  if (typeof link6.asCID === "object") {
    const Hash = CID.asCID(link6);
    if (!Hash) {
      throw new TypeError("Invalid DAG-PB form");
    }
    return { Hash };
  }
  if (typeof link6 !== "object" || Array.isArray(link6)) {
    throw new TypeError("Invalid DAG-PB form");
  }
  const pbl = {};
  if (link6.Hash) {
    let cid = CID.asCID(link6.Hash);
    try {
      if (!cid) {
        if (typeof link6.Hash === "string") {
          cid = CID.parse(link6.Hash);
        } else if (link6.Hash instanceof Uint8Array) {
          cid = CID.decode(link6.Hash);
        }
      }
    } catch (e) {
      throw new TypeError(`Invalid DAG-PB form: ${e.message}`);
    }
    if (cid) {
      pbl.Hash = cid;
    }
  }
  if (!pbl.Hash) {
    throw new TypeError("Invalid DAG-PB form");
  }
  if (typeof link6.Name === "string") {
    pbl.Name = link6.Name;
  }
  if (typeof link6.Tsize === "number") {
    pbl.Tsize = link6.Tsize;
  }
  return pbl;
}
function prepare2(node) {
  if (node instanceof Uint8Array || typeof node === "string") {
    node = { Data: node };
  }
  if (typeof node !== "object" || Array.isArray(node)) {
    throw new TypeError("Invalid DAG-PB form");
  }
  const pbn = {};
  if (node.Data !== void 0) {
    if (typeof node.Data === "string") {
      pbn.Data = textEncoder4.encode(node.Data);
    } else if (node.Data instanceof Uint8Array) {
      pbn.Data = node.Data;
    } else {
      throw new TypeError("Invalid DAG-PB form");
    }
  }
  if (node.Links !== void 0) {
    if (Array.isArray(node.Links)) {
      pbn.Links = node.Links.map(asLink);
      pbn.Links.sort(linkComparator);
    } else {
      throw new TypeError("Invalid DAG-PB form");
    }
  } else {
    pbn.Links = [];
  }
  return pbn;
}
function validate2(node) {
  if (!node || typeof node !== "object" || Array.isArray(node) || node instanceof Uint8Array || node["/"] && node["/"] === node.bytes) {
    throw new TypeError("Invalid DAG-PB form");
  }
  if (!hasOnlyProperties(node, pbNodeProperties)) {
    throw new TypeError("Invalid DAG-PB form (extraneous properties)");
  }
  if (node.Data !== void 0 && !(node.Data instanceof Uint8Array)) {
    throw new TypeError("Invalid DAG-PB form (Data must be bytes)");
  }
  if (!Array.isArray(node.Links)) {
    throw new TypeError("Invalid DAG-PB form (Links must be a list)");
  }
  for (let i = 0; i < node.Links.length; i++) {
    const link6 = node.Links[i];
    if (!link6 || typeof link6 !== "object" || Array.isArray(link6) || link6 instanceof Uint8Array || link6["/"] && link6["/"] === link6.bytes) {
      throw new TypeError("Invalid DAG-PB form (bad link)");
    }
    if (!hasOnlyProperties(link6, pbLinkProperties)) {
      throw new TypeError("Invalid DAG-PB form (extraneous properties on link)");
    }
    if (link6.Hash === void 0) {
      throw new TypeError("Invalid DAG-PB form (link must have a Hash)");
    }
    if (link6.Hash == null || !link6.Hash["/"] || link6.Hash["/"] !== link6.Hash.bytes) {
      throw new TypeError("Invalid DAG-PB form (link Hash must be a CID)");
    }
    if (link6.Name !== void 0 && typeof link6.Name !== "string") {
      throw new TypeError("Invalid DAG-PB form (link Name must be a string)");
    }
    if (link6.Tsize !== void 0) {
      if (typeof link6.Tsize !== "number" || link6.Tsize % 1 !== 0) {
        throw new TypeError("Invalid DAG-PB form (link Tsize must be an integer)");
      }
      if (link6.Tsize < 0) {
        throw new TypeError("Invalid DAG-PB form (link Tsize cannot be negative)");
      }
    }
    if (i > 0 && linkComparator(link6, node.Links[i - 1]) === -1) {
      throw new TypeError("Invalid DAG-PB form (links must be sorted by Name bytes)");
    }
  }
}
function toByteView3(buf2) {
  if (buf2 instanceof ArrayBuffer) {
    return new Uint8Array(buf2, 0, buf2.byteLength);
  }
  return buf2;
}

// node_modules/@ipld/dag-pb/src/index.js
var code16 = 112;
function encode28(node) {
  validate2(node);
  const pbn = {};
  if (node.Links) {
    pbn.Links = node.Links.map((l) => {
      const link6 = {};
      if (l.Hash) {
        link6.Hash = l.Hash.bytes;
      }
      if (l.Name !== void 0) {
        link6.Name = l.Name;
      }
      if (l.Tsize !== void 0) {
        link6.Tsize = l.Tsize;
      }
      return link6;
    });
  }
  if (node.Data) {
    pbn.Data = node.Data;
  }
  return encodeNode(pbn);
}
function decode32(bytes3) {
  const buf2 = toByteView3(bytes3);
  const pbn = decodeNode(buf2);
  const node = {};
  if (pbn.Data) {
    node.Data = pbn.Data;
  }
  if (pbn.Links) {
    node.Links = pbn.Links.map((l) => {
      const link6 = {};
      try {
        link6.Hash = CID.decode(l.Hash);
      } catch {
      }
      if (!link6.Hash) {
        throw new Error("Invalid Hash field found in link, expected CID");
      }
      if (l.Name !== void 0) {
        link6.Name = l.Name;
      }
      if (l.Tsize !== void 0) {
        link6.Tsize = l.Tsize;
      }
      return link6;
    });
  }
  return node;
}

// node_modules/@ipld/unixfs/gen/unixfs.js
var import_minimal = __toESM(require_minimal2(), 1);
var $Reader = import_minimal.default.Reader;
var $Writer = import_minimal.default.Writer;
var $util = import_minimal.default.util;
var $root = import_minimal.default.roots.unixfs || (import_minimal.default.roots.unixfs = {});
var Data = $root.Data = (() => {
  function Data2(p) {
    this.blocksizes = [];
    if (p) {
      for (var ks = Object.keys(p), i = 0; i < ks.length; ++i)
        if (p[ks[i]] != null)
          this[ks[i]] = p[ks[i]];
    }
  }
  Data2.prototype.Type = 0;
  Data2.prototype.Data = $util.newBuffer([]);
  Data2.prototype.filesize = $util.Long ? $util.Long.fromBits(0, 0, true) : 0;
  Data2.prototype.blocksizes = $util.emptyArray;
  Data2.prototype.hashType = $util.Long ? $util.Long.fromBits(0, 0, true) : 0;
  Data2.prototype.fanout = $util.Long ? $util.Long.fromBits(0, 0, true) : 0;
  Data2.prototype.mode = 0;
  Data2.prototype.mtime = null;
  Data2.encode = function encode32(m, w) {
    if (!w)
      w = $Writer.create();
    w.uint32(8).int32(m.Type);
    if (m.Data != null && Object.hasOwnProperty.call(m, "Data"))
      w.uint32(18).bytes(m.Data);
    if (m.filesize != null && Object.hasOwnProperty.call(m, "filesize"))
      w.uint32(24).uint64(m.filesize);
    if (m.blocksizes != null && m.blocksizes.length) {
      for (var i = 0; i < m.blocksizes.length; ++i)
        w.uint32(32).uint64(m.blocksizes[i]);
    }
    if (m.hashType != null && Object.hasOwnProperty.call(m, "hashType"))
      w.uint32(40).uint64(m.hashType);
    if (m.fanout != null && Object.hasOwnProperty.call(m, "fanout"))
      w.uint32(48).uint64(m.fanout);
    if (m.mode != null && Object.hasOwnProperty.call(m, "mode"))
      w.uint32(56).uint32(m.mode);
    if (m.mtime != null && Object.hasOwnProperty.call(m, "mtime"))
      $root.UnixTime.encode(m.mtime, w.uint32(66).fork()).ldelim();
    return w;
  };
  Data2.decode = function decode36(r, l) {
    if (!(r instanceof $Reader))
      r = $Reader.create(r);
    var c = l === void 0 ? r.len : r.pos + l, m = new $root.Data();
    while (r.pos < c) {
      var t = r.uint32();
      switch (t >>> 3) {
        case 1:
          m.Type = r.int32();
          break;
        case 2:
          m.Data = r.bytes();
          break;
        case 3:
          m.filesize = r.uint64();
          break;
        case 4:
          if (!(m.blocksizes && m.blocksizes.length))
            m.blocksizes = [];
          if ((t & 7) === 2) {
            var c2 = r.uint32() + r.pos;
            while (r.pos < c2)
              m.blocksizes.push(r.uint64());
          } else
            m.blocksizes.push(r.uint64());
          break;
        case 5:
          m.hashType = r.uint64();
          break;
        case 6:
          m.fanout = r.uint64();
          break;
        case 7:
          m.mode = r.uint32();
          break;
        case 8:
          m.mtime = $root.UnixTime.decode(r, r.uint32());
          break;
        default:
          r.skipType(t & 7);
          break;
      }
    }
    if (!m.hasOwnProperty("Type"))
      throw $util.ProtocolError("missing required 'Type'", { instance: m });
    return m;
  };
  Data2.fromObject = function fromObject(d) {
    if (d instanceof $root.Data)
      return d;
    var m = new $root.Data();
    switch (d.Type) {
      case "Raw":
      case 0:
        m.Type = 0;
        break;
      case "Directory":
      case 1:
        m.Type = 1;
        break;
      case "File":
      case 2:
        m.Type = 2;
        break;
      case "Metadata":
      case 3:
        m.Type = 3;
        break;
      case "Symlink":
      case 4:
        m.Type = 4;
        break;
      case "HAMTShard":
      case 5:
        m.Type = 5;
        break;
    }
    if (d.Data != null) {
      if (typeof d.Data === "string")
        $util.base64.decode(d.Data, m.Data = $util.newBuffer($util.base64.length(d.Data)), 0);
      else if (d.Data.length)
        m.Data = d.Data;
    }
    if (d.filesize != null) {
      if ($util.Long)
        (m.filesize = $util.Long.fromValue(d.filesize)).unsigned = true;
      else if (typeof d.filesize === "string")
        m.filesize = parseInt(d.filesize, 10);
      else if (typeof d.filesize === "number")
        m.filesize = d.filesize;
      else if (typeof d.filesize === "object")
        m.filesize = new $util.LongBits(d.filesize.low >>> 0, d.filesize.high >>> 0).toNumber(true);
    }
    if (d.blocksizes) {
      if (!Array.isArray(d.blocksizes))
        throw TypeError(".Data.blocksizes: array expected");
      m.blocksizes = [];
      for (var i = 0; i < d.blocksizes.length; ++i) {
        if ($util.Long)
          (m.blocksizes[i] = $util.Long.fromValue(d.blocksizes[i])).unsigned = true;
        else if (typeof d.blocksizes[i] === "string")
          m.blocksizes[i] = parseInt(d.blocksizes[i], 10);
        else if (typeof d.blocksizes[i] === "number")
          m.blocksizes[i] = d.blocksizes[i];
        else if (typeof d.blocksizes[i] === "object")
          m.blocksizes[i] = new $util.LongBits(d.blocksizes[i].low >>> 0, d.blocksizes[i].high >>> 0).toNumber(true);
      }
    }
    if (d.hashType != null) {
      if ($util.Long)
        (m.hashType = $util.Long.fromValue(d.hashType)).unsigned = true;
      else if (typeof d.hashType === "string")
        m.hashType = parseInt(d.hashType, 10);
      else if (typeof d.hashType === "number")
        m.hashType = d.hashType;
      else if (typeof d.hashType === "object")
        m.hashType = new $util.LongBits(d.hashType.low >>> 0, d.hashType.high >>> 0).toNumber(true);
    }
    if (d.fanout != null) {
      if ($util.Long)
        (m.fanout = $util.Long.fromValue(d.fanout)).unsigned = true;
      else if (typeof d.fanout === "string")
        m.fanout = parseInt(d.fanout, 10);
      else if (typeof d.fanout === "number")
        m.fanout = d.fanout;
      else if (typeof d.fanout === "object")
        m.fanout = new $util.LongBits(d.fanout.low >>> 0, d.fanout.high >>> 0).toNumber(true);
    }
    if (d.mode != null) {
      m.mode = d.mode >>> 0;
    }
    if (d.mtime != null) {
      if (typeof d.mtime !== "object")
        throw TypeError(".Data.mtime: object expected");
      m.mtime = $root.UnixTime.fromObject(d.mtime);
    }
    return m;
  };
  Data2.toObject = function toObject(m, o) {
    if (!o)
      o = {};
    var d = {};
    if (o.arrays || o.defaults) {
      d.blocksizes = [];
    }
    if (o.defaults) {
      d.Type = o.enums === String ? "Raw" : 0;
      if (o.bytes === String)
        d.Data = "";
      else {
        d.Data = [];
        if (o.bytes !== Array)
          d.Data = $util.newBuffer(d.Data);
      }
      if ($util.Long) {
        var n = new $util.Long(0, 0, true);
        d.filesize = o.longs === String ? n.toString() : o.longs === Number ? n.toNumber() : n;
      } else
        d.filesize = o.longs === String ? "0" : 0;
      if ($util.Long) {
        var n = new $util.Long(0, 0, true);
        d.hashType = o.longs === String ? n.toString() : o.longs === Number ? n.toNumber() : n;
      } else
        d.hashType = o.longs === String ? "0" : 0;
      if ($util.Long) {
        var n = new $util.Long(0, 0, true);
        d.fanout = o.longs === String ? n.toString() : o.longs === Number ? n.toNumber() : n;
      } else
        d.fanout = o.longs === String ? "0" : 0;
      d.mode = 0;
      d.mtime = null;
    }
    if (m.Type != null && m.hasOwnProperty("Type")) {
      d.Type = o.enums === String ? $root.Data.DataType[m.Type] : m.Type;
    }
    if (m.Data != null && m.hasOwnProperty("Data")) {
      d.Data = o.bytes === String ? $util.base64.encode(m.Data, 0, m.Data.length) : o.bytes === Array ? Array.prototype.slice.call(m.Data) : m.Data;
    }
    if (m.filesize != null && m.hasOwnProperty("filesize")) {
      if (typeof m.filesize === "number")
        d.filesize = o.longs === String ? String(m.filesize) : m.filesize;
      else
        d.filesize = o.longs === String ? $util.Long.prototype.toString.call(m.filesize) : o.longs === Number ? new $util.LongBits(m.filesize.low >>> 0, m.filesize.high >>> 0).toNumber(true) : m.filesize;
    }
    if (m.blocksizes && m.blocksizes.length) {
      d.blocksizes = [];
      for (var j = 0; j < m.blocksizes.length; ++j) {
        if (typeof m.blocksizes[j] === "number")
          d.blocksizes[j] = o.longs === String ? String(m.blocksizes[j]) : m.blocksizes[j];
        else
          d.blocksizes[j] = o.longs === String ? $util.Long.prototype.toString.call(m.blocksizes[j]) : o.longs === Number ? new $util.LongBits(m.blocksizes[j].low >>> 0, m.blocksizes[j].high >>> 0).toNumber(true) : m.blocksizes[j];
      }
    }
    if (m.hashType != null && m.hasOwnProperty("hashType")) {
      if (typeof m.hashType === "number")
        d.hashType = o.longs === String ? String(m.hashType) : m.hashType;
      else
        d.hashType = o.longs === String ? $util.Long.prototype.toString.call(m.hashType) : o.longs === Number ? new $util.LongBits(m.hashType.low >>> 0, m.hashType.high >>> 0).toNumber(true) : m.hashType;
    }
    if (m.fanout != null && m.hasOwnProperty("fanout")) {
      if (typeof m.fanout === "number")
        d.fanout = o.longs === String ? String(m.fanout) : m.fanout;
      else
        d.fanout = o.longs === String ? $util.Long.prototype.toString.call(m.fanout) : o.longs === Number ? new $util.LongBits(m.fanout.low >>> 0, m.fanout.high >>> 0).toNumber(true) : m.fanout;
    }
    if (m.mode != null && m.hasOwnProperty("mode")) {
      d.mode = m.mode;
    }
    if (m.mtime != null && m.hasOwnProperty("mtime")) {
      d.mtime = $root.UnixTime.toObject(m.mtime, o);
    }
    return d;
  };
  Data2.prototype.toJSON = function toJSON4() {
    return this.constructor.toObject(this, import_minimal.default.util.toJSONOptions);
  };
  Data2.DataType = function() {
    const valuesById = {}, values2 = Object.create(valuesById);
    values2[valuesById[0] = "Raw"] = 0;
    values2[valuesById[1] = "Directory"] = 1;
    values2[valuesById[2] = "File"] = 2;
    values2[valuesById[3] = "Metadata"] = 3;
    values2[valuesById[4] = "Symlink"] = 4;
    values2[valuesById[5] = "HAMTShard"] = 5;
    return values2;
  }();
  return Data2;
})();
var UnixTime = $root.UnixTime = (() => {
  function UnixTime2(p) {
    if (p) {
      for (var ks = Object.keys(p), i = 0; i < ks.length; ++i)
        if (p[ks[i]] != null)
          this[ks[i]] = p[ks[i]];
    }
  }
  UnixTime2.prototype.Seconds = $util.Long ? $util.Long.fromBits(0, 0, false) : 0;
  UnixTime2.prototype.FractionalNanoseconds = 0;
  UnixTime2.encode = function encode32(m, w) {
    if (!w)
      w = $Writer.create();
    w.uint32(8).int64(m.Seconds);
    if (m.FractionalNanoseconds != null && Object.hasOwnProperty.call(m, "FractionalNanoseconds"))
      w.uint32(21).fixed32(m.FractionalNanoseconds);
    return w;
  };
  UnixTime2.decode = function decode36(r, l) {
    if (!(r instanceof $Reader))
      r = $Reader.create(r);
    var c = l === void 0 ? r.len : r.pos + l, m = new $root.UnixTime();
    while (r.pos < c) {
      var t = r.uint32();
      switch (t >>> 3) {
        case 1:
          m.Seconds = r.int64();
          break;
        case 2:
          m.FractionalNanoseconds = r.fixed32();
          break;
        default:
          r.skipType(t & 7);
          break;
      }
    }
    if (!m.hasOwnProperty("Seconds"))
      throw $util.ProtocolError("missing required 'Seconds'", { instance: m });
    return m;
  };
  UnixTime2.fromObject = function fromObject(d) {
    if (d instanceof $root.UnixTime)
      return d;
    var m = new $root.UnixTime();
    if (d.Seconds != null) {
      if ($util.Long)
        (m.Seconds = $util.Long.fromValue(d.Seconds)).unsigned = false;
      else if (typeof d.Seconds === "string")
        m.Seconds = parseInt(d.Seconds, 10);
      else if (typeof d.Seconds === "number")
        m.Seconds = d.Seconds;
      else if (typeof d.Seconds === "object")
        m.Seconds = new $util.LongBits(d.Seconds.low >>> 0, d.Seconds.high >>> 0).toNumber();
    }
    if (d.FractionalNanoseconds != null) {
      m.FractionalNanoseconds = d.FractionalNanoseconds >>> 0;
    }
    return m;
  };
  UnixTime2.toObject = function toObject(m, o) {
    if (!o)
      o = {};
    var d = {};
    if (o.defaults) {
      if ($util.Long) {
        var n = new $util.Long(0, 0, false);
        d.Seconds = o.longs === String ? n.toString() : o.longs === Number ? n.toNumber() : n;
      } else
        d.Seconds = o.longs === String ? "0" : 0;
      d.FractionalNanoseconds = 0;
    }
    if (m.Seconds != null && m.hasOwnProperty("Seconds")) {
      if (typeof m.Seconds === "number")
        d.Seconds = o.longs === String ? String(m.Seconds) : m.Seconds;
      else
        d.Seconds = o.longs === String ? $util.Long.prototype.toString.call(m.Seconds) : o.longs === Number ? new $util.LongBits(m.Seconds.low >>> 0, m.Seconds.high >>> 0).toNumber() : m.Seconds;
    }
    if (m.FractionalNanoseconds != null && m.hasOwnProperty("FractionalNanoseconds")) {
      d.FractionalNanoseconds = m.FractionalNanoseconds;
    }
    return d;
  };
  UnixTime2.prototype.toJSON = function toJSON4() {
    return this.constructor.toObject(this, import_minimal.default.util.toJSONOptions);
  };
  return UnixTime2;
})();
var Metadata = $root.Metadata = (() => {
  function Metadata2(p) {
    if (p) {
      for (var ks = Object.keys(p), i = 0; i < ks.length; ++i)
        if (p[ks[i]] != null)
          this[ks[i]] = p[ks[i]];
    }
  }
  Metadata2.prototype.MimeType = "";
  Metadata2.encode = function encode32(m, w) {
    if (!w)
      w = $Writer.create();
    if (m.MimeType != null && Object.hasOwnProperty.call(m, "MimeType"))
      w.uint32(10).string(m.MimeType);
    return w;
  };
  Metadata2.decode = function decode36(r, l) {
    if (!(r instanceof $Reader))
      r = $Reader.create(r);
    var c = l === void 0 ? r.len : r.pos + l, m = new $root.Metadata();
    while (r.pos < c) {
      var t = r.uint32();
      switch (t >>> 3) {
        case 1:
          m.MimeType = r.string();
          break;
        default:
          r.skipType(t & 7);
          break;
      }
    }
    return m;
  };
  Metadata2.fromObject = function fromObject(d) {
    if (d instanceof $root.Metadata)
      return d;
    var m = new $root.Metadata();
    if (d.MimeType != null) {
      m.MimeType = String(d.MimeType);
    }
    return m;
  };
  Metadata2.toObject = function toObject(m, o) {
    if (!o)
      o = {};
    var d = {};
    if (o.defaults) {
      d.MimeType = "";
    }
    if (m.MimeType != null && m.hasOwnProperty("MimeType")) {
      d.MimeType = m.MimeType;
    }
    return d;
  };
  Metadata2.prototype.toJSON = function toJSON4() {
    return this.constructor.toObject(this, import_minimal.default.util.toJSONOptions);
  };
  return Metadata2;
})();

// node_modules/@ipld/unixfs/src/unixfs.js
var NodeType = Data.DataType;

// node_modules/@ipld/unixfs/src/codec.js
var EMPTY2 = Object.freeze([]);
var EMPTY_BUFFER = new Uint8Array(0);
var BLANK = Object.freeze({});
var DEFAULT_FILE_MODE = parseInt("0644", 8);
var DEFAULT_DIRECTORY_MODE = parseInt("0755", 8);
var code17 = code16;
var name13 = "UnixFS";
var encodePB = (data, links3) => {
  Object(globalThis).debug && console.log({ data, links: links3 });
  return encode28(
    // We run through prepare as links need to be sorted by name which it will
    // do.
    prepare2({
      Data: Data.encode(data).finish(),
      // We can cast to mutable array as we know no mutation occurs there
      Links: (
        /** @type {PB.PBLink[]} */
        links3
      )
    })
  );
};
var createRaw = (content2) => ({
  type: NodeType.Raw,
  content: content2
});
var createEmptyFile = (metadata) => createSimpleFile(EMPTY_BUFFER, metadata);
var createSimpleFile = (content2, metadata) => ({
  type: NodeType.File,
  layout: "simple",
  content: content2,
  metadata: decodeMetadata(metadata)
});
var createFileChunk = (content2) => ({
  type: NodeType.File,
  layout: "simple",
  content: content2
});
var createAdvancedFile = (parts, metadata) => ({
  type: NodeType.File,
  layout: "advanced",
  parts,
  metadata: decodeMetadata(metadata)
});
var createFileShard = (parts) => ({
  type: NodeType.File,
  layout: "advanced",
  parts
});
var createComplexFile = (content2, parts, metadata) => ({
  type: NodeType.File,
  layout: "complex",
  content: content2,
  parts,
  metadata: decodeMetadata(metadata)
});
var createFlatDirectory = (entries3, metadata) => ({
  type: NodeType.Directory,
  metadata: decodeMetadata(metadata),
  entries: entries3
});
var createShardedDirectory = (entries3, bitfield, fanout, hashType, metadata = BLANK) => ({
  type: NodeType.HAMTShard,
  bitfield,
  fanout: readFanout(fanout),
  hashType: readInt3(hashType),
  entries: entries3,
  metadata: decodeMetadata(metadata)
});
var createDirectoryShard = (entries3, bitfield, fanout, hashType) => ({
  type: NodeType.HAMTShard,
  bitfield,
  fanout: readFanout(fanout),
  hashType: readInt3(hashType),
  entries: entries3
});
var encodeRaw = (content2) => encodePB(
  {
    Type: NodeType.Raw,
    // TODO:
    Data: content2.byteLength > 0 ? content2 : void 0,
    filesize: content2.byteLength,
    // @ts-ignore
    blocksizes: EMPTY2
  },
  []
);
var encodeFile = (node, ignoreMetadata = false) => {
  const metadata = ignoreMetadata ? BLANK : Object(node).metadata;
  switch (node.layout) {
    case "simple":
      return encodeSimpleFile(node.content, metadata);
    case "advanced":
      return encodeAdvancedFile(node.parts, metadata);
    case "complex":
      return encodeComplexFile(node.content, node.parts, metadata);
    default:
      throw new TypeError(
        `File with unknown layout "${Object(node).layout}" was passed`
      );
  }
};
var encodeFileChunk = (content2) => encodeSimpleFile(content2, BLANK);
var encodeFileShard = (parts) => encodePB(
  {
    Type: NodeType.File,
    blocksizes: parts.map(contentByteLength),
    filesize: cumulativeContentByteLength(parts)
  },
  parts.map(encodeLink2)
);
var encodeAdvancedFile = (parts, metadata = BLANK) => encodePB(
  {
    Type: NodeType.File,
    blocksizes: parts.map(contentByteLength),
    filesize: cumulativeContentByteLength(parts),
    ...encodeMetadata(metadata)
  },
  parts.map(encodeLink2)
);
var encodeLink2 = (dag) => ({
  Name: "",
  Tsize: dag.dagByteLength,
  // @ts-ignore - @see https://github.com/multiformats/js-multiformats/pull/161
  Hash: dag.cid
});
var encodeSimpleFile = (content2, metadata = BLANK) => encodePB(
  {
    Type: NodeType.File,
    // adding empty file to both go-ipfs and js-ipfs produces block in
    // which `Data` is omitted but filesize and blocksizes are present.
    // For the sake of hash consistency we do the same.
    Data: content2.byteLength > 0 ? content2 : void 0,
    filesize: content2.byteLength,
    blocksizes: [],
    ...encodeMetadata(metadata)
  },
  []
);
var encodeComplexFile = (content2, parts, metadata = BLANK) => encodePB(
  {
    Type: NodeType.File,
    Data: content2,
    filesize: content2.byteLength + cumulativeContentByteLength(parts),
    blocksizes: parts.map(contentByteLength)
  },
  parts.map(encodeLink2)
);
var encodeDirectory = (node) => encodePB(
  {
    Type: node.type,
    ...encodeDirectoryMetadata(node.metadata || BLANK)
  },
  node.entries.map(encodeNamedLink)
);
var encodeHAMTShard = ({
  bitfield,
  fanout,
  hashType,
  entries: entries3,
  metadata = BLANK
}) => encodePB(
  {
    Type: NodeType.HAMTShard,
    Data: bitfield.byteLength > 0 ? bitfield : void 0,
    fanout: readFanout(fanout),
    hashType: readInt3(hashType),
    ...encodeDirectoryMetadata(metadata)
  },
  entries3.map(encodeNamedLink)
);
var readFanout = (n) => {
  if (Math.log2(n) % 1 === 0) {
    return n;
  } else {
    throw new TypeError(
      `Expected hamt size to be a power of two instead got ${n}`
    );
  }
};
var readInt3 = (n) => {
  if (Number.isInteger(n)) {
    return n;
  } else {
    throw new TypeError(`Expected an integer value instead got ${n}`);
  }
};
var createSymlink = (path, metadata = BLANK) => ({
  type: NodeType.Symlink,
  content: path,
  metadata: decodeMetadata(metadata)
});
var encodeSymlink = (node, ignoreMetadata = false) => {
  const metadata = ignoreMetadata ? BLANK : Object(node).metadata;
  return encodePB(
    {
      Type: NodeType.Symlink,
      Data: node.content,
      ...encodeMetadata(metadata || BLANK)
    },
    []
  );
};
var encode29 = (node, root2 = true) => {
  switch (node.type) {
    case NodeType.Raw:
      return encodeRaw(node.content);
    case NodeType.File:
      return encodeFile(node);
    case NodeType.Directory:
      return encodeDirectory(node);
    case NodeType.HAMTShard:
      return encodeHAMTShard(node);
    case NodeType.Symlink:
      return encodeSymlink(node);
    default:
      throw new Error(`Unknown node type ${Object(node).type}`);
  }
};
var decode33 = (bytes3) => {
  const pb = decode32(bytes3);
  const message = Data.decode(
    /** @type {Uint8Array} */
    pb.Data
  );
  const {
    Type: type2,
    Data: data,
    mtime,
    mode,
    blocksizes,
    ...rest
  } = Data.toObject(message, {
    defaults: false,
    arrays: true,
    longs: Number,
    objects: false
  });
  const metadata = {
    ...mode && { mode },
    ...decodeMtime(mtime)
  };
  const links3 = pb.Links;
  switch (message.Type) {
    case NodeType.Raw:
      return createRaw(data);
    case NodeType.File:
      if (links3.length === 0) {
        return new SimpleFileView(data, metadata);
      } else if (data.byteLength === 0) {
        return new AdvancedFileView(
          decodeFileLinks(rest.blocksizes, links3),
          metadata
        );
      } else {
        return new ComplexFileView(
          data,
          decodeFileLinks(rest.blocksizes, links3),
          metadata
        );
      }
    case NodeType.Directory:
      return createFlatDirectory(decodeDirectoryLinks(links3), metadata);
    case NodeType.HAMTShard:
      return createShardedDirectory(
        decodeDirectoryLinks(links3),
        data || EMPTY_BUFFER,
        rest.fanout,
        rest.hashType,
        metadata
      );
    case NodeType.Symlink:
      return createSymlink(data, metadata);
    default:
      throw new TypeError(`Unsupported node type ${message.Type}`);
  }
};
var decodeMtime = (mtime) => mtime == null ? void 0 : {
  mtime: { secs: mtime.Seconds, nsecs: mtime.FractionalNanoseconds || 0 }
};
var decodeFileLinks = (blocksizes, links3) => {
  const parts = [];
  const length2 = blocksizes.length;
  let n = 0;
  while (n < length2) {
    parts.push(
      /** @type {UnixFS.FileLink} */
      {
        cid: links3[n].Hash,
        dagByteLength: links3[n].Tsize || 0,
        contentByteLength: blocksizes[n]
      }
    );
  }
  return parts;
};
var decodeDirectoryLinks = (links3) => links3.map(
  (link6) => (
    /** @type {UnixFS.DirectoryEntryLink} */
    {
      cid: link6.Hash,
      name: link6.Name || "",
      dagByteLength: link6.Tsize || 0
    }
  )
);
var cumulativeContentByteLength = (links3) => links3.reduce((size5, link6) => size5 + link6.contentByteLength, 0);
var cumulativeDagByteLength = (root2, links3) => links3.reduce((size5, link6) => size5 + link6.dagByteLength, root2.byteLength);
var contentByteLength = (link6) => link6.contentByteLength;
var encodeNamedLink = ({ name: name15, dagByteLength, cid }) => ({
  Name: name15,
  Tsize: dagByteLength,
  Hash: cid
});
var encodeDirectoryMetadata = (metadata) => encodeMetadata(metadata, DEFAULT_DIRECTORY_MODE);
var encodeMetadata = ({ mode, mtime }, defaultMode = DEFAULT_FILE_MODE) => ({
  mode: mode != null ? encodeMode(mode, defaultMode) : void 0,
  mtime: mtime != null ? encodeMTime(mtime) : void 0
});
var decodeMetadata = (data) => data == null ? BLANK : {
  ...data.mode == null ? void 0 : { mode: decodeMode(data.mode) },
  ...data.mtime == null ? void 0 : { mtime: data.mtime }
};
var encodeMTime = (mtime) => {
  return mtime == null ? void 0 : mtime.nsecs !== 0 ? { Seconds: mtime.secs, FractionalNanoseconds: mtime.nsecs } : { Seconds: mtime.secs };
};
var encodeMode = (specifiedMode, defaultMode) => {
  const mode = specifiedMode == null ? void 0 : decodeMode(specifiedMode);
  return mode === defaultMode || mode == null ? void 0 : mode;
};
var decodeMode = (mode) => mode & 4095 | mode & 4294963200;
var matchFile = ({
  content: content2 = EMPTY_BUFFER,
  parts = EMPTY2,
  metadata = BLANK,
  ...rest
}) => {
  if (parts.length === 0) {
    return new SimpleFileView(content2, metadata);
  } else if (content2.byteLength === 0) {
    return new AdvancedFileView(parts, metadata);
  } else {
    return new ComplexFileView(content2, parts, metadata);
  }
};
var SimpleFileView = class {
  /**
   * @param {Uint8Array} content
   * @param {UnixFS.Metadata} metadata
   */
  constructor(content2, metadata) {
    this.content = content2;
    this.metadata = metadata;
    this.layout = "simple";
    this.type = NodeType.File;
  }
  get filesize() {
    return this.content.byteLength;
  }
  encode() {
    return encodeSimpleFile(this.content, this.metadata);
  }
};
var AdvancedFileView = class {
  /**
   * @param {ReadonlyArray<UnixFS.FileLink>} parts
   * @param {UnixFS.Metadata} metadata
   */
  constructor(parts, metadata) {
    this.parts = parts;
    this.metadata = metadata;
  }
  /** @type {"advanced"} */
  get layout() {
    return "advanced";
  }
  /**
   * @returns {NodeType.File}
   */
  get type() {
    return NodeType.File;
  }
  get fileSize() {
    return cumulativeContentByteLength(this.parts);
  }
  get blockSizes() {
    return this.parts.map(contentByteLength);
  }
  encode() {
    return encodeAdvancedFile(this.parts, this.metadata);
  }
};
var ComplexFileView = class {
  /**
   * @param {Uint8Array} content
   * @param {ReadonlyArray<UnixFS.FileLink>} parts
   * @param {UnixFS.Metadata} metadata
   */
  constructor(content2, parts, metadata) {
    this.content = content2;
    this.parts = parts;
    this.metadata = metadata;
  }
  /** @type {"complex"} */
  get layout() {
    return "complex";
  }
  /**
   * @returns {NodeType.File}
   */
  get type() {
    return NodeType.File;
  }
  get fileSize() {
    return this.content.byteLength + cumulativeContentByteLength(this.parts);
  }
  get blockSizes() {
    return this.parts.map(contentByteLength);
  }
  encode() {
    return encodeComplexFile(this.content, this.parts, this.metadata);
  }
};
var filesize = (node) => {
  switch (node.type) {
    case NodeType.Raw:
    case NodeType.Symlink:
      return node.content.byteLength;
    case NodeType.File:
      switch (node.layout) {
        case "simple":
          return node.content.byteLength;
        case "advanced":
          return cumulativeContentByteLength(node.parts);
        case "complex":
          return node.content.byteLength + cumulativeContentByteLength(node.parts);
      }
    default:
      return 0;
  }
};

// node_modules/actor/src/lib.js
var effect = function* (task) {
  const message = yield* task;
  yield* send(message);
};
function* current() {
  return (
    /** @type {Task.Controller<T, X, M>} */
    yield CURRENT
  );
}
var suspend = function* () {
  yield SUSPEND;
};
var wait = function* (input11) {
  const task = yield* current();
  if (isAsync(input11)) {
    let failed = false;
    let output = void 0;
    input11.then(
      (value) => {
        failed = false;
        output = value;
        enqueue(task);
      },
      (error4) => {
        failed = true;
        output = error4;
        enqueue(task);
      }
    );
    yield* suspend();
    if (failed) {
      throw output;
    } else {
      return (
        /** @type {T} */
        output
      );
    }
  } else {
    main(wake(task));
    yield* suspend();
    return input11;
  }
};
function* wake(task) {
  enqueue(task);
}
var isAsync = (node) => node != null && typeof /** @type {{then?:unknown}} */
node.then === "function";
var send = function* (message) {
  yield (
    /** @type {Task.Message<T>} */
    message
  );
};
var listen = function* (source) {
  const forks = [];
  for (const entry of Object.entries(source)) {
    const [name15, effect2] = (
      /** @type {[Tag, Task.Effect<T>]} */
      entry
    );
    if (effect2 !== NONE) {
      forks.push(yield* fork(tag(effect2, name15)));
    }
  }
  yield* group(forks);
};
var effects = (tasks) => tasks.length > 0 ? batch(tasks.map(effect)) : NONE;
function* batch(effects2) {
  const forks = [];
  for (const effect2 of effects2) {
    forks.push(yield* fork(effect2));
  }
  yield* group(forks);
}
var tag = (effect2, tag2) => (
  // @ts-ignore
  effect2 === NONE ? NONE : effect2 instanceof Tagger ? new Tagger([...effect2.tags, tag2], effect2.source) : new Tagger([tag2], effect2)
);
var Tagger = class {
  /**
   * @param {Task.Task<Success, Failure, Message>} source
   * @param {string[]} tags
   */
  constructor(tags, source) {
    this.tags = tags;
    this.source = source;
    this.controller;
  }
  /* c8 ignore next 3 */
  [Symbol.iterator]() {
    if (!this.controller) {
      this.controller = this.source[Symbol.iterator]();
    }
    return this;
  }
  /**
   * @param {Task.TaskState<Success, Message>} state
   * @returns {Task.TaskState<Success, Tagged<Tag, Message>>}
   */
  box(state) {
    if (state.done) {
      return state;
    } else {
      switch (state.value) {
        case SUSPEND:
        case CURRENT:
          return (
            /** @type {Task.TaskState<Success, Tagged<Tag, Message>>} */
            state
          );
        default: {
          const tagged = (
            /** @type {{ done: false, value: any }} */
            state
          );
          let { value } = tagged;
          for (const tag2 of this.tags) {
            value = withTag(tag2, value);
          }
          tagged.value = value;
          return tagged;
        }
      }
    }
  }
  /**
   *
   * @param {Task.Instruction<Message>} instruction
   */
  next(instruction) {
    return this.box(this.controller.next(instruction));
  }
  /**
   *
   * @param {Failure} error
   */
  throw(error4) {
    return this.box(this.controller.throw(error4));
  }
  /**
   * @param {Success} value
   */
  return(value) {
    return this.box(this.controller.return(value));
  }
  get [Symbol.toStringTag]() {
    return "TaggedEffect";
  }
};
var none = () => NONE;
var withTag = (tag2, value) => (
  /** @type {Tagged<Tag, T>} */
  { type: tag2, [tag2]: value }
);
var CURRENT = Symbol("current");
var SUSPEND = Symbol("suspend");
var Group = class _Group {
  /**
   * @template T, X, M
   * @param {Task.Controller<T, X, M>|Task.Fork<T, X, M>} member
   * @returns {Task.Group<T, X, M>}
   */
  static of(member) {
    return (
      /** @type {{group?:Task.TaskGroup<T, X, M>}} */
      member.group || MAIN
    );
  }
  /**
   * @template T, X, M
   * @param {(Task.Controller<T, X, M>|Task.Fork<T, X, M>) & {group?:Task.TaskGroup<T, X, M>}} member
   * @param {Task.TaskGroup<T, X, M>} group
   */
  static enqueue(member, group2) {
    member.group = group2;
    group2.stack.active.push(member);
  }
  /**
   * @param {Task.Controller<T, X, M>} driver
   * @param {Task.Controller<T, X, M>[]} [active]
   * @param {Set<Task.Controller<T, X, M>>} [idle]
   * @param {Task.Stack<T, X, M>} [stack]
   */
  constructor(driver, active = [], idle = /* @__PURE__ */ new Set(), stack = new Stack(active, idle)) {
    this.driver = driver;
    this.parent = _Group.of(driver);
    this.stack = stack;
    this.id = ++ID;
  }
};
var Main = class {
  constructor() {
    this.status = IDLE;
    this.stack = new Stack();
    this.id = /** @type {0} */
    0;
  }
};
var Stack = class {
  /**
   * @param {Task.Controller<T, X, M>[]} [active]
   * @param {Set<Task.Controller<T, X, M>>} [idle]
   */
  constructor(active = [], idle = /* @__PURE__ */ new Set()) {
    this.active = active;
    this.idle = idle;
  }
  /**
   *
   * @param {Task.Stack<unknown, unknown, unknown>} stack
   * @returns
   */
  static size({ active, idle }) {
    return active.length + idle.size;
  }
};
var main = (task) => enqueue(task[Symbol.iterator]());
var enqueue = (task) => {
  let group2 = Group.of(task);
  group2.stack.active.push(task);
  group2.stack.idle.delete(task);
  while (group2.parent) {
    const { idle, active } = group2.parent.stack;
    if (idle.has(group2.driver)) {
      idle.delete(group2.driver);
      active.push(group2.driver);
    } else {
      break;
    }
    group2 = group2.parent;
  }
  if (MAIN.status === IDLE) {
    MAIN.status = ACTIVE;
    while (true) {
      try {
        for (const _message of step(MAIN)) {
        }
        MAIN.status = IDLE;
        break;
      } catch (_error) {
        MAIN.stack.active.shift();
      }
    }
  }
};
var resume = (task) => enqueue(task);
var step = function* (group2) {
  const { active } = group2.stack;
  let task = active[0];
  group2.stack.idle.delete(task);
  while (task) {
    let state = INIT;
    loop: while (!state.done && task === active[0]) {
      const instruction = state.value;
      switch (instruction) {
        case SUSPEND:
          group2.stack.idle.add(task);
          break loop;
        case CURRENT:
          state = task.next(task);
          break;
        default:
          state = task.next(
            yield (
              /** @type {M & Task.Message<M>}*/
              instruction
            )
          );
          break;
      }
    }
    active.shift();
    task = active[0];
    group2.stack.idle.delete(task);
  }
};
var fork = (task, options) => new Fork(task, options);
var exit = (handle, value) => conclude2(handle, { ok: true, value });
var abort = (handle, error4) => conclude2(handle, { ok: false, error: error4 });
function* conclude2(handle, result) {
  try {
    const task = handle;
    const state = result.ok ? task.return(result.value) : task.throw(result.error);
    if (!state.done) {
      if (state.value === SUSPEND) {
        const { idle } = Group.of(task).stack;
        idle.add(task);
      } else {
        enqueue(task);
      }
    }
  } catch (error4) {
  }
}
function* group(forks) {
  if (forks.length === 0) return;
  const self2 = yield* current();
  const group2 = new Group(self2);
  let failure = null;
  for (const fork5 of forks) {
    const { result } = fork5;
    if (result) {
      if (!result.ok && !failure) {
        failure = result;
      }
      continue;
    }
    move(fork5, group2);
  }
  try {
    if (failure) {
      throw failure.error;
    }
    while (true) {
      yield* step(group2);
      if (Stack.size(group2.stack) > 0) {
        yield* suspend();
      } else {
        break;
      }
    }
  } catch (error4) {
    for (const task of group2.stack.active) {
      yield* abort(task, error4);
    }
    for (const task of group2.stack.idle) {
      yield* abort(task, error4);
      enqueue(task);
    }
    throw error4;
  }
}
var move = (fork5, group2) => {
  const from18 = Group.of(fork5);
  if (from18 !== group2) {
    const { active, idle } = from18.stack;
    const target = group2.stack;
    fork5.group = group2;
    if (idle.has(fork5)) {
      idle.delete(fork5);
      target.idle.add(fork5);
    } else {
      const index3 = active.indexOf(fork5);
      if (index3 >= 0) {
        active.splice(index3, 1);
        target.active.push(fork5);
      }
    }
  }
};
function* join2(fork5) {
  if (fork5.status === IDLE) {
    yield* fork5;
  }
  if (!fork5.result) {
    yield* group([fork5]);
  }
  const result = (
    /** @type {Task.Result<T, X>} */
    fork5.result
  );
  if (result.ok) {
    return result.value;
  } else {
    throw result.error;
  }
}
var Future = class {
  /**
   * @param {Task.StateHandler<T, X>} handler
   */
  constructor(handler) {
    this.handler = handler;
    this.result;
  }
  /**
   * @type {Promise<T>}
   */
  get promise() {
    const { result } = this;
    const promise = result == null ? new Promise((succeed, fail3) => {
      this.handler.onsuccess = succeed;
      this.handler.onfailure = fail3;
    }) : result.ok ? Promise.resolve(result.value) : Promise.reject(result.error);
    Object.defineProperty(this, "promise", { value: promise });
    return promise;
  }
  /**
   * @template U, [E=never]
   * @param {((value:T) => U | PromiseLike<U>)|undefined|null} [onresolve]
   * @param {((error:X) => E|PromiseLike<E>)|undefined|null} [onreject]
   * @returns {Promise<U|E>}
   */
  then(onresolve, onreject) {
    return this.activate().promise.then(onresolve, onreject);
  }
  /**
   * @template [U=never]
   * @param {(error:X) => U} onreject
   */
  catch(onreject) {
    return (
      /** @type {Task.Future<T|U, never>} */
      this.activate().promise.catch(onreject)
    );
  }
  /**
   * @param {() => void} onfinally
   * @returns {Task.Future<T, X>}
   */
  finally(onfinally) {
    return (
      /** @type {Task.Future<T, X>} */
      this.activate().promise.finally(onfinally)
    );
  }
  /**
   * @abstract
   */
  /* c8 ignore next 3 */
  activate() {
    return this;
  }
};
var Fork = class extends Future {
  /**
   * @param {Task.Task<T, X, M>} task
   * @param {Task.ForkOptions} [options]
   * @param {Task.StateHandler<T, X>} [handler]
   * @param {Task.TaskState<T, M>} [state]
   */
  constructor(task, options = BLANK2, handler = {}, state = INIT) {
    super(handler);
    this.id = ++ID;
    this.name = options.name || "";
    this.task = task;
    this.state = state;
    this.status = IDLE;
    this.result;
    this.handler = handler;
    this.controller;
  }
  *resume() {
    resume(this);
  }
  /**
   * @returns {Task.Task<T, X, M>}
   */
  join() {
    return join2(this);
  }
  /**
   * @param {X} error
   */
  abort(error4) {
    return abort(this, error4);
  }
  /**
   * @param {T} value
   */
  exit(value) {
    return exit(this, value);
  }
  get [Symbol.toStringTag]() {
    return "Fork";
  }
  /**
   * @returns {Task.Controller<Task.Fork<T, X, M>, never, never>}
   */
  *[Symbol.iterator]() {
    return this.activate();
  }
  activate() {
    this.controller = this.task[Symbol.iterator]();
    this.status = ACTIVE;
    enqueue(this);
    return this;
  }
  /**
   * @private
   * @param {any} error
   * @returns {never}
   */
  panic(error4) {
    this.result = { ok: false, error: error4 };
    this.status = FINISHED;
    const { handler } = this;
    if (handler.onfailure) {
      handler.onfailure(error4);
    }
    throw error4;
  }
  /**
   * @private
   * @param {Task.TaskState<T, M>} state
   */
  step(state) {
    this.state = state;
    if (state.done) {
      this.result = { ok: true, value: state.value };
      this.status = FINISHED;
      const { handler } = this;
      if (handler.onsuccess) {
        handler.onsuccess(state.value);
      }
    }
    return state;
  }
  /**
   * @param {unknown} value
   */
  next(value) {
    try {
      return this.step(this.controller.next(value));
    } catch (error4) {
      return this.panic(error4);
    }
  }
  /**
   * @param {T} value
   */
  return(value) {
    try {
      return this.step(this.controller.return(value));
    } catch (error4) {
      return this.panic(error4);
    }
  }
  /**
   * @param {X} error
   */
  throw(error4) {
    try {
      return this.step(this.controller.throw(error4));
    } catch (error5) {
      return this.panic(error5);
    }
  }
};
var loop = function* (init2, next) {
  const controller = yield* current();
  const group2 = new Group(controller);
  Group.enqueue(init2[Symbol.iterator](), group2);
  while (true) {
    for (const message of step(group2)) {
      Group.enqueue(next(message)[Symbol.iterator](), group2);
    }
    if (Stack.size(group2.stack) > 0) {
      yield* suspend();
    } else {
      break;
    }
  }
};
var ID = 0;
var IDLE = "idle";
var ACTIVE = "active";
var FINISHED = "finished";
var INIT = { done: false, value: CURRENT };
var BLANK2 = {};
var NONE = function* none2() {
}();
var MAIN = new Main();

// node_modules/@ipld/unixfs/src/file/chunker/indexed.js
function Indexed() {
}
Object.defineProperties(Indexed, {
  prototype: {
    value: new Proxy(Object.prototype, {
      /**
       * @param {object} target
       * @param {PropertyKey} property
       * @param {{get(key:PropertyKey): any}} receiver
       */
      get(target, property, receiver) {
        return typeof property === "symbol" ? Reflect.get(target, property, receiver) : receiver.get(property);
      }
    })
  }
});

// node_modules/@ipld/unixfs/src/file/chunker/buffer.js
var empty3 = () => new BufferView();
var slice2 = (buffer2, startOffset = 0, endOffset = buffer2.byteLength) => {
  const segments = [];
  const start = startOffset < 0 ? buffer2.byteLength - startOffset : startOffset;
  const end = endOffset < 0 ? buffer2.byteLength - endOffset : endOffset;
  if (start === 0 && end >= buffer2.byteLength) {
    return buffer2;
  }
  if (start > end || start > buffer2.byteLength || end <= 0) {
    return empty3();
  }
  let byteLength = 0;
  let offset2 = 0;
  for (const segment of buffer2.segments) {
    const nextOffset = offset2 + segment.byteLength;
    if (byteLength === 0) {
      if (end <= nextOffset) {
        const range = segment.subarray(start - offset2, end - offset2);
        segments.push(range);
        byteLength = range.byteLength;
        break;
      } else if (start < nextOffset) {
        const range = start === offset2 ? segment : segment.subarray(start - offset2);
        segments.push(range);
        byteLength = range.byteLength;
      }
    } else if (end <= nextOffset) {
      const range = end === nextOffset ? segment : segment.subarray(0, end - offset2);
      segments.push(range);
      byteLength += range.byteLength;
      break;
    } else {
      segments.push(segment);
      byteLength += segment.byteLength;
    }
    offset2 = nextOffset;
  }
  return new BufferView(segments, buffer2.byteOffset + start, byteLength);
};
var push = (buffer2, part) => {
  if (part.byteLength > 0) {
    buffer2.segments.push(part);
    return new BufferView(
      buffer2.segments,
      buffer2.byteOffset,
      buffer2.byteLength + part.byteLength
    );
  } else {
    return buffer2;
  }
};
var get12 = (buffer2, n) => {
  if (n < buffer2.byteLength) {
    let offset2 = 0;
    for (const segment of buffer2.segments) {
      if (n < offset2 + segment.byteLength) {
        return segment[n - offset2];
      } else {
        offset2 += segment.byteLength;
      }
    }
  }
  return void 0;
};
var copyTo = (buffer2, target, byteOffset) => {
  let offset2 = byteOffset;
  for (const segment of buffer2.segments) {
    target.set(segment, offset2);
    offset2 += segment.byteLength;
  }
  return target;
};
function* iterate2(buffer2) {
  for (const part of buffer2.segments) {
    yield* part;
  }
}
var BufferView = class extends Indexed {
  /**
   * @param {Uint8Array[]} segments
   * @param {number} byteOffset
   * @param {number} byteLength
   */
  constructor(segments = [], byteOffset = 0, byteLength = 0) {
    super();
    this.segments = segments;
    this.byteLength = byteLength;
    this.length = byteLength;
    this.byteOffset = byteOffset;
  }
  [Symbol.iterator]() {
    return iterate2(this);
  }
  /**
   * @param {number} [start]
   * @param {number} [end]
   */
  slice(start, end) {
    return (
      /** @type {BufferView} */
      slice2(this, start, end)
    );
  }
  /**
   * @param {number} [start]
   * @param {number} [end]
   */
  subarray(start, end) {
    return (
      /** @type {BufferView} */
      slice2(this, start, end)
    );
  }
  /**
   *
   * @param {Uint8Array} bytes
   */
  push(bytes3) {
    return (
      /** @type {BufferView} */
      push(this, bytes3)
    );
  }
  /**
   * @param {number} n
   */
  get(n) {
    return get12(this, n);
  }
  /**
   *
   * @param {Uint8Array} target
   * @param {number} offset
   */
  copyTo(target, offset2) {
    return copyTo(this, target, offset2);
  }
};

// node_modules/@ipld/unixfs/src/writer/util.js
var panic2 = (reason) => {
  throw new Error(reason);
};
var unreachable = (template, subject, ...substitutions) => panic2(String.raw(template, JSON.stringify(subject), ...substitutions));
var EMPTY_BUFFER2 = new Uint8Array(0);
var EMPTY3 = [];

// node_modules/@ipld/unixfs/src/file/chunker.js
var open2 = (config2) => ({
  config: config2,
  buffer: empty3()
});
var write4 = (state, bytes3) => bytes3.byteLength > 0 ? split2(state.config, state.buffer.push(bytes3), false) : { ...state, chunks: EMPTY3 };
var close2 = (state) => split2(state.config, state.buffer, true);
var split2 = (config2, buffer2, end) => {
  const chunker = config2.chunker;
  const chunks = [];
  let offset2 = 0;
  for (const size5 of chunker.cut(chunker.context, buffer2, end)) {
    if (size5 > 0) {
      const chunk = buffer2.subarray(offset2, offset2 + size5);
      chunks.push(chunk);
      offset2 += size5;
    }
  }
  return { config: config2, chunks, buffer: buffer2.subarray(offset2) };
};

// node_modules/@ipld/unixfs/src/file/layout/queue.js
var mutable = () => ({
  mutable: true,
  needs: {},
  nodes: {},
  links: {},
  linked: EMPTY4
});
var addNodes = (newNodes, input11) => {
  let queue = patch(input11, {});
  for (const node of newNodes) {
    const { ready, has: has3, wants } = collect(node.children, queue.links);
    if (wants.length === 0) {
      queue = patch(queue, {
        links: assign(void 0, has3),
        linked: [{ id: node.id, links: ready }]
      });
    } else {
      queue = patch(queue, {
        needs: assign(node.id, wants),
        nodes: {
          [node.id]: {
            children: node.children,
            count: wants.length
          }
        }
      });
    }
  }
  return queue;
};
var addLink = (id, link6, queue) => {
  const nodeID = queue.needs[id];
  const node = queue.nodes[nodeID];
  if (node != null) {
    if (node.count === 1) {
      const { ready, has: has3 } = collect(node.children, {
        ...queue.links,
        [id]: link6
      });
      return patch(queue, {
        needs: { [id]: void 0 },
        links: assign(void 0, has3),
        nodes: { [nodeID]: void 0 },
        linked: [{ id: nodeID, links: ready }]
      });
    } else {
      return patch(queue, {
        needs: { [id]: void 0 },
        links: { [id]: link6 },
        nodes: {
          [nodeID]: {
            ...node,
            count: node.count - 1
          }
        }
      });
    }
  } else {
    return patch(queue, {
      links: { [id]: link6 }
    });
  }
};
var patch = (queue, { needs, nodes, links: links3, linked }) => {
  const result = queue.mutable ? queue : { ...queue };
  const original = queue.mutable ? BLANK3 : void 0;
  if (needs) {
    result.needs = patchDict(queue.needs, needs, original);
  }
  if (nodes) {
    result.nodes = patchDict(queue.nodes, nodes, original);
  }
  if (links3) {
    result.links = patchDict(queue.links, links3, original);
  }
  result.linked = linked ? append(queue.linked || EMPTY4, linked, EMPTY4) : queue.linked || [];
  return (
    /** @type {Queue.Result} */
    result
  );
};
var assign = (value, keys2) => {
  const delta = (
    /** @type {Record<K, V>} */
    {}
  );
  for (const key of keys2) {
    delta[key] = value;
  }
  return delta;
};
var patchDict = (target, delta, original = target) => {
  const result = target === original ? { ...target } : target;
  for (const entry of Object.entries(delta)) {
    const [id, value] = (
      /** @type {[K, V|void]} */
      entry
    );
    if (value == null) {
      delete result[id];
    } else {
      result[id] = value;
    }
  }
  return result;
};
var append = (target, items, original = target) => {
  if (target === original) {
    return [...target, ...items];
  } else {
    for (const item of items) {
      target.push(item);
    }
    return target;
  }
};
var collect = (children, source) => {
  const has3 = [];
  const wants = [];
  const ready = [];
  for (const child of children) {
    const link6 = source[child];
    if (link6) {
      has3.push(child);
      ready.push(link6);
    } else {
      wants.push(child);
    }
  }
  return { has: has3, wants, ready };
};
var EMPTY4 = (
  /** @type {never[]} */
  Object.freeze([])
);
var BLANK3 = (
  /** @type {Record<never, never>} */
  Object.freeze({})
);

// node_modules/@ipld/unixfs/src/file/writer.js
var update = (message, state) => {
  switch (message.type) {
    case "write":
      return write5(state, message.bytes);
    case "link":
      return link5(state, message.link);
    case "block":
      return { state, effect: none() };
    case "close":
      return close3(state);
    case "end":
      return { state, effect: none() };
    default:
      return unreachable`File Writer got unknown message ${message}`;
  }
};
var init = (writer, metadata, config2) => {
  return {
    status: "open",
    metadata,
    config: config2,
    writer,
    chunker: open2({ chunker: config2.chunker }),
    layout: config2.fileLayout.open(),
    // Note: Writing in large slices e.g. 1GiB at a time creates large queues
    // with around `16353` items. Immutable version ends up copying it every
    // time state of the queue changes, which introduces significant overhead.
    // To avoid this overhead we use mutable implementation which is API
    // compatible but makes in place updates.
    // TODO: We should consider using Persistent bit-partitioned vector tries
    // instead of arrays which would provide immutable interface with neglegable
    // overhead.
    // @see https://github.com/Gozala/vectrie
    nodeQueue: mutable()
  };
};
var write5 = (state, bytes3) => {
  if (state.status === "open") {
    const { chunks, ...chunker } = write4(state.chunker, bytes3);
    const { nodes, leaves, layout } = state.config.fileLayout.write(
      state.layout,
      chunks
    );
    const { linked, ...nodeQueue } = addNodes(nodes, state.nodeQueue);
    const tasks = [
      ...encodeLeaves(leaves, state.config),
      ...encodeBranches(linked, state.config)
    ];
    return {
      state: {
        ...state,
        chunker,
        layout,
        nodeQueue
      },
      effect: listen({
        link: effects(tasks)
      })
    };
  } else {
    return panic2("Unable to perform write on closed file");
  }
};
var link5 = (state, { id, link: link6, block }) => {
  let { linked, ...nodeQueue } = addLink(id, link6, state.nodeQueue);
  const tasks = encodeBranches(linked, state.config);
  const newState = state.status === "closed" && id === state.rootID ? {
    ...state,
    status: "linked",
    link: link6,
    nodeQueue
  } : { ...state, nodeQueue };
  const end = state.status === "closed" && id === state.rootID && state.end ? state.end.resume() : none();
  return {
    state: newState,
    effect: listen({
      link: effects(tasks),
      block: writeBlock(state.writer, block),
      end
    })
  };
};
var close3 = (state) => {
  if (state.status === "open") {
    const { chunks } = close2(state.chunker);
    const { layout, ...write8 } = state.config.fileLayout.write(
      state.layout,
      chunks
    );
    const { root: root2, ...close9 } = state.config.fileLayout.close(
      layout,
      state.metadata
    );
    const [nodes, leaves] = isLeafNode(root2) ? [
      [...write8.nodes, ...close9.nodes],
      [...write8.leaves, ...close9.leaves, root2]
    ] : [
      [...write8.nodes, ...close9.nodes, root2],
      [...write8.leaves, ...close9.leaves]
    ];
    const { linked, ...nodeQueue } = addNodes(nodes, state.nodeQueue);
    const tasks = [
      ...encodeLeaves(leaves, state.config),
      ...encodeBranches(linked, state.config)
    ];
    const fork5 = fork(suspend());
    return {
      state: {
        ...state,
        chunker: null,
        layout: null,
        rootID: root2.id,
        status: "closed",
        end: fork5,
        nodeQueue
      },
      effect: listen({
        link: effects(tasks),
        end: join2(fork5)
      })
    };
  } else {
    return { state, effect: none() };
  }
};
var encodeLeaves = (leaves, config2) => leaves.map((leaf) => encodeLeaf(config2, leaf, config2.fileChunkEncoder));
var encodeLeaf = function* ({ hasher, linker }, { id, content: content2 }, encoder3) {
  const bytes3 = encoder3.encode(content2 ? asUint8Array2(content2) : EMPTY_BUFFER2);
  const hash = yield* wait(hasher.digest(bytes3));
  const cid = linker.createLink(encoder3.code, hash);
  const block = { cid, bytes: bytes3 };
  const link6 = (
    /** @type {UnixFS.FileLink} */
    {
      cid,
      contentByteLength: content2 ? content2.byteLength : 0,
      dagByteLength: bytes3.byteLength
    }
  );
  return { id, block, link: link6 };
};
var encodeBranches = (nodes, config2) => nodes.map((node) => encodeBranch(config2, node));
var encodeBranch = function* (config2, { id, links: links3 }, metadata) {
  const bytes3 = config2.fileEncoder.encode({
    type: NodeType.File,
    layout: "advanced",
    parts: links3,
    metadata
  });
  const hash = yield* wait(Promise.resolve(config2.hasher.digest(bytes3)));
  const cid = config2.linker.createLink(config2.fileEncoder.code, hash);
  const block = { bytes: bytes3, cid };
  const link6 = (
    /** @type {UnixFS.FileLink} */
    {
      cid,
      contentByteLength: cumulativeContentByteLength(links3),
      dagByteLength: cumulativeDagByteLength(bytes3, links3)
    }
  );
  return { id, block, link: link6 };
};
var writeBlock = function* (writer, block) {
  if ((writer.desiredSize || 0) <= 0) {
    yield* wait(writer.ready);
  }
  writer.write(block);
};
var asUint8Array2 = (buffer2) => buffer2 instanceof Uint8Array ? buffer2 : buffer2.copyTo(new Uint8Array(buffer2.byteLength), 0);
var isLeafNode = (node) => node.children == null;

// node_modules/@ipld/unixfs/src/file/chunker/fixed.js
var fixed_exports = {};
__export(fixed_exports, {
  context: () => context,
  cut: () => cut,
  name: () => name14,
  type: () => type,
  withMaxChunkSize: () => withMaxChunkSize
});
var name14 = "fixed";
var context = {
  maxChunkSize: 262144
};
var type = "Stateless";
var withMaxChunkSize = (maxChunkSize) => ({
  type: "Stateless",
  context: { maxChunkSize },
  name: name14,
  cut
});
var cut = ({ maxChunkSize }, { byteLength }, end) => {
  const n = byteLength / maxChunkSize | 0;
  const chunks = new Array(n).fill(maxChunkSize);
  const lastChunkSize = end ? byteLength - n * maxChunkSize : 0;
  if (lastChunkSize > 0) {
    chunks.push(lastChunkSize);
  }
  return chunks;
};

// node_modules/@ipld/unixfs/src/file/layout/balanced.js
var Node = class {
  /**
   *
   * @param {number} id
   * @param {number[]} children
   * @param {Layout.Metadata} [metadata]
   */
  constructor(id, children, metadata) {
    this.id = id;
    this.children = children;
    this.metadata = metadata;
  }
};
var withWidth = (width) => ({
  open: () => open3({ width }),
  write: write6,
  close: close4
});
var defaults = { width: 174 };
var open3 = ({ width } = defaults) => ({
  width,
  head: null,
  leafIndex: [],
  nodeIndex: [],
  lastID: 0
});
var write6 = (layout, chunks) => {
  if (chunks.length === 0) {
    return { layout, nodes: EMPTY5, leaves: EMPTY5 };
  } else {
    let { lastID } = layout;
    const [head, slices] = layout.head ? (
      // If we had a head we have more then two chunks (we already checked
      // chunks weren't empty) so we process head along with other chunks.
      [null, (chunks.unshift(layout.head), chunks)]
    ) : (
      // If we have no head no leaves and got only one chunk we have to save it
      // until we can decide what to do with it.
      chunks.length === 1 && layout.leafIndex.length === 0 ? [chunks[0], EMPTY5] : (
        // Otherwise we have no head but got enough chunks to know we'll have a
        // node.
        [null, chunks]
      )
    );
    if (slices.length === 0) {
      return { layout: { ...layout, head }, nodes: EMPTY5, leaves: EMPTY5 };
    } else {
      const leafIndex = [...layout.leafIndex];
      const leaves = [];
      for (const chunk of slices) {
        const leaf = { id: ++lastID, content: chunk };
        leaves.push(leaf);
        leafIndex.push(leaf.id);
      }
      if (leafIndex.length > layout.width) {
        return flush2({ ...layout, leafIndex, head, lastID }, leaves);
      } else {
        return {
          layout: { ...layout, head, leafIndex, lastID },
          leaves,
          nodes: EMPTY5
        };
      }
    }
  }
};
var flush2 = (state, leaves = EMPTY5, nodes = [], close9 = false) => {
  let { lastID } = state;
  const nodeIndex = state.nodeIndex.map((row) => [...row]);
  const leafIndex = [...state.leafIndex];
  const { width } = state;
  while (leafIndex.length > width || leafIndex.length > 0 && close9) {
    grow(nodeIndex, 1);
    const node = new Node(++lastID, leafIndex.splice(0, width));
    nodeIndex[0].push(node.id);
    nodes.push(node);
  }
  let depth = 0;
  while (depth < nodeIndex.length) {
    const row = nodeIndex[depth];
    depth++;
    while (row.length > width || row.length > 0 && close9 && depth < nodeIndex.length) {
      const node = new Node(++lastID, row.splice(0, width));
      grow(nodeIndex, depth + 1);
      nodeIndex[depth].push(node.id);
      nodes.push(node);
    }
  }
  return { layout: { ...state, lastID, leafIndex, nodeIndex }, leaves, nodes };
};
var close4 = (layout, metadata) => {
  const state = layout;
  if (layout.head) {
    return {
      root: { id: 1, content: layout.head, metadata },
      leaves: EMPTY5,
      nodes: EMPTY5
    };
  } else if (layout.leafIndex.length === 0) {
    return {
      root: { id: 1, metadata },
      leaves: EMPTY5,
      nodes: EMPTY5
    };
  } else {
    const { nodes, layout: layout2 } = flush2(state, EMPTY5, [], true);
    const { nodeIndex } = layout2;
    const height2 = nodeIndex.length - 1;
    const top2 = nodeIndex[height2];
    if (top2.length === 1) {
      const root2 = nodes[nodes.length - 1];
      nodes.length = nodes.length - 1;
      return { root: root2, nodes, leaves: EMPTY5 };
    } else {
      const root2 = new Node(layout2.lastID + 1, top2, metadata);
      return { root: root2, nodes, leaves: EMPTY5 };
    }
  }
};
var grow = (index3, length2) => {
  while (index3.length < length2) {
    index3.push([]);
  }
  return index3;
};
var EMPTY5 = [];

// node_modules/@ipld/unixfs/src/file.js
var defaults2 = () => ({
  chunker: fixed_exports,
  fileChunkEncoder: UnixFSLeaf,
  smallFileEncoder: UnixFSLeaf,
  fileEncoder: codec_exports2,
  fileLayout: withWidth(174),
  hasher: sha2562,
  linker: { createLink: CID.createV1 }
});
var configure = (config2) => ({
  ...defaults2(),
  ...config2
});
var UnixFSLeaf = {
  code: code17,
  name: name13,
  encode: encodeFileChunk
};
var create9 = ({ writer, metadata = {}, settings = defaults2() }) => new FileWriterView(init(writer, metadata, configure(settings)));
var write7 = async (view7, bytes3) => {
  await perform(view7, send({ type: "write", bytes: bytes3 }));
  return view7;
};
var close5 = async (view7, { releaseLock = false, closeWriter = false } = {}) => {
  await perform(view7, send({ type: "close" }));
  const { state } = view7;
  if (state.status === "linked") {
    if (closeWriter) {
      await view7.state.writer.close();
    } else if (releaseLock) {
      view7.state.writer.releaseLock();
    }
    return state.link;
  } else {
    panic2(
      `Expected writer to be in 'linked' state after close, but it is in "${state.status}" instead`
    );
  }
};
var perform = (view7, effect2) => fork(
  loop(effect2, (message) => {
    const { state, effect: effect3 } = update(message, view7.state);
    view7.state = state;
    return effect3;
  })
);
var FileWriterView = class {
  /**
   * @param {Writer.State<Layout>} state
   */
  constructor(state) {
    this.state = state;
  }
  get writer() {
    return this.state.writer;
  }
  get settings() {
    return this.state.config;
  }
  /**
   * @param {Uint8Array} bytes
   * @returns {Promise<API.View<Layout>>}
   */
  write(bytes3) {
    return write7(this, bytes3);
  }
  /**
   * @param {API.CloseOptions} [options]
   * @returns {Promise<UnixFS.FileLink>}
   */
  close(options) {
    return close5(this, options);
  }
};

// node_modules/@ipld/unixfs/src/directory.js
var defaults3 = defaults2;
var create10 = ({ writer, settings = defaults3(), metadata = {} }) => new DirectoryWriter({
  writer,
  metadata,
  settings,
  entries: /* @__PURE__ */ new Map(),
  closed: false
});
var set2 = (view7, name15, link6, { overwrite = false } = {}) => {
  const writable = asWritable(view7.state);
  if (name15.includes("/")) {
    throw new Error(
      `Directory entry name "${name15}" contains forbidden "/" character`
    );
  }
  if (!overwrite && writable.entries.has(name15)) {
    throw new Error(`Directory already contains entry with name "${name15}"`);
  } else {
    writable.entries.set(name15, link6);
    return view7;
  }
};
var remove7 = (view7, name15) => {
  const writer = asWritable(view7.state);
  writer.entries.delete(name15);
  return view7;
};
var asWritable = (writer) => {
  if (!writer.closed) {
    return writer;
  } else {
    throw new Error(
      `Can not change written directory, but you can .fork() and make changes to it`
    );
  }
};
var close6 = async (view7, { closeWriter = false, releaseLock = false } = {}) => {
  const { writer, settings, metadata } = asWritable(view7.state);
  view7.state.closed = true;
  const entries3 = [...links(view7)];
  const node = createFlatDirectory(entries3, metadata);
  const bytes3 = encodeDirectory(node);
  const digest4 = await settings.hasher.digest(bytes3);
  const cid = settings.linker.createLink(code17, digest4);
  if ((writer.desiredSize || 0) <= 0) {
    await writer.ready;
  }
  writer.write({ cid, bytes: bytes3 });
  if (closeWriter) {
    await writer.close();
  } else if (releaseLock) {
    writer.releaseLock();
  }
  return {
    cid,
    dagByteLength: cumulativeDagByteLength(bytes3, entries3)
  };
};
var links = function* ({ state }) {
  for (const [name15, { dagByteLength, cid }] of state.entries) {
    yield (
      /** @type {UnixFS.DirectoryEntryLink} */
      {
        name: name15,
        dagByteLength,
        cid
      }
    );
  }
};
var fork2 = ({ state }, {
  writer = state.writer,
  metadata = state.metadata,
  settings = state.settings
} = {}) => new DirectoryWriter({
  writer,
  metadata,
  settings,
  entries: new Map(state.entries.entries()),
  closed: false
});
var DirectoryWriter = class {
  /**
   * @param {API.State<Layout>} state
   */
  constructor(state) {
    this.state = state;
  }
  get writer() {
    return this.state.writer;
  }
  get settings() {
    return this.state.settings;
  }
  links() {
    return links(this);
  }
  /**
   * @param {string} name
   * @param {UnixFS.FileLink | UnixFS.DirectoryLink} link
   * @param {API.WriteOptions} [options]
   */
  set(name15, link6, options) {
    return set2(this, name15, link6, options);
  }
  /**
   * @param {string} name
   */
  remove(name15) {
    return remove7(this, name15);
  }
  /**
   * @template L
   * @param {Partial<API.Options<L>>} [options]
   * @returns {API.View<Layout|L>}
   */
  fork(options) {
    return fork2(this, options);
  }
  /**
   * @param {API.CloseOptions} [options]
   * @returns {Promise<UnixFS.DirectoryLink>}
   */
  close(options) {
    return close6(this, options);
  }
  entries() {
    return this.state.entries.entries();
  }
  /**
   * @param {string} name
   */
  has(name15) {
    return this.state.entries.has(name15);
  }
  get size() {
    return this.state.entries.size;
  }
};

// node_modules/@perma/map/src/bitfield/Uint32.js
var Uint32_exports = {};
__export(Uint32_exports, {
  API: () => api_exports7,
  and: () => and4,
  bitCount: () => bitCount,
  empty: () => empty4,
  from: () => from14,
  fromBytes: () => fromBytes4,
  get: () => get13,
  or: () => or9,
  popcount: () => popcount,
  set: () => set3,
  size: () => size3,
  toBytes: () => toBytes2,
  unset: () => unset
});

// node_modules/@perma/map/src/bitfield/api.js
var api_exports7 = {};

// node_modules/@perma/map/src/bitfield/Uint32.js
var empty4 = (size5 = 32) => {
  if (size5 !== 32) {
    throw new Error(`Uint32 BitField does not support size: ${size5}`);
  }
  return 0;
};
var from14 = (bits, size5) => {
  let bitfield = empty4(size5);
  for (const bit of bits) {
    bitfield = set3(bitfield, bit);
  }
  return bitfield;
};
var size3 = (_bitField) => 32;
var mask = (bitField2, index3) => bitField2 >>> index3 & 31;
var offset = (bitField2, index3) => 1 << mask(bitField2, index3);
var popcount = (bitField2, index3 = 31) => bitCount(bitField2 & offset(index3, 0) - 1);
var set3 = (bitField2, index3) => bitField2 | 1 << index3;
var unset = (bitField2, index3) => bitField2 & (255 ^ 1 << index3);
var get13 = (bitField2, index3) => (bitField2 >> index3 & 1) !== 0;
var bitCount = (bitField2) => {
  const n1 = bitField2 - (bitField2 >> 1 & 1431655765);
  const n2 = (n1 & 858993459) + (n1 >> 2 & 858993459);
  const n3 = (n2 + (n2 >> 4) & 252645135) * 16843009;
  return n3 >> 24;
};
var and4 = (left, right) => left & right;
var or9 = (left, right) => left | right;
var toBytes2 = (bitField2) => Uint8Array.of(
  bitField2 >> 24 & 255,
  bitField2 >> 16 & 255,
  bitField2 >> 8 & 255,
  bitField2 & 255
);
var fromBytes4 = (bytes3) => {
  if (bytes3.length !== 4) {
    throw new Error(`Expected 4 bytes instead got ${bytes3.length}`);
  }
  return (bytes3[0] << 24) + (bytes3[1] << 16) + (bytes3[2] << 8) + bytes3[3];
};

// node_modules/@perma/map/src/path/Uint32.js
var import_murmurhash3js_revisited = __toESM(require_murmurhash3js_revisited(), 1);
var utf8 = new TextEncoder();
var hash32 = import_murmurhash3js_revisited.default.x64.hash126;
var configure2 = ({ bitWidth: bitWidth2 = 5, hash = hash32 }) => {
  const hashSize = 4;
  if (bitWidth2 > hashSize * 8) {
    throw new RangeError(
      `Can not use bitWidth ${bitWidth2} which exceeds the hashSize ${hashSize}`
    );
  }
  if (hashSize * 8 > 32) {
    throw new RangeError(
      `Can not use hashSize ${hashSize} as it can not be encoded in Uint32`
    );
  }
  const mask2 = 4294967295 >>> 32 - bitWidth2;
  const at2 = (path, depth) => path >>> depth * bitWidth2 & mask2;
  const from18 = (key) => hash(utf8.encode(key));
  return { at: at2, from: from18, size: Math.ceil(hashSize * 8 / bitWidth2) };
};

// node_modules/@perma/map/src/node.js
var BitmapIndexedNode = class {
  /**
   * @param {API.Edit|null} edit
   * @param {ReturnType<C['BitField']['empty']>} datamap
   * @param {ReturnType<C['BitField']['empty']>} nodemap
   * @param {API.Children<T, K, C>} children
   * @param {C} config
   */
  constructor(edit, datamap, nodemap, children, config2) {
    this.edit = edit;
    this.config = config2;
    this.datamap = datamap;
    this.nodemap = nodemap;
    this.children = children;
  }
  get nodeArity() {
    return this.config.BitField.popcount(this.nodemap);
  }
  get dataArity() {
    return this.config.BitField.popcount(this.datamap);
  }
  /**
   * @returns {API.BitmapIndexedNode<T, K, C>}
   */
  /* c8 ignore next 3 */
  empty() {
    return create11(this.config);
  }
  /**
   * @template X
   * @param {API.Uint32} depth
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K} key
   * @param {X} notFound
   * @returns {T|X}
   */
  lookup(depth, path, key, notFound2) {
    return lookup(this, depth, path, key, notFound2);
  }
  /**
   * @template {string} R
   * @param {API.Edit|null} edit
   * @param {API.Uint32} depth
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K|R} key
   * @param {T} value
   * @param {{value:boolean}} addedLeaf
   * @returns {API.BitmapIndexedNode<T, K | R, C>}
   */
  associate(edit, depth, path, key, value, addedLeaf) {
    return associate(this, edit, depth, path, key, value, addedLeaf);
  }
  /**
   * @param {API.Edit|null} edit
   * @param {API.Uint32} depth
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K} key
   * @param {{value:boolean}} removedLeaf
   * @returns {API.BitmapIndexedNode<T, K, C>}
   */
  dissociate(edit, depth, path, key, removedLeaf) {
    return dissociate(this, edit, depth, path, key, removedLeaf);
  }
  /**
   * @param {API.Edit|null} edit
   * @returns {API.BitmapIndexedNode<T, K, C>}
   */
  fork(edit = null) {
    return fork3(this, edit);
  }
  /**
   * @returns {IterableIterator<[K, T]>}
   */
  entries() {
    return entries2(this);
  }
  /**
   * @returns {IterableIterator<K>}
   */
  keys() {
    return keys(this);
  }
  /**
   * @returns {IterableIterator<T>}
   */
  values() {
    return values(this);
  }
};
var HashCollisionNode = class {
  /**
   * @param {API.Edit|null} edit
   * @param {number} count
   * @param {API.CollisionEntries<T, K>} children
   * @param {C} config
   */
  /* c8 ignore next 12 */
  constructor(edit, count, children, config2) {
    this.edit = edit;
    this.count = count;
    this.children = children;
    this.config = config2;
  }
  get nodeArity() {
    return (
      /** @type {0} */
      0
    );
  }
  get dataArity() {
    return this.count;
  }
  /**
   * @template X
   * @param {API.Uint32} _shift
   * @param {unknown} _path
   * @param {K} key
   * @param {X} notFound
   * @returns {T|X}
   */
  /* c8 ignore next 3 */
  lookup(_shift, _path, key, notFound2) {
    return lookupCollision(this, key, notFound2);
  }
  /**
   * @template {string} R
   * @param {API.Edit|null} edit
   * @param {API.Uint32} _shift
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K|R} key
   * @param {T} value
   * @param {{value:boolean}} addedLeaf
   * @returns {API.HashCollisionNode<T, K | R, C>}
   */
  /* c8 ignore next 3 */
  associate(edit, _shift, path, key, value, addedLeaf) {
    return associateCollision(this, edit, path, key, value, addedLeaf);
  }
  /**
   * @param {API.Edit|null} edit
   * @param {API.Uint32} _shift
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K} key
   * @param {{value:boolean}} removedLeaf
   * @returns {API.Node<T, K, C>}
   */
  /* c8 ignore next 3 */
  dissociate(edit, _shift, path, key, removedLeaf) {
    return dissociateCollision(this, edit, path, key, removedLeaf);
  }
  /**
   * @param {API.Edit|null} edit
   * @returns {this}
   */
  /* c8 ignore next 3 */
  fork(edit = null) {
    return (
      /** @type {this} */
      forkCollision(this, edit)
    );
  }
  /**
   * @returns {IterableIterator<[K, T]>}
   */
  /* c8 ignore next 3 */
  entries() {
    return entries2(this);
  }
  /**
   * @returns {IterableIterator<K>}
   */
  /* c8 ignore next 3 */
  keys() {
    return keys(this);
  }
  /**
   * @returns {IterableIterator<T>}
   */
  /* c8 ignore next 3 */
  values() {
    return values(this);
  }
};
var lookupCollision = (node, name15, notFound2) => {
  const { children: entries3, count } = node;
  const n = findHashCollisionNodeIndex(entries3, count, name15);
  return entries3[n] === name15 ? (
    /** @type {T} */
    entries3[n + 1]
  ) : notFound2;
};
var associateCollision = (node, edit, key, name15, value, addedLeaf) => {
  const { children, count } = node;
  const index3 = findHashCollisionNodeIndex(children, count, name15);
  if (children[index3] !== name15) {
    const newNode = node.fork(edit);
    addedLeaf.value = true;
    newNode.count += 1;
    newNode.children.splice(index3, key, value);
    return newNode;
  } else if (children[index3 + 1] !== value) {
    const newNode = node.fork(edit);
    newNode.children[index3 + 1] = value;
    return newNode;
  } else {
    return node;
  }
};
var dissociateCollision = (node, edit, hash, name15, removedLeaf) => {
  const { children: entries3, count, config: config2 } = node;
  const index3 = findHashCollisionNodeIndex(entries3, count, name15);
  if (entries3[index3] !== name15) {
    return node;
  } else {
    removedLeaf.value = true;
    if (count === 2) {
      const offset2 = index3 === 0 ? 2 : 0;
      return (
        /** @type {API.BitmapIndexedNode<T, K, C>} */
        associate(
          create11(config2),
          edit,
          0,
          hash,
          /** @type {K} */
          entries3[offset2],
          /** @type {T} */
          entries3[offset2 + 1],
          removedLeaf
        )
      );
    } else {
      const newNode = node.fork(edit);
      newNode.children.splice(index3, 2);
      newNode.count -= 1;
      return newNode;
    }
  }
};
var forkCollision = (node, edit = null) => {
  if (canEdit(node.edit, edit)) {
    return node;
  } else {
    return new HashCollisionNode(
      edit,
      node.count,
      /** @type {API.CollisionEntries<T, K>} */
      node.children.slice(),
      node.config
    );
  }
};
var findHashCollisionNodeIndex = (entries3, count, key) => {
  let index3 = 0;
  while (index3 < count && entries3[index3] > key) {
    index3 += 2;
  }
  return index3;
};
var create11 = (config2, edit = null) => new BitmapIndexedNode(
  edit,
  config2.BitField.empty(Math.pow(2, config2.bitWidth)),
  config2.BitField.empty(Math.pow(2, config2.bitWidth)),
  /** @type {API.Children<T, K, C>} */
  [],
  config2
);
var get14 = (node, key, notFound2) => lookup(node, 0, node.config.Path.from(key), key, notFound2);
var lookup = (node, depth, path, key, notFound2) => {
  const { datamap, nodemap, config: config2 } = node;
  const { Path, BitField } = config2;
  const offset2 = Path.at(path, depth);
  if (BitField.get(datamap, offset2)) {
    const index3 = BitField.popcount(datamap, offset2);
    if (keyAt(node, index3) === key) {
      return valueAt(node, index3);
    } else {
      return notFound2;
    }
  } else if (BitField.get(nodemap, offset2)) {
    const child = resolveNode(node, offset2);
    return child.lookup(depth + 1, path, key, notFound2);
  } else {
    return notFound2;
  }
};
var set4 = (node, edit, key, value, addedLeaf) => associate(node, edit, 0, node.config.Path.from(key), key, value, addedLeaf);
var associate = (node, edit, depth, path, key, value, addedLeaf) => {
  const { datamap, nodemap, config: config2 } = node;
  const { Path, BitField } = config2;
  const offset2 = Path.at(path, depth);
  if (BitField.get(datamap, offset2)) {
    const index3 = BitField.popcount(datamap, offset2);
    const found = keyAt(node, index3);
    if (key === found) {
      return valueAt(node, index3) === value ? node : forkAndSet(node, edit, index3, value);
    } else {
      const branch = mergeTwoLeaves(
        config2,
        edit,
        depth + 1,
        Path.from(found),
        found,
        valueAt(node, index3),
        path,
        key,
        value
      );
      addedLeaf.value = true;
      return migrateLeafToBranch(node, edit, offset2, branch);
    }
  } else if (BitField.get(nodemap, offset2)) {
    const child = resolveNode(node, offset2);
    const newChild = child.associate(
      edit,
      depth + 1,
      path,
      key,
      value,
      addedLeaf
    );
    if (child === newChild) {
      return node;
    } else {
      return copyAndSetChild(node, edit, offset2, newChild);
    }
  } else {
    const index3 = BitField.popcount(datamap, offset2);
    addedLeaf.value = true;
    const newNode = node.fork(edit);
    newNode.datamap = BitField.set(datamap, offset2);
    newNode.children.splice(keyPosition(index3), 0, key, value);
    return newNode;
  }
};
var remove8 = (node, edit, key, removedLeaf) => dissociate(node, edit, 0, node.config.Path.from(key), key, removedLeaf);
var dissociate = (source, edit, depth, path, key, removedLeaf) => {
  const { datamap, nodemap, config: config2 } = source;
  const { BitField, Path } = config2;
  const offset2 = Path.at(path, depth);
  if (BitField.get(datamap, offset2)) {
    const index3 = BitField.popcount(datamap, offset2);
    if (key === keyAt(source, index3)) {
      removedLeaf.value = true;
      const node = fork3(source, edit);
      node.datamap = BitField.unset(source.datamap, offset2);
      node.children.splice(keyPosition(index3), 2);
      return node;
    } else {
      return source;
    }
  } else if (BitField.get(nodemap, offset2)) {
    const node = resolveNode(source, offset2);
    const child = node.dissociate(edit, depth + 1, path, key, removedLeaf);
    if (hasSingleLeaf(child)) {
      return hasSingleNode(source) ? child : inlineChild(source, edit, offset2, child);
    } else if (node === child) {
      return source;
    } else {
      return copyAndSetChild(source, edit, offset2, child);
    }
  } else {
    return source;
  }
};
var entries2 = function* ({ children }) {
  let offset2 = 0;
  const count = children.length;
  while (offset2 < count) {
    const key = children[offset2];
    if (typeof key === "string") {
      offset2 += 1;
      const value = children[offset2];
      yield (
        /** @type {[K, T]} */
        [key, value]
      );
      offset2 += 1;
    } else {
      break;
    }
  }
  while (offset2 < count) {
    const node = (
      /** @type {API.BitmapIndexedNode<T, K, C>} */
      children[offset2]
    );
    yield* node.entries();
    offset2 += 1;
  }
};
var fork3 = (node, edit) => {
  if (canEdit(node.edit, edit)) {
    return node;
  } else {
    const newNode = new BitmapIndexedNode(
      edit,
      node.datamap,
      node.nodemap,
      node.children.slice(),
      node.config
    );
    return newNode;
  }
};
var keys = function* ({ children }) {
  let offset2 = 0;
  const count = children.length;
  while (offset2 < count) {
    const key = children[offset2];
    if (typeof key === "string") {
      yield (
        /** @type {K} */
        key
      );
      offset2 += 2;
    } else {
      break;
    }
  }
  while (offset2 < count) {
    const node = (
      /** @type {API.BitmapIndexedNode<T, K>} */
      children[offset2]
    );
    yield* node.keys();
    offset2 += 1;
  }
};
var values = function* ({ children }) {
  let offset2 = 0;
  const count = children.length;
  while (offset2 < count) {
    const key = children[offset2];
    if (typeof key === "string") {
      offset2 += 1;
      yield (
        /** @type {T} */
        children[offset2]
      );
      offset2 += 1;
    } else {
      break;
    }
  }
  while (offset2 < count) {
    const node = (
      /** @type {API.BitmapIndexedNode<T, K>} */
      children[offset2]
    );
    yield* node.values();
    offset2 += 1;
  }
};
var forkAndSet = (node, edit, offset2, value) => {
  const newNode = node.fork(edit);
  newNode.children[valuePosition(offset2)] = value;
  return newNode;
};
var inlineChild = (source, edit, offset2, child) => {
  const { datamap, nodemap, config: config2 } = source;
  const { BitField } = config2;
  const node = fork3(source, edit);
  node.children.splice(nodePosition(source, offset2), 1);
  node.children.splice(
    keyPosition(BitField.popcount(datamap, offset2)),
    0,
    child.children[0],
    child.children[1]
  );
  node.datamap = BitField.set(datamap, offset2);
  node.nodemap = BitField.unset(nodemap, offset2);
  return node;
};
var copyAndSetChild = (node, edit, offset2, child) => {
  const newNode = fork3(node, edit);
  newNode.children[nodePosition(node, offset2)] = child;
  return newNode;
};
var migrateLeafToBranch = (source, edit, offset2, branch) => {
  const { nodemap, datamap, config: config2 } = source;
  const { BitField } = config2;
  const index3 = BitField.popcount(datamap, offset2);
  const oldId = keyPosition(index3);
  const newId = nodePosition(source, offset2);
  const node = fork3(source, edit);
  node.datamap = BitField.unset(datamap, offset2);
  node.children.splice(oldId, 2);
  node.nodemap = BitField.set(nodemap, offset2);
  node.children.splice(newId - 1, 0, branch);
  return node;
};
var mergeTwoLeaves = (config2, edit, depth, oldPath, oldKey, oldValue, newPath, newKey, newValue) => {
  const { BitField, Path } = config2;
  if (Path.size < depth) {
    return new HashCollisionNode(
      edit,
      2,
      [oldKey, oldValue, newKey, newValue],
      config2
    );
  } else {
    const oldOffset = Path.at(oldPath, depth);
    const newOffset = Path.at(newPath, depth);
    if (oldOffset === newOffset) {
      return new BitmapIndexedNode(
        edit,
        BitField.empty(Math.pow(2, config2.bitWidth)),
        BitField.from([oldOffset], Math.pow(2, config2.bitWidth)),
        [
          mergeTwoLeaves(
            config2,
            edit,
            depth + 1,
            oldPath,
            oldKey,
            oldValue,
            newPath,
            newKey,
            newValue
          )
        ],
        config2
      );
    } else {
      return new BitmapIndexedNode(
        edit,
        BitField.from([oldOffset, newOffset], Math.pow(2, config2.bitWidth)),
        BitField.empty(Math.pow(2, config2.bitWidth)),
        /** @type {API.Children<T, K, C>} */
        // We insert child with a lower index first so that we can derive it's
        // index on access via popcount
        oldOffset < newOffset ? [oldKey, oldValue, newKey, newValue] : [newKey, newValue, oldKey, oldValue],
        config2
      );
    }
  }
};
var keyAt = ({ children }, index3) => (
  /** @type {K} */
  children[keyPosition(index3)]
);
var keyPosition = (index3) => index3 * 2;
var valueAt = ({ children }, index3) => (
  /** @type {T} */
  children[valuePosition(index3)]
);
var valuePosition = (index3) => index3 * 2 + 1;
var resolveNode = (node, offset2) => (
  /** @type {API.BitmapIndexedNode<T, K, C>|API.HashCollisionNode<T, K, C>} */
  node.children[nodePosition(node, offset2)]
);
var nodePosition = ({ children, nodemap, config: config2 }, offset2) => children.length - 1 - config2.BitField.popcount(nodemap, offset2);
var canEdit = (owner, editor) => owner != null && owner === editor;
var hasSingleLeaf = (node) => node.nodeArity === 0 && node.dataArity === 1;
var hasSingleNode = ({ config: { BitField }, datamap, nodemap }) => BitField.popcount(datamap) === 0 && BitField.popcount(nodemap) === 1;

// node_modules/@multiformats/murmur3/src/index.js
var import_murmurhash3js_revisited2 = __toESM(require_murmurhash3js_revisited());
function fromNumberTo32BitBuf(number3) {
  const bytes3 = new Array(4);
  for (let i = 0; i < 4; i++) {
    bytes3[i] = number3 & 255;
    number3 = number3 >> 8;
  }
  return new Uint8Array(bytes3);
}
var murmur332 = from2({
  name: "murmur3-32",
  code: 35,
  encode: (input11) => fromNumberTo32BitBuf(import_murmurhash3js_revisited2.default.x86.hash32(input11))
});
var murmur3128 = from2({
  name: "murmur3-128",
  code: 34,
  encode: (input11) => bytes_exports2.fromHex(import_murmurhash3js_revisited2.default.x64.hash128(input11))
});
var murmur364 = from2({
  name: "murmur3-x64-64",
  code: 34,
  encode: (input11) => bytes_exports2.fromHex(import_murmurhash3js_revisited2.default.x64.hash128(input11)).subarray(0, 8)
});

// node_modules/@perma/map/src/path/Uint8Array.js
var utf82 = new TextEncoder();
var hash64 = (bytes3) => (
  /** @type {Uint8Array} */
  murmur364.encode(bytes3)
);
var configure3 = ({ bitWidth: bitWidth2 = 8, hash = hash64 } = {}) => {
  const hashSize = hash(new Uint8Array()).byteLength;
  const at2 = (path, depth) => {
    const offset2 = depth * bitWidth2;
    if (offset2 > hashSize) {
      throw new RangeError(`Out of bounds`);
    }
    return toInt(path, offset2, bitWidth2);
  };
  const from18 = (key) => hash(utf82.encode(key));
  return { from: from18, at: at2, size: Math.ceil(hashSize * 8 / bitWidth2) };
};
var toInt = (bytes3, offset2, count) => {
  let byteOffset = offset2 / 8 | 0;
  let bitOffset = offset2 % 8;
  let desired = count;
  let bits = 0;
  while (desired > 0 && byteOffset < bytes3.byteLength) {
    const byte = bytes3[byteOffset];
    const available = 8 - bitOffset;
    const taking = available < desired ? available : desired;
    const bitsLeft = 8 - bitOffset - taking;
    const mask2 = 255 >> bitOffset;
    const value = (mask2 & byte) >> bitsLeft;
    bits = (bits << taking) + value;
    desired -= taking;
    byteOffset++;
    bitOffset = 0;
  }
  return bits;
};

// node_modules/@perma/map/src/bitfield/Uint8Array.js
var Uint8Array_exports = {};
__export(Uint8Array_exports, {
  API: () => api_exports7,
  and: () => and5,
  empty: () => empty5,
  from: () => from15,
  fromBytes: () => fromBytes5,
  get: () => get15,
  or: () => or10,
  popcount: () => popcount2,
  set: () => set5,
  size: () => size4,
  toBytes: () => toBytes3,
  unset: () => unset2
});
var empty5 = (size5 = 256) => {
  if (size5 % 8 !== 0) {
    throw new Error(`Must be multiple of 8`);
  }
  return new Uint8Array(size5 / 8);
};
var from15 = (bits, size5) => {
  let bitfield = empty5(size5);
  for (const index3 of bits) {
    const { byte, byteOffset, bitOffset } = at(bitfield, index3);
    bitfield[byteOffset] = byte | 1 << bitOffset;
  }
  return bitfield;
};
var size4 = (bitfield) => bitfield.byteLength * 8;
var at = (bitfield, index3) => {
  const byteOffset = bitfield.byteLength - 1 - (index3 / 8 | 0);
  const bitOffset = index3 % 8;
  const byte = bitfield[byteOffset];
  return { byte, byteOffset, bitOffset };
};
var setByte = (bytes3, index3, byte) => {
  if (bytes3[index3] !== byte) {
    const result = bytes3.slice(0);
    result[index3] = byte;
    return result;
  }
  return bytes3;
};
var set5 = (bitfield, index3) => {
  const { byte, byteOffset, bitOffset } = at(bitfield, index3);
  return setByte(bitfield, byteOffset, byte | 1 << bitOffset);
};
var unset2 = (bitfield, index3) => {
  const { byte, byteOffset, bitOffset } = at(bitfield, index3);
  return setByte(bitfield, byteOffset, byte & (255 ^ 1 << bitOffset));
};
var get15 = (bitfield, index3) => {
  var { byte, bitOffset } = at(bitfield, index3);
  return (byte >> bitOffset & 1) !== 0;
};
var toBytes3 = (bitfield) => bitfield;
var fromBytes5 = (bytes3) => bytes3;
var popcount2 = (bitfield, index3 = bitfield.byteLength * 8) => {
  const { byteOffset, bitOffset, byte } = at(bitfield, index3);
  let count = popcount(byte, bitOffset);
  let offset2 = bitfield.byteLength - 1;
  while (offset2 > byteOffset) {
    const byte2 = bitfield[offset2];
    count += bitCount(byte2);
    offset2--;
  }
  return count;
};
var or10 = (left, right) => {
  const result = left.slice();
  let offset2 = 0;
  while (offset2 < left.length) {
    result[offset2] |= right[offset2];
    offset2++;
  }
  return result;
};
var and5 = (left, right) => {
  const result = left.slice();
  let offset2 = 0;
  while (offset2 < left.length) {
    result[offset2] &= right[offset2];
    offset2++;
  }
  return result;
};

// node_modules/@perma/map/src/lib.js
var NOT_FOUND = new RangeError("Not Found");
var configure4 = ({
  bitWidth: bitWidth2 = 5,
  /* c8 ignore next 4 */
  BitField = bitWidth2 === 5 ? Uint32_exports : Uint8Array_exports,
  Path = bitWidth2 === 5 ? configure2({ bitWidth: bitWidth2 }) : configure3({ bitWidth: bitWidth2 })
} = {}) => (
  /** @type {C} */
  { bitWidth: bitWidth2, BitField, Path }
);
var from16 = (entries3, options) => {
  const node = (
    /** @type {API.HashMapBuilder<V, K, C>} */
    builder(options)
  );
  for (const [key, value] of entries3) {
    node.set(key, value);
  }
  return node.build();
};
var has2 = (hamt, key) => get14(hamt.root, key, NOT_FOUND) !== NOT_FOUND;
var get16 = (hamt, key, notFound2 = (
  /** @type {U} */
  void 0
)) => get14(hamt.root, key, notFound2);
var builder = (options) => {
  const edit = {};
  const config2 = configure4(options);
  return new HashMapBuilder(
    edit,
    0,
    create11(config2, edit),
    config2
  );
};
var PersistentHashMap = class _PersistentHashMap {
  /**
   *
   * @param {number} count
   * @param {API.BitmapIndexedNode<T, K, C>} root
   * @param {C} config
   */
  constructor(count = 0, root2, config2) {
    this.count = count;
    this.root = root2;
    this.config = config2;
  }
  get size() {
    return this.count;
  }
  clone() {
    return new _PersistentHashMap(this.count, this.root, this.config);
  }
  /**
   * @returns {API.PersistentHashMap<T, K, C>}
   */
  empty() {
    return new _PersistentHashMap(
      0,
      create11(this.config, null),
      this.config
    );
  }
  /**
   * @param {K} key
   * @returns {boolean}
   */
  has(key) {
    return has2(this, key);
  }
  /**
   * @param {K} key
   * @returns {T|undefined}
   */
  get(key) {
    return get14(this.root, key, void 0);
  }
  /**
   * @template {string} R
   * @param {R} key
   * @param {T} value
   * @returns {PersistentHashMap<T, K|R, C>}
   */
  set(key, value) {
    const addedLeaf = { value: false };
    const root2 = set4(this.root, null, key, value, addedLeaf);
    if (root2 === this.root) {
      return this;
    } else {
      return new _PersistentHashMap(
        addedLeaf.value ? this.count + 1 : this.count,
        root2,
        this.config
      );
    }
  }
  /**
   * @param {K} key
   */
  delete(key) {
    const root2 = remove8(this.root, null, key, { value: false });
    if (root2 === this.root) {
      return this;
    } else {
      return new _PersistentHashMap(this.count - 1, root2, this.config);
    }
  }
  /* c8 ignore next 3 */
  get bitField() {
    return this.config.BitField.or(this.root.datamap, this.root.nodemap);
  }
  [Symbol.iterator]() {
    return this.entries();
  }
  entries() {
    return this.root.entries();
  }
  keys() {
    return this.root.keys();
  }
  values() {
    return this.root.values();
  }
  /**
   * @returns {API.HashMapBuilder<T, K, C>}
   */
  createBuilder() {
    return new HashMapBuilder({}, this.count, this.root, this.config);
  }
};
var HashMapBuilder = class {
  /**
   * @param {API.Edit} edit
   * @param {number} count
   * @param {API.BitmapIndexedNode<T, K, C>} root
   * @param {C} config
   */
  constructor(edit, count, root2, config2) {
    this.edit = edit;
    this.count = count;
    this.root = root2;
    this.config = config2;
  }
  get size() {
    if (this.edit) {
      return this.count;
    } else {
      throw new Error(`.size was accessed on the finalized builder`);
    }
  }
  /**
   * @template {string} R
   * @param {R} key
   * @param {T} value
   * @returns {HashMapBuilder<T, K|R, C>}
   */
  set(key, value) {
    if (this.edit) {
      const addedLeaf = { value: false };
      const root2 = set4(this.root, this.edit, key, value, addedLeaf);
      if (this.root !== root2) {
        this.root = /** @type {API.BitmapIndexedNode<T, K, C>} */
        root2;
      }
      if (addedLeaf.value) {
        this.count += 1;
      }
      return this;
    } else {
      throw new Error(`.set was called on the finalized builder`);
    }
  }
  /**
   * @param {K} key
   */
  delete(key) {
    if (this.edit) {
      if (this.count === 0) {
        return this;
      }
      const removedLeaf = { value: false };
      const root2 = remove8(this.root, this.edit, key, removedLeaf);
      if (root2 !== this.root) {
        this.root = root2;
      }
      if (removedLeaf.value) {
        this.count -= 1;
      }
      return this;
    } else {
      throw new Error(`.delete was called on the finalized builder`);
    }
  }
  build() {
    if (this.edit) {
      this.edit = null;
      return new PersistentHashMap(this.count, this.root, this.config);
    } else {
      throw new Error(`.build was called on the finalized builder`);
    }
  }
};

// node_modules/@perma/map/src/path/InfiniteUint8Array.js
var utf83 = new TextEncoder();
var hash642 = (bytes3) => (
  /** @type {Uint8Array} */
  murmur364.encode(bytes3)
);
var configure5 = ({ bitWidth: bitWidth2 = 8, hash = hash642 }) => {
  const hashSize = hash(new Uint8Array()).byteLength;
  const options = { bitWidth: bitWidth2, hash, hashSize };
  const at2 = (path, depth) => read7(path, depth, options);
  const from18 = (key) => utf83.encode(key);
  return { at: at2, from: from18, size: Infinity };
};
var read7 = (key, depth = 0, { bitWidth: bitWidth2 = 8, hash, hashSize }) => {
  const frameBitSize = hashSize * 8;
  let digest4 = 0;
  let bitCount2 = bitWidth2;
  let bitOffset = bitWidth2 * depth;
  while (bitCount2 > 0) {
    const frameOffset = bitOffset / frameBitSize >> 0;
    const frame = frameOffset === 0 ? hash(key) : hash(appendByte(key, frameOffset));
    const offset2 = frameBitSize <= bitOffset ? bitOffset % frameBitSize : bitOffset;
    const maxBits = frameBitSize - offset2;
    const count = maxBits < bitCount2 ? maxBits : bitCount2;
    digest4 = (digest4 << count) + toInt(frame, offset2, count);
    bitCount2 -= count;
    bitOffset += count;
  }
  return digest4;
};
var appendByte = (source, byte) => {
  const bytes3 = new Uint8Array(source.byteLength + 1).fill(
    byte,
    source.byteLength
  );
  bytes3.set(source);
  return bytes3;
};

// node_modules/@perma/map/src/unixfs.js
var bitWidth = 8;
var config = {
  bitWidth,
  Path: configure5({ bitWidth })
};
var tableSize = (hamt) => Math.pow(2, hamt.config.bitWidth);
var builder2 = (options = (
  /** @type {C} */
  config
)) => builder(options);
var from17 = (entries3, options = (
  /** @type {C} */
  config
)) => from16(entries3, options);
var bitField = ({ datamap, nodemap, config: { BitField } }) => withoutLeadingZeros(BitField.toBytes(BitField.or(datamap, nodemap)));
var withoutLeadingZeros = (bytes3) => {
  let offset2 = 0;
  while (offset2 < bytes3.byteLength) {
    if (bytes3[offset2] !== 0) {
      return bytes3.subarray(offset2);
    }
    offset2 += 1;
  }
  return bytes3.subarray(offset2);
};
var iterate3 = function* (root2) {
  const { config: config2, datamap, nodemap } = root2;
  const { BitField: bitfield } = config2;
  const size5 = bitfield.size(datamap);
  let bitOffset = 0;
  let dataCount = 0;
  while (bitOffset < size5) {
    const prefix2 = bitOffset.toString(16).toUpperCase().padStart(2, "0");
    if (bitfield.get(datamap, bitOffset)) {
      const key = keyAt(root2, dataCount);
      yield {
        prefix: prefix2,
        key,
        value: valueAt(root2, dataCount)
      };
      dataCount++;
    } else if (bitfield.get(nodemap, bitOffset)) {
      yield {
        prefix: prefix2,
        // UnixFS never contains hash collision nodes because it uses
        // inifinite hashes
        node: (
          /** @type {HAMT.BitmapIndexedNode<T, K, C>} */
          resolveNode(root2, bitOffset)
        )
      };
    }
    bitOffset++;
  }
};

// node_modules/multiformats/dist/src/block.js
function readonly({ enumerable = true, configurable = false } = {}) {
  return { enumerable, configurable, writable: false };
}
function* linksWithin(path, value) {
  if (value != null && typeof value === "object") {
    if (Array.isArray(value)) {
      for (const [index3, element] of value.entries()) {
        const elementPath = [...path, index3];
        const cid = CID.asCID(element);
        if (cid != null) {
          yield [elementPath.join("/"), cid];
        } else if (typeof element === "object") {
          yield* links2(element, elementPath);
        }
      }
    } else {
      const cid = CID.asCID(value);
      if (cid != null) {
        yield [path.join("/"), cid];
      } else {
        yield* links2(value, path);
      }
    }
  }
}
function* links2(source, base3) {
  if (source == null || source instanceof Uint8Array) {
    return;
  }
  const cid = CID.asCID(source);
  if (cid != null) {
    yield [base3.join("/"), cid];
  }
  for (const [key, value] of Object.entries(source)) {
    const path = [...base3, key];
    yield* linksWithin(path, value);
  }
}
function* treeWithin(path, value) {
  if (Array.isArray(value)) {
    for (const [index3, element] of value.entries()) {
      const elementPath = [...path, index3];
      yield elementPath.join("/");
      if (typeof element === "object" && CID.asCID(element) == null) {
        yield* tree(element, elementPath);
      }
    }
  } else {
    yield* tree(value, path);
  }
}
function* tree(source, base3) {
  if (source == null || typeof source !== "object") {
    return;
  }
  for (const [key, value] of Object.entries(source)) {
    const path = [...base3, key];
    yield path.join("/");
    if (value != null && !(value instanceof Uint8Array) && typeof value === "object" && CID.asCID(value) == null) {
      yield* treeWithin(path, value);
    }
  }
}
function get17(source, path) {
  let node = source;
  for (const [index3, key] of path.entries()) {
    node = node[key];
    if (node == null) {
      throw new Error(`Object has no property at ${path.slice(0, index3 + 1).map((part) => `[${JSON.stringify(part)}]`).join("")}`);
    }
    const cid = CID.asCID(node);
    if (cid != null) {
      return { value: cid, remaining: path.slice(index3 + 1).join("/") };
    }
  }
  return { value: node };
}
var Block = class {
  constructor({ cid, bytes: bytes3, value }) {
    __publicField(this, "cid");
    __publicField(this, "bytes");
    __publicField(this, "value");
    __publicField(this, "asBlock");
    if (cid == null || bytes3 == null || typeof value === "undefined") {
      throw new Error("Missing required argument");
    }
    this.cid = cid;
    this.bytes = bytes3;
    this.value = value;
    this.asBlock = this;
    Object.defineProperties(this, {
      cid: readonly(),
      bytes: readonly(),
      value: readonly(),
      asBlock: readonly()
    });
  }
  links() {
    return links2(this.value, []);
  }
  tree() {
    return tree(this.value, []);
  }
  get(path = "/") {
    return get17(this.value, path.split("/").filter(Boolean));
  }
};

// node_modules/@ipld/unixfs/src/sharded-directory.js
var defaults4 = defaults2;
var create12 = ({ writer, settings = defaults4(), metadata = {} }) => new HAMTDirectoryWriter({
  writer,
  metadata,
  settings,
  entries: new HashMap(),
  closed: false
});
var asWritable2 = (writer) => {
  if (!writer.closed) {
    return writer;
  } else {
    throw new Error("Can not change written HAMT directory, but you can .fork() and make changes to it");
  }
};
var close7 = async (view7, { closeWriter = false, releaseLock = false } = {}) => {
  const { writer, settings, metadata } = asWritable2(view7.state);
  view7.state.closed = true;
  const { entries: entries3 } = view7.state;
  if (!(entries3 instanceof HashMap)) {
    throw new Error(`not a HAMT: ${entries3}`);
  }
  const hamt = entries3.builder.build();
  const blocks = iterateBlocks(hamt, hamt.root, settings);
  let root2 = null;
  for await (const block of blocks) {
    root2 = block;
    if ((writer.desiredSize || 0) <= 0) {
      await writer.ready;
    }
    writer.write(block);
  }
  if (root2 == null) throw new Error("no root block yielded");
  if (closeWriter) {
    await writer.close();
  } else if (releaseLock) {
    writer.releaseLock();
  }
  return {
    cid: root2.cid,
    dagByteLength: cumulativeDagByteLength(root2.bytes, root2.value.entries)
  };
};
var iterateBlocks = async function* (hamt, node, settings) {
  const entries3 = [];
  for (const ent of iterate3(node)) {
    if ("key" in ent) {
      entries3.push(
        /** @type {UnixFS.DirectoryEntryLink} */
        {
          name: `${ent.prefix ?? ""}${ent.key ?? ""}`,
          dagByteLength: ent.value.dagByteLength,
          cid: ent.value.cid
        }
      );
    } else {
      let root2 = null;
      for await (const block of iterateBlocks(hamt, ent.node, settings)) {
        yield block;
        root2 = block;
      }
      if (root2 == null) throw new Error("no root block yielded");
      entries3.push(
        /** @type {UnixFS.ShardedDirectoryLink} */
        {
          name: ent.prefix,
          dagByteLength: cumulativeDagByteLength(root2.bytes, root2.value.entries),
          cid: root2.cid
        }
      );
    }
  }
  const shard = createDirectoryShard(
    entries3,
    bitField(node),
    tableSize(hamt),
    murmur364.code
  );
  yield await encodeHAMTShardBlock(shard, settings);
};
async function encodeHAMTShardBlock(shard, settings) {
  const bytes3 = encodeHAMTShard(shard);
  const hash = await settings.hasher.digest(bytes3);
  const cid = settings.linker.createLink(code16, hash);
  return new Block({ cid, bytes: bytes3, value: shard });
}
var fork4 = ({ state }, {
  writer = state.writer,
  metadata = state.metadata,
  settings = state.settings
} = {}) => new HAMTDirectoryWriter({
  writer,
  metadata,
  settings,
  entries: new HashMap(from17(state.entries.entries()).createBuilder()),
  closed: false
});
var HAMTDirectoryWriter = class {
  /**
   * @param {API.State<Layout>} state
   */
  constructor(state) {
    this.state = state;
  }
  get writer() {
    return this.state.writer;
  }
  get settings() {
    return this.state.settings;
  }
  /**
   * @param {string} name
   * @param {UnixFS.FileLink | UnixFS.DirectoryLink} link
   * @param {API.WriteOptions} [options]
   */
  set(name15, link6, options) {
    return set2(this, name15, link6, options);
  }
  /**
   * @param {string} name
   */
  remove(name15) {
    return remove7(this, name15);
  }
  /**
   * @template L
   * @param {Partial<API.Options<L>>} [options]
   * @returns {API.View<Layout|L>}
   */
  fork(options) {
    return fork4(this, options);
  }
  /**
   * @param {API.CloseOptions} [options]
   * @returns {Promise<UnixFS.DirectoryLink>}
   */
  close(options) {
    return close7(this, options);
  }
  entries() {
    return this.state.entries.entries();
  }
  /**
   * @param {string} name
   */
  has(name15) {
    return this.state.entries.has(name15);
  }
  get size() {
    return this.state.entries.size;
  }
};
var HashMap = class extends Map {
  /**
   * @param {UnixFSPermaMap.HashMapBuilder} [builder]
   */
  constructor(builder3 = builder2()) {
    super();
    this.builder = builder3;
  }
  clear() {
    this.builder = builder2();
  }
  /**
   * @param {string} key
   */
  delete(key) {
    const { root: root2 } = this.builder;
    this.builder.delete(key);
    return this.builder.root !== root2;
  }
  /**
   * @param {(value: API.EntryLink, key: string, map: Map<string, API.EntryLink>) => void} callbackfn
   * @param {any} [thisArg]
   */
  forEach(callbackfn, thisArg = this) {
    for (const [k, v] of this.builder.root.entries()) {
      callbackfn.call(thisArg, v, k, this);
    }
  }
  /**
   * @param {string} key
   */
  get(key) {
    return get16(this.builder, key);
  }
  /**
   * @param {string} key
   */
  has(key) {
    return has2(this.builder, key);
  }
  /**
   * @param {string} key 
   * @param {API.EntryLink} value 
   */
  set(key, value) {
    this.builder.set(key, value);
    return this;
  }
  get size() {
    return this.builder.size;
  }
  [Symbol.iterator]() {
    return this.builder.root.entries();
  }
  entries() {
    return this.builder.root.entries();
  }
  keys() {
    return this.builder.root.keys();
  }
  values() {
    return this.builder.root.values();
  }
};

// node_modules/@ipld/unixfs/src/lib.js
var createWriter3 = ({ writable, settings = defaults2() }) => new FileSystemWriter({
  writer: writable.getWriter(),
  settings
});
var close8 = async (view7, { releaseLock = true, closeWriter = true } = {}) => {
  if (closeWriter) {
    await view7.writer.close();
  } else if (releaseLock) {
    view7.writer.releaseLock();
  }
  return view7;
};
var FileSystemWriter = class {
  /**
   * @param {object} options
   * @param {API.BlockWriter} options.writer
   * @param {Partial<API.EncoderSettings<Layout>>} options.settings
   */
  constructor({ writer, settings }) {
    this.writer = writer;
    this.settings = configure(settings);
  }
  /**
   * @template [L=unknown]
   * @param {API.WriterOptions<L|Layout>} config
   */
  createFileWriter({ settings = this.settings, metadata } = {}) {
    return create9({
      writer: this.writer,
      settings,
      metadata
    });
  }
  /**
   * @template [L=unknown]
   * @param {API.WriterOptions<L|Layout>} config
   */
  createDirectoryWriter({ settings = this.settings, metadata } = {}) {
    return create10({
      writer: this.writer,
      settings,
      metadata
    });
  }
  /**
   * @param {API.CloseOptions} [options]
   */
  close(options) {
    return close8(this, options);
  }
};
var BLOCK_SIZE_LIMIT = 1048576;
var defaultCapacity = BLOCK_SIZE_LIMIT * 100;
var withCapacity = (byteLength = defaultCapacity) => ({
  highWaterMark: byteLength,
  size: (block) => block.bytes.length
});

// node_modules/@storacha/upload-client/dist/unixfs.js
var SHARD_THRESHOLD = 1e3;
var queuingStrategy = withCapacity();
var defaultSettings = configure({
  fileChunkEncoder: raw_exports,
  smallFileEncoder: raw_exports,
  chunker: withMaxChunkSize(1024 * 1024),
  fileLayout: withWidth(1024)
});
async function encodeFile2(blob4, options) {
  const readable = createFileEncoderStream(blob4, options);
  const blocks = await collect2(readable);
  return { cid: blocks.at(-1).cid, blocks };
}
function createFileEncoderStream(blob4, options) {
  const { readable, writable } = new TransformStream({}, queuingStrategy);
  const settings = (options == null ? void 0 : options.settings) ?? defaultSettings;
  const unixfsWriter = createWriter3({ writable, settings });
  const fileBuilder = new UnixFSFileBuilder("", blob4);
  void (async () => {
    await fileBuilder.finalize(unixfsWriter);
    await unixfsWriter.close();
  })();
  return readable;
}
var _file;
var UnixFSFileBuilder = class {
  /**
   * @param {string} name
   * @param {import('./types.js').BlobLike} file
   */
  constructor(name15, file) {
    __privateAdd(this, _file);
    this.name = name15;
    __privateSet(this, _file, file);
  }
  /** @param {import('@ipld/unixfs').View} writer */
  async finalize(writer) {
    const unixfsFileWriter = create9(writer);
    await __privateGet(this, _file).stream().pipeTo(new WritableStream({
      async write(chunk) {
        await unixfsFileWriter.write(chunk);
      }
    }));
    return await unixfsFileWriter.close();
  }
};
_file = new WeakMap();
var _options;
var UnixFSDirectoryBuilder = class {
  /**
   * @param {string} name
   * @param {import('./types.js').UnixFSDirectoryEncoderOptions} [options]
   */
  constructor(name15, options) {
    __privateAdd(this, _options);
    /** @type {Map<string, UnixFSFileBuilder | UnixFSDirectoryBuilder>} */
    __publicField(this, "entries", /* @__PURE__ */ new Map());
    this.name = name15;
    __privateSet(this, _options, options);
  }
  /** @param {import('@ipld/unixfs').View} writer */
  async finalize(writer) {
    var _a15;
    const dirWriter = this.entries.size <= SHARD_THRESHOLD ? create10(writer) : create12(writer);
    for (const [name15, entry] of this.entries) {
      const link6 = await entry.finalize(writer);
      if ((_a15 = __privateGet(this, _options)) == null ? void 0 : _a15.onDirectoryEntryLink) {
        __privateGet(this, _options).onDirectoryEntryLink({ name: entry.name, ...link6 });
      }
      dirWriter.set(name15, link6);
    }
    return await dirWriter.close();
  }
};
_options = new WeakMap();
async function encodeDirectory2(files, options) {
  const readable = createDirectoryEncoderStream(files, options);
  const blocks = await collect2(readable);
  return { cid: blocks.at(-1).cid, blocks };
}
function createDirectoryEncoderStream(files, options) {
  const rootDir = new UnixFSDirectoryBuilder("", options);
  for (const file of files) {
    const path = file.name.split("/");
    if (path[0] === "" || path[0] === ".") {
      path.shift();
    }
    let dir = rootDir;
    for (const [i, name15] of path.entries()) {
      if (i === path.length - 1) {
        dir.entries.set(name15, new UnixFSFileBuilder(path.join("/"), file));
        break;
      }
      let dirBuilder = dir.entries.get(name15);
      if (dirBuilder == null) {
        const dirName = dir === rootDir ? name15 : `${dir.name}/${name15}`;
        dirBuilder = new UnixFSDirectoryBuilder(dirName, options);
        dir.entries.set(name15, dirBuilder);
      }
      if (!(dirBuilder instanceof UnixFSDirectoryBuilder)) {
        throw new Error(`"${file.name}" cannot be a file and a directory`);
      }
      dir = dirBuilder;
    }
  }
  const { readable, writable } = new TransformStream({}, queuingStrategy);
  const settings = (options == null ? void 0 : options.settings) ?? defaultSettings;
  const unixfsWriter = createWriter3({ writable, settings });
  void (async () => {
    const link6 = await rootDir.finalize(unixfsWriter);
    if (options == null ? void 0 : options.onDirectoryEntryLink) {
      options.onDirectoryEntryLink({ name: "", ...link6 });
    }
    await unixfsWriter.close();
  })();
  return readable;
}
async function collect2(collectable) {
  const chunks = [];
  await collectable.pipeTo(new WritableStream({
    write(chunk) {
      chunks.push(chunk);
    }
  }));
  return chunks;
}

// node_modules/@ipld/car/src/decoder.js
async function readHeader2(reader, strictVersion) {
  const length2 = decodeVarint(await reader.upTo(8), reader);
  if (length2 === 0) {
    throw new Error("Invalid CAR header (zero length)");
  }
  const header = await reader.exactly(length2, true);
  const block = decode6(header);
  if (CarV1HeaderOrV2Pragma.toTyped(block) === void 0) {
    throw new Error("Invalid CAR header format");
  }
  if (block.version !== 1 && block.version !== 2 || strictVersion !== void 0 && block.version !== strictVersion) {
    throw new Error(`Invalid CAR version: ${block.version}${strictVersion !== void 0 ? ` (expected ${strictVersion})` : ""}`);
  }
  if (block.version === 1) {
    if (!Array.isArray(block.roots)) {
      throw new Error("Invalid CAR header format");
    }
    return block;
  }
  if (block.roots !== void 0) {
    throw new Error("Invalid CAR header format");
  }
  const v2Header = decodeV2Header(await reader.exactly(V2_HEADER_LENGTH, true));
  reader.seek(v2Header.dataOffset - reader.pos);
  const v1Header = await readHeader2(reader, 1);
  return Object.assign(v1Header, v2Header);
}
async function readCid2(reader) {
  const first = await reader.exactly(2, false);
  if (first[0] === CIDV0_BYTES.SHA2_256 && first[1] === CIDV0_BYTES.LENGTH) {
    const bytes4 = await reader.exactly(34, true);
    const multihash2 = decode5(bytes4);
    return CID.create(0, CIDV0_BYTES.DAG_PB, multihash2);
  }
  const version2 = decodeVarint(await reader.upTo(8), reader);
  if (version2 !== 1) {
    throw new Error(`Unexpected CID version (${version2})`);
  }
  const codec = decodeVarint(await reader.upTo(8), reader);
  const bytes3 = await reader.exactly(getMultihashLength(await reader.upTo(8)), true);
  const multihash = decode5(bytes3);
  return CID.create(version2, codec, multihash);
}
async function readBlockHead2(reader) {
  const start = reader.pos;
  let length2 = decodeVarint(await reader.upTo(8), reader);
  if (length2 === 0) {
    throw new Error("Invalid CAR section (zero length)");
  }
  length2 += reader.pos - start;
  const cid = await readCid2(reader);
  const blockLength2 = length2 - Number(reader.pos - start);
  return { cid, length: length2, blockLength: blockLength2 };
}
async function readBlock(reader) {
  const { cid, blockLength: blockLength2 } = await readBlockHead2(reader);
  const bytes3 = await reader.exactly(blockLength2, true);
  return { bytes: bytes3, cid };
}
async function readBlockIndex(reader) {
  const offset2 = reader.pos;
  const { cid, length: length2, blockLength: blockLength2 } = await readBlockHead2(reader);
  const index3 = { cid, length: length2, blockLength: blockLength2, offset: offset2, blockOffset: reader.pos };
  reader.seek(index3.blockLength);
  return index3;
}
function createDecoder(reader) {
  const headerPromise = (async () => {
    const header = await readHeader2(reader);
    if (header.version === 2) {
      const v1length = reader.pos - header.dataOffset;
      reader = limitReader2(reader, header.dataSize - v1length);
    }
    return header;
  })();
  return {
    header: () => headerPromise,
    async *blocks() {
      await headerPromise;
      while ((await reader.upTo(8)).length > 0) {
        yield await readBlock(reader);
      }
    },
    async *blocksIndex() {
      await headerPromise;
      while ((await reader.upTo(8)).length > 0) {
        yield await readBlockIndex(reader);
      }
    }
  };
}
function bytesReader2(bytes3) {
  let pos = 0;
  return {
    async upTo(length2) {
      const out = bytes3.subarray(pos, pos + Math.min(length2, bytes3.length - pos));
      return out;
    },
    async exactly(length2, seek = false) {
      if (length2 > bytes3.length - pos) {
        throw new Error("Unexpected end of data");
      }
      const out = bytes3.subarray(pos, pos + length2);
      if (seek) {
        pos += length2;
      }
      return out;
    },
    seek(length2) {
      pos += length2;
    },
    get pos() {
      return pos;
    }
  };
}
function chunkReader(readChunk) {
  let pos = 0;
  let have = 0;
  let offset2 = 0;
  let currentChunk = new Uint8Array(0);
  const read8 = async (length2) => {
    have = currentChunk.length - offset2;
    const bufa = (
      /** @type {Uint8Array<ArrayBufferLike>[]} */
      [currentChunk.subarray(offset2)]
    );
    while (have < length2) {
      const chunk = await readChunk();
      if (chunk == null) {
        break;
      }
      if (have < 0) {
        if (chunk.length > have) {
          bufa.push(chunk.subarray(-have));
        }
      } else {
        bufa.push(chunk);
      }
      have += chunk.length;
    }
    currentChunk = new Uint8Array(bufa.reduce((p, c) => p + c.length, 0));
    let off = 0;
    for (const b of bufa) {
      currentChunk.set(b, off);
      off += b.length;
    }
    offset2 = 0;
  };
  return {
    async upTo(length2) {
      if (currentChunk.length - offset2 < length2) {
        await read8(length2);
      }
      return currentChunk.subarray(offset2, offset2 + Math.min(currentChunk.length - offset2, length2));
    },
    async exactly(length2, seek = false) {
      if (currentChunk.length - offset2 < length2) {
        await read8(length2);
      }
      if (currentChunk.length - offset2 < length2) {
        throw new Error("Unexpected end of data");
      }
      const out = currentChunk.subarray(offset2, offset2 + length2);
      if (seek) {
        pos += length2;
        offset2 += length2;
      }
      return out;
    },
    seek(length2) {
      pos += length2;
      offset2 += length2;
    },
    get pos() {
      return pos;
    }
  };
}
function asyncIterableReader(asyncIterable) {
  const iterator = asyncIterable[Symbol.asyncIterator]();
  async function readChunk() {
    const next = await iterator.next();
    if (next.done) {
      return null;
    }
    return next.value;
  }
  return chunkReader(readChunk);
}
function limitReader2(reader, byteLimit) {
  let bytesRead = 0;
  return {
    async upTo(length2) {
      let bytes3 = await reader.upTo(length2);
      if (bytes3.length + bytesRead > byteLimit) {
        bytes3 = bytes3.subarray(0, byteLimit - bytesRead);
      }
      return bytes3;
    },
    async exactly(length2, seek = false) {
      const bytes3 = await reader.exactly(length2, seek);
      if (bytes3.length + bytesRead > byteLimit) {
        throw new Error("Unexpected end of data");
      }
      if (seek) {
        bytesRead += length2;
      }
      return bytes3;
    },
    seek(length2) {
      bytesRead += length2;
      reader.seek(length2);
    },
    get pos() {
      return reader.pos;
    }
  };
}

// node_modules/@ipld/car/src/iterator.js
var CarIteratorBase = class {
  /**
   * @param {number} version
   * @param {CID[]} roots
   * @param {AsyncIterable<Block>|void} iterable
   */
  constructor(version2, roots, iterable) {
    this._version = version2;
    this._roots = roots;
    this._iterable = iterable;
    this._decoded = false;
  }
  get version() {
    return this._version;
  }
  /**
   * @returns {Promise<CID[]>}
   */
  async getRoots() {
    return this._roots;
  }
};
var CarBlockIterator = class _CarBlockIterator extends CarIteratorBase {
  // inherited method
  /**
   * Get the list of roots defined by the CAR referenced by this iterator. May be
   * zero or more `CID`s.
   *
   * @function getRoots
   * @memberof CarBlockIterator
   * @instance
   * @async
   * @returns {Promise<CID[]>}
   */
  /**
   * @returns {AsyncIterator<Block>}
   */
  [Symbol.asyncIterator]() {
    if (this._decoded) {
      throw new Error("Cannot decode more than once");
    }
    if (!this._iterable) {
      throw new Error("Block iterable not found");
    }
    this._decoded = true;
    return this._iterable[Symbol.asyncIterator]();
  }
  /**
   * Instantiate a {@link CarBlockIterator} from a `Uint8Array` blob. Rather
   * than decoding the entire byte array prior to returning the iterator, as in
   * {@link CarReader.fromBytes}, only the header is decoded and the remainder
   * of the CAR is parsed as the `Block`s as yielded.
   *
   * @async
   * @static
   * @memberof CarBlockIterator
   * @param {Uint8Array} bytes
   * @returns {Promise<CarBlockIterator>}
   */
  static async fromBytes(bytes3) {
    const { version: version2, roots, iterator } = await fromBytes6(bytes3);
    return new _CarBlockIterator(version2, roots, iterator);
  }
  /**
   * Instantiate a {@link CarBlockIterator} from a `AsyncIterable<Uint8Array>`,
   * such as a [modern Node.js stream](https://nodejs.org/api/stream.html#stream_streams_compatibility_with_async_generators_and_async_iterators).
   * Rather than decoding the entire byte array prior to returning the iterator,
   * as in {@link CarReader.fromIterable}, only the header is decoded and the
   * remainder of the CAR is parsed as the `Block`s as yielded.
   *
   * @async
   * @static
   * @param {AsyncIterable<Uint8Array>} asyncIterable
   * @returns {Promise<CarBlockIterator>}
   */
  static async fromIterable(asyncIterable) {
    const { version: version2, roots, iterator } = await fromIterable(asyncIterable);
    return new _CarBlockIterator(version2, roots, iterator);
  }
};
async function fromBytes6(bytes3) {
  if (!(bytes3 instanceof Uint8Array)) {
    throw new TypeError("fromBytes() requires a Uint8Array");
  }
  return decodeIterator(bytesReader2(bytes3));
}
async function fromIterable(asyncIterable) {
  if (!asyncIterable || !(typeof asyncIterable[Symbol.asyncIterator] === "function")) {
    throw new TypeError("fromIterable() requires an async iterable");
  }
  return decodeIterator(asyncIterableReader(asyncIterable));
}
async function decodeIterator(reader) {
  const decoder3 = createDecoder(reader);
  const { version: version2, roots } = await decoder3.header();
  return { version: version2, roots, iterator: decoder3.blocks() };
}

// node_modules/@ipld/car/src/encoder.js
var import_varint4 = __toESM(require_varint(), 1);
var CAR_V1_VERSION = 1;
function createHeader(roots) {
  const headerBytes = encode4({ version: CAR_V1_VERSION, roots });
  const varintBytes = import_varint4.default.encode(headerBytes.length);
  const header = new Uint8Array(varintBytes.length + headerBytes.length);
  header.set(varintBytes, 0);
  header.set(headerBytes, varintBytes.length);
  return header;
}
function createEncoder(writer) {
  return {
    /**
     * @param {CID[]} roots
     * @returns {Promise<void>}
     */
    async setRoots(roots) {
      const bytes3 = createHeader(roots);
      await writer.write(bytes3);
    },
    /**
     * @param {Block} block
     * @returns {Promise<void>}
     */
    async writeBlock(block) {
      const { cid, bytes: bytes3 } = block;
      await writer.write(new Uint8Array(import_varint4.default.encode(cid.bytes.length + bytes3.length)));
      await writer.write(cid.bytes);
      if (bytes3.length) {
        await writer.write(bytes3);
      }
    },
    /**
     * @returns {Promise<void>}
     */
    async close() {
      await writer.end();
    },
    /**
     * @returns {number}
     */
    version() {
      return CAR_V1_VERSION;
    }
  };
}

// node_modules/@ipld/car/src/iterator-channel.js
function noop() {
}
function create13() {
  const chunkQueue = [];
  let drainer = null;
  let drainerResolver = noop;
  let ended = false;
  let outWait = null;
  let outWaitResolver = noop;
  const makeDrainer = () => {
    if (!drainer) {
      drainer = new Promise((resolve) => {
        drainerResolver = () => {
          drainer = null;
          drainerResolver = noop;
          resolve();
        };
      });
    }
    return drainer;
  };
  const writer = {
    /**
     * @param {T} chunk
     * @returns {Promise<void>}
     */
    write(chunk) {
      chunkQueue.push(chunk);
      const drainer2 = makeDrainer();
      outWaitResolver();
      return drainer2;
    },
    async end() {
      ended = true;
      const drainer2 = makeDrainer();
      outWaitResolver();
      await drainer2;
    }
  };
  const iterator = {
    /** @returns {Promise<IteratorResult<T>>} */
    async next() {
      const chunk = chunkQueue.shift();
      if (chunk) {
        if (chunkQueue.length === 0) {
          drainerResolver();
        }
        return { done: false, value: chunk };
      }
      if (ended) {
        drainerResolver();
        return { done: true, value: void 0 };
      }
      if (!outWait) {
        outWait = new Promise((resolve) => {
          outWaitResolver = () => {
            outWait = null;
            outWaitResolver = noop;
            return resolve(iterator.next());
          };
        });
      }
      return outWait;
    }
  };
  return { writer, iterator };
}

// node_modules/@ipld/car/src/writer-browser.js
var CarWriter = class _CarWriter {
  /**
   * @param {CID[]} roots
   * @param {CarEncoder} encoder
   */
  constructor(roots, encoder3) {
    this._encoder = encoder3;
    this._mutex = encoder3.setRoots(roots);
    this._ended = false;
  }
  /**
   * Write a `Block` (a `{ cid:CID, bytes:Uint8Array }` pair) to the archive.
   *
   * @function
   * @memberof CarWriter
   * @instance
   * @async
   * @param {Block} block - A `{ cid:CID, bytes:Uint8Array }` pair.
   * @returns {Promise<void>} The returned promise will only resolve once the
   * bytes this block generates are written to the `out` iterable.
   */
  async put(block) {
    if (!(block.bytes instanceof Uint8Array) || !block.cid) {
      throw new TypeError("Can only write {cid, bytes} objects");
    }
    if (this._ended) {
      throw new Error("Already closed");
    }
    const cid = CID.asCID(block.cid);
    if (!cid) {
      throw new TypeError("Can only write {cid, bytes} objects");
    }
    this._mutex = this._mutex.then(() => this._encoder.writeBlock({ cid, bytes: block.bytes }));
    return this._mutex;
  }
  /**
   * Finalise the CAR archive and signal that the `out` iterable should end once
   * any remaining bytes are written.
   *
   * @function
   * @memberof CarWriter
   * @instance
   * @async
   * @returns {Promise<void>}
   */
  async close() {
    if (this._ended) {
      throw new Error("Already closed");
    }
    await this._mutex;
    this._ended = true;
    return this._encoder.close();
  }
  /**
   * Returns the version number of the CAR file being written
   *
   * @returns {number}
   */
  version() {
    return this._encoder.version();
  }
  /**
   * Create a new CAR writer "channel" which consists of a
   * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }` pair.
   *
   * @async
   * @static
   * @memberof CarWriter
   * @param {CID[] | CID | void} roots
   * @returns {WriterChannel} The channel takes the form of
   * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }`.
   */
  static create(roots) {
    roots = toRoots(roots);
    const { encoder: encoder3, iterator } = encodeWriter();
    const writer = new _CarWriter(roots, encoder3);
    const out = new CarWriterOut(iterator);
    return { writer, out };
  }
  /**
   * Create a new CAR appender "channel" which consists of a
   * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }` pair.
   * This appender does not consider roots and does not produce a CAR header.
   * It is designed to append blocks to an _existing_ CAR archive. It is
   * expected that `out` will be concatenated onto the end of an existing
   * archive that already has a properly formatted header.
   *
   * @async
   * @static
   * @memberof CarWriter
   * @returns {WriterChannel} The channel takes the form of
   * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }`.
   */
  static createAppender() {
    const { encoder: encoder3, iterator } = encodeWriter();
    encoder3.setRoots = () => Promise.resolve();
    const writer = new _CarWriter([], encoder3);
    const out = new CarWriterOut(iterator);
    return { writer, out };
  }
  /**
   * Update the list of roots in the header of an existing CAR as represented
   * in a Uint8Array.
   *
   * This operation is an _overwrite_, the total length of the CAR will not be
   * modified. A rejection will occur if the new header will not be the same
   * length as the existing header, in which case the CAR will not be modified.
   * It is the responsibility of the user to ensure that the roots being
   * replaced encode as the same length as the new roots.
   *
   * The byte array passed in an argument will be modified and also returned
   * upon successful modification.
   *
   * @async
   * @static
   * @memberof CarWriter
   * @param {Uint8Array} bytes
   * @param {CID[]} roots - A new list of roots to replace the existing list in
   * the CAR header. The new header must take up the same number of bytes as the
   * existing header, so the roots should collectively be the same byte length
   * as the existing roots.
   * @returns {Promise<Uint8Array>}
   */
  static async updateRootsInBytes(bytes3, roots) {
    const reader = bytesReader2(bytes3);
    await readHeader2(reader);
    const newHeader = createHeader(roots);
    if (Number(reader.pos) !== newHeader.length) {
      throw new Error(`updateRoots() can only overwrite a header of the same length (old header is ${reader.pos} bytes, new header is ${newHeader.length} bytes)`);
    }
    bytes3.set(newHeader, 0);
    return bytes3;
  }
};
var CarWriterOut = class {
  /**
   * @param {AsyncIterator<Uint8Array>} iterator
   */
  constructor(iterator) {
    this._iterator = iterator;
  }
  [Symbol.asyncIterator]() {
    if (this._iterating) {
      throw new Error("Multiple iterator not supported");
    }
    this._iterating = true;
    return this._iterator;
  }
};
function encodeWriter() {
  const iw = create13();
  const { writer, iterator } = iw;
  const encoder3 = createEncoder(writer);
  return { encoder: encoder3, iterator };
}
function toRoots(roots) {
  if (roots === void 0) {
    return [];
  }
  if (!Array.isArray(roots)) {
    const cid = CID.asCID(roots);
    if (!cid) {
      throw new TypeError("roots must be a single CID or an array of CIDs");
    }
    return [cid];
  }
  const _roots = [];
  for (const root2 of roots) {
    const _root = CID.asCID(root2);
    if (!_root) {
      throw new TypeError("roots must be a single CID or an array of CIDs");
    }
    _roots.push(_root);
  }
  return _roots;
}

// node_modules/@storacha/upload-client/dist/car.js
var import_varint5 = __toESM(require_varint(), 1);
var code18 = 514;
var NO_ROOTS_HEADER_LENGTH = 18;
function headerEncodingLength(root2) {
  if (!root2)
    return NO_ROOTS_HEADER_LENGTH;
  const headerLength2 = encode4({ version: 1, roots: [root2] }).length;
  const varintLength = import_varint5.default.encodingLength(headerLength2);
  return varintLength + headerLength2;
}
function blockHeaderEncodingLength(block) {
  const payloadLength = block.cid.bytes.length + block.bytes.length;
  const varintLength = import_varint5.default.encodingLength(payloadLength);
  return varintLength + block.cid.bytes.length;
}
function blockEncodingLength(block) {
  return blockHeaderEncodingLength(block) + block.bytes.length;
}
async function encode30(blocks, root2) {
  const { writer, out } = CarWriter.create(root2);
  let error4;
  void (async () => {
    try {
      for await (const block of blocks) {
        await writer.put(block);
      }
    } catch (err) {
      error4 = err;
    } finally {
      await writer.close();
    }
  })();
  const chunks = [];
  for await (const chunk of out)
    chunks.push(chunk);
  if (error4 != null)
    throw error4;
  const roots = root2 != null ? [root2] : [];
  return Object.assign(new Blob(chunks), { version: 1, roots });
}
async function decode34(car) {
  const stream = new BlockStream(car);
  const blocks = (
    /** @type {Block[]} */
    []
  );
  await stream.pipeTo(new WritableStream({
    write: (block) => {
      blocks.push(block);
    }
  }));
  const roots = await stream.getRoots();
  return { blocks, roots };
}
var BlockStream = class extends ReadableStream {
  /** @param {import('./types.js').BlobLike} car */
  constructor(car) {
    let blocksPromise = null;
    const getBlocksIterable = () => {
      if (blocksPromise)
        return blocksPromise;
      blocksPromise = CarBlockIterator.fromIterable(toIterable(car.stream()));
      return blocksPromise;
    };
    let iterator = null;
    super({
      async start() {
        const blocks = await getBlocksIterable();
        iterator = /** @type {AsyncIterator<Block>} */
        blocks[Symbol.asyncIterator]();
      },
      async pull(controller) {
        if (!iterator)
          throw new Error("missing blocks iterator");
        const { value, done } = await iterator.next();
        if (done)
          return controller.close();
        controller.enqueue(value);
      }
    });
    this.getRoots = async () => {
      const blocks = await getBlocksIterable();
      return await blocks.getRoots();
    };
  }
};
function toIterable(stream) {
  return Symbol.asyncIterator in stream ? stream : async function* () {
    const reader = stream.getReader();
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done)
          return;
        yield value;
      }
    } finally {
      reader.releaseLock();
    }
  }();
}

// node_modules/@storacha/upload-client/dist/deduplication.js
var BlockDeduplicationStream = class extends TransformStream {
  constructor() {
    const seen = /* @__PURE__ */ new Set();
    super({
      transform(block, controller) {
        const key = block.cid.toString();
        if (seen.has(key))
          return;
        seen.add(key);
        controller.enqueue(block);
      },
      flush() {
        seen.clear();
      }
    });
  }
};
var dedupe = function* (blocks) {
  const seen = /* @__PURE__ */ new Set();
  for (const b of blocks) {
    const key = b.cid.toString();
    if (seen.has(key))
      continue;
    seen.add(key);
    yield b;
  }
};

// node_modules/@storacha/blob-index/dist/sharded-dag-index.js
var sharded_dag_index_exports = {};
__export(sharded_dag_index_exports, {
  BlobIndexSchema: () => BlobIndexSchema,
  DecodeFailure: () => DecodeFailure,
  MultihashSchema: () => MultihashSchema,
  ShardedDAGIndexSchema: () => ShardedDAGIndexSchema,
  UnknownFormat: () => UnknownFormat,
  archive: () => archive2,
  create: () => create14,
  extract: () => extract2,
  version: () => version,
  view: () => view6
});

// node_modules/@storacha/blob-index/dist/digest-map.js
var cache3 = /* @__PURE__ */ new WeakMap();
var toBase58String = (digest4) => {
  let str = cache3.get(digest4.bytes);
  if (!str) {
    str = base58btc.encode(digest4.bytes);
    cache3.set(digest4.bytes, str);
  }
  return str;
};
var _data2;
var DigestMap = class {
  /**
   * @param {Array<[Key, Value]>} [entries]
   */
  constructor(entries3) {
    /** @type {Map<string, [Key, Value]>} */
    __privateAdd(this, _data2);
    __privateSet(this, _data2, /* @__PURE__ */ new Map());
    for (const [k, v] of entries3 ?? []) {
      this.set(k, v);
    }
  }
  get [Symbol.toStringTag]() {
    return "DigestMap";
  }
  clear() {
    __privateGet(this, _data2).clear();
  }
  /**
   * @param {Key} key
   * @returns {boolean}
   */
  delete(key) {
    const mhstr = toBase58String(key);
    return __privateGet(this, _data2).delete(mhstr);
  }
  /**
   * @param {(value: Value, key: Key, map: Map<Key, Value>) => void} callbackfn
   * @param {any} [thisArg]
   */
  forEach(callbackfn, thisArg) {
    for (const [k, v] of __privateGet(this, _data2).values()) {
      callbackfn.call(thisArg, v, k, this);
    }
  }
  /**
   * @param {Key} key
   * @returns {Value|undefined}
   */
  get(key) {
    const data = __privateGet(this, _data2).get(toBase58String(key));
    if (data)
      return data[1];
  }
  /**
   * @param {Key} key
   * @returns {boolean}
   */
  has(key) {
    return __privateGet(this, _data2).has(toBase58String(key));
  }
  /**
   * @param {Key} key
   * @param {Value} value
   */
  set(key, value) {
    __privateGet(this, _data2).set(toBase58String(key), [key, value]);
    return this;
  }
  /** @returns {number} */
  get size() {
    return __privateGet(this, _data2).size;
  }
  /** @returns */
  [Symbol.iterator]() {
    return this.entries();
  }
  /** @returns {IterableIterator<[Key, Value]>} */
  *entries() {
    yield* __privateGet(this, _data2).values();
  }
  /** @returns {IterableIterator<Key>} */
  *keys() {
    for (const [k] of __privateGet(this, _data2).values()) {
      yield k;
    }
  }
  /** @returns {IterableIterator<Value>} */
  *values() {
    for (const [, v] of __privateGet(this, _data2).values()) {
      yield v;
    }
  }
};
_data2 = new WeakMap();

// node_modules/@storacha/blob-index/dist/sharded-dag-index.js
var version = "index/sharded/dag@0.1";
var ShardedDAGIndexSchema = schema_exports3.variant({
  [version]: schema_exports3.struct({
    /** DAG root. */
    content: schema_exports3.link(),
    /** Shards the DAG can be found in. */
    shards: schema_exports3.array(schema_exports3.link())
  })
});
var MultihashSchema = schema_exports3.bytes();
var BlobIndexSchema = schema_exports3.tuple([
  MultihashSchema,
  schema_exports3.array(
    /** multihash bytes, offset, length. */
    schema_exports3.tuple([
      MultihashSchema,
      schema_exports3.tuple([schema_exports3.number(), schema_exports3.number()])
    ])
  )
]);
var extract2 = (archive4) => {
  const { roots, blocks } = car_exports.decode(archive4);
  if (!roots.length) {
    return error(new UnknownFormat("missing root block"));
  }
  const { code: code19 } = roots[0].cid;
  if (code19 !== code) {
    return error(new UnknownFormat(`unexpected root CID codec: 0x${code19.toString(16)}`));
  }
  return view6({ root: roots[0], blocks });
};
var view6 = ({ root: root2, blocks }) => {
  const [version2, dagIndexData] = ShardedDAGIndexSchema.match(decode6(root2.bytes));
  switch (version2) {
    case version2: {
      const dagIndex = create14(dagIndexData.content);
      for (const shardLink of dagIndexData.shards) {
        const shard = blocks.get(shardLink.toString());
        if (!shard) {
          return error(new DecodeFailure(`missing shard block: ${shardLink}`));
        }
        const blobIndexData = BlobIndexSchema.from(decode6(shard.bytes));
        const blobIndex = new DigestMap();
        for (const [digest4, [offset2, length2]] of blobIndexData[1]) {
          blobIndex.set(decode5(digest4), [offset2, length2]);
        }
        dagIndex.shards.set(decode5(blobIndexData[0]), blobIndex);
      }
      return ok(dagIndex);
    }
    default:
      return error(new UnknownFormat(`unknown index version: ${version2}`));
  }
};
var _content, _shards;
var ShardedDAGIndex = class {
  /** @param {API.UnknownLink} content */
  constructor(content2) {
    __privateAdd(this, _content);
    __privateAdd(this, _shards);
    __privateSet(this, _content, content2);
    __privateSet(this, _shards, new DigestMap());
  }
  get content() {
    return __privateGet(this, _content);
  }
  get shards() {
    return __privateGet(this, _shards);
  }
  /**
   * @param {API.ShardDigest} shard
   * @param {API.SliceDigest} slice
   * @param {API.Position} pos
   */
  setSlice(shard, slice3, pos) {
    let index3 = __privateGet(this, _shards).get(shard);
    if (!index3) {
      index3 = new DigestMap();
      __privateGet(this, _shards).set(shard, index3);
    }
    index3.set(slice3, pos);
  }
  archive() {
    return archive2(this);
  }
};
_content = new WeakMap();
_shards = new WeakMap();
var _reason;
var UnknownFormat = class extends Failure {
  /** @param {string} [reason] */
  constructor(reason) {
    super();
    __privateAdd(this, _reason);
    this.name = /** @type {const} */
    "UnknownFormat";
    __privateSet(this, _reason, reason);
  }
  describe() {
    return __privateGet(this, _reason) ?? "unknown format";
  }
};
_reason = new WeakMap();
var _reason2;
var DecodeFailure = class extends Failure {
  /** @param {string} [reason] */
  constructor(reason) {
    super();
    __privateAdd(this, _reason2);
    this.name = /** @type {const} */
    "DecodeFailure";
    __privateSet(this, _reason2, reason);
  }
  describe() {
    return __privateGet(this, _reason2) ?? "failed to decode";
  }
};
_reason2 = new WeakMap();
var create14 = (content2) => new ShardedDAGIndex(content2);
var archive2 = async (model) => {
  const roots = [];
  const blocks = /* @__PURE__ */ new Map();
  try {
    const shards = [...model.shards.entries()].sort((a, b) => compare2(a[0].digest, b[0].digest));
    const index3 = {
      content: model.content,
      shards: (
        /** @type {API.Link[]} */
        []
      )
    };
    for (const s of shards) {
      const slices = [...s[1].entries()].sort((a, b) => compare2(a[0].digest, b[0].digest)).map((e) => [e[0].bytes, e[1]]);
      const bytes4 = encode4([s[0].bytes, slices]);
      const digest5 = await sha2562.digest(bytes4);
      const cid2 = create2(code, digest5);
      blocks.set(cid2.toString(), { cid: cid2, bytes: bytes4 });
      index3.shards.push(cid2);
    }
    const bytes3 = encode4({ [version]: index3 });
    const digest4 = await sha2562.digest(bytes3);
    const cid = create2(code, digest4);
    roots.push({ cid, bytes: bytes3 });
  } catch (err) {
    return error(
      /** @type {API.EncodeFailure} */
      {
        name: "EncodeFailure",
        message: `encoding DAG: ${err.message}`,
        stack: err.stack
      }
    );
  }
  try {
    return ok(car_exports.encode({ roots, blocks }));
  } catch (err) {
    return error(
      /** @type {API.EncodeFailure} */
      {
        name: "EncodeFailure",
        message: `encoding CAR: ${err.message}`,
        stack: err.stack
      }
    );
  }
};

// node_modules/uint8arraylist/dist/src/index.js
var symbol = Symbol.for("@achingbrain/uint8arraylist");
function findBufAndOffset(bufs, index3) {
  if (index3 == null || index3 < 0) {
    throw new RangeError("index is out of bounds");
  }
  let offset2 = 0;
  for (const buf2 of bufs) {
    const bufEnd = offset2 + buf2.byteLength;
    if (index3 < bufEnd) {
      return {
        buf: buf2,
        index: index3 - offset2
      };
    }
    offset2 = bufEnd;
  }
  throw new RangeError("index is out of bounds");
}
function isUint8ArrayList(value) {
  return Boolean(value == null ? void 0 : value[symbol]);
}
var _a2;
var Uint8ArrayList = class _Uint8ArrayList {
  constructor(...data) {
    __publicField(this, "bufs");
    __publicField(this, "length");
    __publicField(this, _a2, true);
    this.bufs = [];
    this.length = 0;
    if (data.length > 0) {
      this.appendAll(data);
    }
  }
  *[(_a2 = symbol, Symbol.iterator)]() {
    yield* this.bufs;
  }
  get byteLength() {
    return this.length;
  }
  /**
   * Add one or more `bufs` to the end of this Uint8ArrayList
   */
  append(...bufs) {
    this.appendAll(bufs);
  }
  /**
   * Add all `bufs` to the end of this Uint8ArrayList
   */
  appendAll(bufs) {
    let length2 = 0;
    for (const buf2 of bufs) {
      if (buf2 instanceof Uint8Array) {
        length2 += buf2.byteLength;
        this.bufs.push(buf2);
      } else if (isUint8ArrayList(buf2)) {
        length2 += buf2.byteLength;
        this.bufs.push(...buf2.bufs);
      } else {
        throw new Error("Could not append value, must be an Uint8Array or a Uint8ArrayList");
      }
    }
    this.length += length2;
  }
  /**
   * Add one or more `bufs` to the start of this Uint8ArrayList
   */
  prepend(...bufs) {
    this.prependAll(bufs);
  }
  /**
   * Add all `bufs` to the start of this Uint8ArrayList
   */
  prependAll(bufs) {
    let length2 = 0;
    for (const buf2 of bufs.reverse()) {
      if (buf2 instanceof Uint8Array) {
        length2 += buf2.byteLength;
        this.bufs.unshift(buf2);
      } else if (isUint8ArrayList(buf2)) {
        length2 += buf2.byteLength;
        this.bufs.unshift(...buf2.bufs);
      } else {
        throw new Error("Could not prepend value, must be an Uint8Array or a Uint8ArrayList");
      }
    }
    this.length += length2;
  }
  /**
   * Read the value at `index`
   */
  get(index3) {
    const res = findBufAndOffset(this.bufs, index3);
    return res.buf[res.index];
  }
  /**
   * Set the value at `index` to `value`
   */
  set(index3, value) {
    const res = findBufAndOffset(this.bufs, index3);
    res.buf[res.index] = value;
  }
  /**
   * Copy bytes from `buf` to the index specified by `offset`
   */
  write(buf2, offset2 = 0) {
    if (buf2 instanceof Uint8Array) {
      for (let i = 0; i < buf2.length; i++) {
        this.set(offset2 + i, buf2[i]);
      }
    } else if (isUint8ArrayList(buf2)) {
      for (let i = 0; i < buf2.length; i++) {
        this.set(offset2 + i, buf2.get(i));
      }
    } else {
      throw new Error("Could not write value, must be an Uint8Array or a Uint8ArrayList");
    }
  }
  /**
   * Remove bytes from the front of the pool
   */
  consume(bytes3) {
    bytes3 = Math.trunc(bytes3);
    if (Number.isNaN(bytes3) || bytes3 <= 0) {
      return;
    }
    if (bytes3 === this.byteLength) {
      this.bufs = [];
      this.length = 0;
      return;
    }
    while (this.bufs.length > 0) {
      if (bytes3 >= this.bufs[0].byteLength) {
        bytes3 -= this.bufs[0].byteLength;
        this.length -= this.bufs[0].byteLength;
        this.bufs.shift();
      } else {
        this.bufs[0] = this.bufs[0].subarray(bytes3);
        this.length -= bytes3;
        break;
      }
    }
  }
  /**
   * Extracts a section of an array and returns a new array.
   *
   * This is a copy operation as it is with Uint8Arrays and Arrays
   * - note this is different to the behaviour of Node Buffers.
   */
  slice(beginInclusive, endExclusive) {
    const { bufs, length: length2 } = this._subList(beginInclusive, endExclusive);
    return concat2(bufs, length2);
  }
  /**
   * Returns a alloc from the given start and end element index.
   *
   * In the best case where the data extracted comes from a single Uint8Array
   * internally this is a no-copy operation otherwise it is a copy operation.
   */
  subarray(beginInclusive, endExclusive) {
    const { bufs, length: length2 } = this._subList(beginInclusive, endExclusive);
    if (bufs.length === 1) {
      return bufs[0];
    }
    return concat2(bufs, length2);
  }
  /**
   * Returns a allocList from the given start and end element index.
   *
   * This is a no-copy operation.
   */
  sublist(beginInclusive, endExclusive) {
    const { bufs, length: length2 } = this._subList(beginInclusive, endExclusive);
    const list10 = new _Uint8ArrayList();
    list10.length = length2;
    list10.bufs = [...bufs];
    return list10;
  }
  _subList(beginInclusive, endExclusive) {
    beginInclusive = beginInclusive ?? 0;
    endExclusive = endExclusive ?? this.length;
    if (beginInclusive < 0) {
      beginInclusive = this.length + beginInclusive;
    }
    if (endExclusive < 0) {
      endExclusive = this.length + endExclusive;
    }
    if (beginInclusive < 0 || endExclusive > this.length) {
      throw new RangeError("index is out of bounds");
    }
    if (beginInclusive === endExclusive) {
      return { bufs: [], length: 0 };
    }
    if (beginInclusive === 0 && endExclusive === this.length) {
      return { bufs: this.bufs, length: this.length };
    }
    const bufs = [];
    let offset2 = 0;
    for (let i = 0; i < this.bufs.length; i++) {
      const buf2 = this.bufs[i];
      const bufStart = offset2;
      const bufEnd = bufStart + buf2.byteLength;
      offset2 = bufEnd;
      if (beginInclusive >= bufEnd) {
        continue;
      }
      const sliceStartInBuf = beginInclusive >= bufStart && beginInclusive < bufEnd;
      const sliceEndsInBuf = endExclusive > bufStart && endExclusive <= bufEnd;
      if (sliceStartInBuf && sliceEndsInBuf) {
        if (beginInclusive === bufStart && endExclusive === bufEnd) {
          bufs.push(buf2);
          break;
        }
        const start = beginInclusive - bufStart;
        bufs.push(buf2.subarray(start, start + (endExclusive - beginInclusive)));
        break;
      }
      if (sliceStartInBuf) {
        if (beginInclusive === 0) {
          bufs.push(buf2);
          continue;
        }
        bufs.push(buf2.subarray(beginInclusive - bufStart));
        continue;
      }
      if (sliceEndsInBuf) {
        if (endExclusive === bufEnd) {
          bufs.push(buf2);
          break;
        }
        bufs.push(buf2.subarray(0, endExclusive - bufStart));
        break;
      }
      bufs.push(buf2);
    }
    return { bufs, length: endExclusive - beginInclusive };
  }
  indexOf(search, offset2 = 0) {
    if (!isUint8ArrayList(search) && !(search instanceof Uint8Array)) {
      throw new TypeError('The "value" argument must be a Uint8ArrayList or Uint8Array');
    }
    const needle = search instanceof Uint8Array ? search : search.subarray();
    offset2 = Number(offset2 ?? 0);
    if (isNaN(offset2)) {
      offset2 = 0;
    }
    if (offset2 < 0) {
      offset2 = this.length + offset2;
    }
    if (offset2 < 0) {
      offset2 = 0;
    }
    if (search.length === 0) {
      return offset2 > this.length ? this.length : offset2;
    }
    const M = needle.byteLength;
    if (M === 0) {
      throw new TypeError("search must be at least 1 byte long");
    }
    const radix3 = 256;
    const rightmostPositions = new Int32Array(radix3);
    for (let c = 0; c < radix3; c++) {
      rightmostPositions[c] = -1;
    }
    for (let j = 0; j < M; j++) {
      rightmostPositions[needle[j]] = j;
    }
    const right = rightmostPositions;
    const lastIndex = this.byteLength - needle.byteLength;
    const lastPatIndex = needle.byteLength - 1;
    let skip2;
    for (let i = offset2; i <= lastIndex; i += skip2) {
      skip2 = 0;
      for (let j = lastPatIndex; j >= 0; j--) {
        const char = this.get(i + j);
        if (needle[j] !== char) {
          skip2 = Math.max(1, j - right[char]);
          break;
        }
      }
      if (skip2 === 0) {
        return i;
      }
    }
    return -1;
  }
  getInt8(byteOffset) {
    const buf2 = this.subarray(byteOffset, byteOffset + 1);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getInt8(0);
  }
  setInt8(byteOffset, value) {
    const buf2 = allocUnsafe(1);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setInt8(0, value);
    this.write(buf2, byteOffset);
  }
  getInt16(byteOffset, littleEndian) {
    const buf2 = this.subarray(byteOffset, byteOffset + 2);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getInt16(0, littleEndian);
  }
  setInt16(byteOffset, value, littleEndian) {
    const buf2 = alloc2(2);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setInt16(0, value, littleEndian);
    this.write(buf2, byteOffset);
  }
  getInt32(byteOffset, littleEndian) {
    const buf2 = this.subarray(byteOffset, byteOffset + 4);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getInt32(0, littleEndian);
  }
  setInt32(byteOffset, value, littleEndian) {
    const buf2 = alloc2(4);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setInt32(0, value, littleEndian);
    this.write(buf2, byteOffset);
  }
  getBigInt64(byteOffset, littleEndian) {
    const buf2 = this.subarray(byteOffset, byteOffset + 8);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getBigInt64(0, littleEndian);
  }
  setBigInt64(byteOffset, value, littleEndian) {
    const buf2 = alloc2(8);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setBigInt64(0, value, littleEndian);
    this.write(buf2, byteOffset);
  }
  getUint8(byteOffset) {
    const buf2 = this.subarray(byteOffset, byteOffset + 1);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getUint8(0);
  }
  setUint8(byteOffset, value) {
    const buf2 = allocUnsafe(1);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setUint8(0, value);
    this.write(buf2, byteOffset);
  }
  getUint16(byteOffset, littleEndian) {
    const buf2 = this.subarray(byteOffset, byteOffset + 2);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getUint16(0, littleEndian);
  }
  setUint16(byteOffset, value, littleEndian) {
    const buf2 = alloc2(2);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setUint16(0, value, littleEndian);
    this.write(buf2, byteOffset);
  }
  getUint32(byteOffset, littleEndian) {
    const buf2 = this.subarray(byteOffset, byteOffset + 4);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getUint32(0, littleEndian);
  }
  setUint32(byteOffset, value, littleEndian) {
    const buf2 = alloc2(4);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setUint32(0, value, littleEndian);
    this.write(buf2, byteOffset);
  }
  getBigUint64(byteOffset, littleEndian) {
    const buf2 = this.subarray(byteOffset, byteOffset + 8);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getBigUint64(0, littleEndian);
  }
  setBigUint64(byteOffset, value, littleEndian) {
    const buf2 = alloc2(8);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setBigUint64(0, value, littleEndian);
    this.write(buf2, byteOffset);
  }
  getFloat32(byteOffset, littleEndian) {
    const buf2 = this.subarray(byteOffset, byteOffset + 4);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getFloat32(0, littleEndian);
  }
  setFloat32(byteOffset, value, littleEndian) {
    const buf2 = alloc2(4);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setFloat32(0, value, littleEndian);
    this.write(buf2, byteOffset);
  }
  getFloat64(byteOffset, littleEndian) {
    const buf2 = this.subarray(byteOffset, byteOffset + 8);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    return view7.getFloat64(0, littleEndian);
  }
  setFloat64(byteOffset, value, littleEndian) {
    const buf2 = alloc2(8);
    const view7 = new DataView(buf2.buffer, buf2.byteOffset, buf2.byteLength);
    view7.setFloat64(0, value, littleEndian);
    this.write(buf2, byteOffset);
  }
  equals(other) {
    if (other == null) {
      return false;
    }
    if (!(other instanceof _Uint8ArrayList)) {
      return false;
    }
    if (other.bufs.length !== this.bufs.length) {
      return false;
    }
    for (let i = 0; i < this.bufs.length; i++) {
      if (!equals3(this.bufs[i], other.bufs[i])) {
        return false;
      }
    }
    return true;
  }
  /**
   * Create a Uint8ArrayList from a pre-existing list of Uint8Arrays.  Use this
   * method if you know the total size of all the Uint8Arrays ahead of time.
   */
  static fromUint8Arrays(bufs, length2) {
    const list10 = new _Uint8ArrayList();
    list10.bufs = bufs;
    if (length2 == null) {
      length2 = bufs.reduce((acc, curr) => acc + curr.byteLength, 0);
    }
    list10.length = length2;
    return list10;
  }
};

// node_modules/carstream/src/varint.js
var MSB2 = 128;
var REST2 = 127;
var MSBALL2 = ~REST2;
var INT2 = Math.pow(2, 31);
var encode31 = (num) => {
  const out = [];
  let offset2 = 0;
  while (num >= INT2) {
    out[offset2++] = num & 255 | MSB2;
    num /= 128;
  }
  while (num & MSBALL2) {
    out[offset2++] = num & 255 | MSB2;
    num >>>= 7;
  }
  out[offset2] = num | 0;
  return out;
};
var decode35 = (buf2, offset2) => {
  let res = 0;
  offset2 = offset2 || 0;
  let shift = 0;
  let counter = offset2;
  let b;
  const l = buf2.length;
  do {
    if (counter >= l || shift > 49) throw new RangeError("Could not decode varint");
    b = buf2.get(counter++);
    res += shift < 28 ? (b & REST2) << shift : (b & REST2) * Math.pow(2, shift);
    shift += 7;
  } while (b >= MSB2);
  return [res, counter - offset2];
};

// node_modules/carstream/src/reader.js
var State = {
  ReadHeaderLength: 0,
  ReadHeader: 1,
  ReadBlockLength: 2,
  ReadBlock: 3
};
var CIDV0_BYTES2 = {
  SHA2_256: 18,
  LENGTH: 32,
  DAG_PB: 112
};
var _headerPromise;
var CARReaderStream = class extends TransformStream {
  /**
   * @param {QueuingStrategy<Uint8Array>} [writableStrategy]
   * An object that optionally defines a queuing strategy for the stream.
   * @param {QueuingStrategy<import('./api.js').Block & import('./api.js').Position>} [readableStrategy]
   * An object that optionally defines a queuing strategy for the stream.
   * Defaults to a CountQueuingStrategy with highWaterMark of `1` to allow
   * `getHeader` to be called before the stream is consumed.
   */
  constructor(writableStrategy, readableStrategy) {
    const buffer2 = new Uint8ArrayList();
    let offset2 = 0;
    let prevOffset = offset2;
    let wanted = 8;
    let state = State.ReadHeaderLength;
    let resolveHeader;
    const headerPromise = new Promise((resolve) => {
      resolveHeader = resolve;
    });
    super({
      transform(chunk, controller) {
        buffer2.append(chunk);
        while (true) {
          if (buffer2.length < wanted) break;
          if (state === State.ReadHeaderLength) {
            const [length2, bytes3] = decode35(buffer2);
            buffer2.consume(bytes3);
            prevOffset = offset2;
            offset2 += bytes3;
            state = State.ReadHeader;
            wanted = length2;
          } else if (state === State.ReadHeader) {
            const header = decode6(buffer2.slice(0, wanted));
            resolveHeader && resolveHeader(header);
            buffer2.consume(wanted);
            prevOffset = offset2;
            offset2 += wanted;
            state = State.ReadBlockLength;
            wanted = 8;
          } else if (state === State.ReadBlockLength) {
            const [length2, bytes3] = decode35(buffer2);
            buffer2.consume(bytes3);
            prevOffset = offset2;
            offset2 += bytes3;
            state = State.ReadBlock;
            wanted = length2;
          } else if (state === State.ReadBlock) {
            const _offset = prevOffset;
            const length2 = offset2 - prevOffset + wanted;
            prevOffset = offset2;
            let cid;
            if (buffer2.get(0) === CIDV0_BYTES2.SHA2_256 && buffer2.get(1) === CIDV0_BYTES2.LENGTH) {
              const bytes4 = buffer2.subarray(0, 34);
              const multihash = decode5(bytes4);
              cid = createLegacy(multihash);
              buffer2.consume(34);
              offset2 += 34;
            } else {
              const [version2, versionBytes] = decode35(buffer2);
              if (version2 !== 1) throw new Error(`unexpected CID version (${version2})`);
              buffer2.consume(versionBytes);
              offset2 += versionBytes;
              const [codec, codecBytes] = decode35(buffer2);
              buffer2.consume(codecBytes);
              offset2 += codecBytes;
              const multihashBytes = getMultihashLength2(buffer2);
              const multihash = decode5(buffer2.subarray(0, multihashBytes));
              cid = create2(codec, multihash);
              buffer2.consume(multihashBytes);
              offset2 += multihashBytes;
            }
            const blockBytes = wanted - (offset2 - prevOffset);
            const bytes3 = buffer2.subarray(0, blockBytes);
            controller.enqueue({ cid, bytes: bytes3, offset: _offset, length: length2, blockOffset: offset2, blockLength: blockBytes });
            buffer2.consume(blockBytes);
            prevOffset = offset2;
            offset2 += blockBytes;
            state = State.ReadBlockLength;
            wanted = 8;
          }
        }
      },
      flush(controller) {
        if (state !== State.ReadBlockLength) {
          controller.error(new Error("unexpected end of data"));
        }
      }
    }, writableStrategy, readableStrategy ?? new CountQueuingStrategy({ highWaterMark: 1 }));
    /** @type {Promise<import('./api.js').CARHeader>} */
    __privateAdd(this, _headerPromise);
    __privateSet(this, _headerPromise, headerPromise);
  }
  /**
   * Get the decoded CAR header.
   */
  getHeader() {
    return __privateGet(this, _headerPromise);
  }
};
_headerPromise = new WeakMap();
var getMultihashLength2 = (bytes3) => {
  const [, codeBytes] = decode35(bytes3);
  const [length2, lengthBytes] = decode35(bytes3, codeBytes);
  return codeBytes + lengthBytes + length2;
};

// node_modules/carstream/src/writer.js
var encodeHeader2 = (roots) => {
  const headerBytes = encode4({ version: 1, roots });
  const varintBytes = encode31(headerBytes.length);
  const header = new Uint8Array(varintBytes.length + headerBytes.length);
  header.set(varintBytes, 0);
  header.set(headerBytes, varintBytes.length);
  return header;
};
var encodeBlock = (block) => {
  const varintBytes = encode31(block.cid.bytes.length + block.bytes.length);
  const bytes3 = new Uint8Array(varintBytes.length + block.cid.bytes.length + block.bytes.length);
  bytes3.set(varintBytes);
  bytes3.set(block.cid.bytes, varintBytes.length);
  bytes3.set(block.bytes, varintBytes.length + block.cid.bytes.length);
  return bytes3;
};
var CARWriterStream = class extends TransformStream {
  /**
   * @param {import('multiformats').UnknownLink[]} [roots]
   * @param {QueuingStrategy<import('./api.js').Block>} [writableStrategy]
   * @param {QueuingStrategy<Uint8Array>} [readableStrategy]
   */
  constructor(roots = [], writableStrategy, readableStrategy) {
    super({
      start: (controller) => controller.enqueue(encodeHeader2(roots)),
      transform: (block, controller) => controller.enqueue(encodeBlock(block))
    }, writableStrategy, readableStrategy);
  }
};

// node_modules/@storacha/blob-index/dist/util.js
async function indexShardedDAG(root2, shards, shardIndexes) {
  const index3 = create14(root2);
  for (const [i, shard] of shards.entries()) {
    const slices = shardIndexes[i];
    index3.shards.set(shard.multihash, slices);
  }
  return await index3.archive();
}

// node_modules/@storacha/upload-client/dist/sharding.js
var SHARD_SIZE = 133169152;
var ShardingStream = class extends TransformStream {
  /**
   * @param {import('./types.js').ShardingOptions} [options]
   */
  constructor(options = {}) {
    const shardSize = options.shardSize ?? SHARD_SIZE;
    const maxBlockLength = shardSize - headerEncodingLength();
    let blocks = [];
    let readyBlocks = null;
    let slices = new DigestMap();
    let readySlices = null;
    let currentLength = 0;
    super({
      async transform(block, controller) {
        if (readyBlocks != null && readySlices != null) {
          controller.enqueue(await encodeCAR(readyBlocks, readySlices));
          readyBlocks = null;
          readySlices = null;
        }
        const blockHeaderLength = blockHeaderEncodingLength(block);
        const blockLength2 = blockHeaderLength + block.bytes.length;
        if (blockLength2 > maxBlockLength) {
          throw new Error(`block will cause CAR to exceed shard size: ${block.cid}`);
        }
        if (blocks.length && currentLength + blockLength2 > maxBlockLength) {
          readyBlocks = blocks;
          readySlices = slices;
          blocks = [];
          slices = new DigestMap();
          currentLength = 0;
        }
        blocks.push(block);
        slices.set(block.cid.multihash, [
          headerEncodingLength() + currentLength + blockHeaderLength,
          block.bytes.length
        ]);
        currentLength += blockLength2;
      },
      async flush(controller) {
        if (readyBlocks != null && readySlices != null) {
          controller.enqueue(await encodeCAR(readyBlocks, readySlices));
        }
        const rootBlock = blocks.at(-1);
        if (rootBlock == null)
          return;
        const rootCID = options.rootCID ?? rootBlock.cid;
        const headerLength2 = headerEncodingLength(rootCID);
        if (headerLength2 + currentLength > shardSize) {
          const overage = headerLength2 + currentLength - shardSize;
          const overflowBlocks = [];
          let overflowCurrentLength = 0;
          while (overflowCurrentLength < overage) {
            const block = blocks[blocks.length - 1];
            blocks.pop();
            slices.delete(block.cid.multihash);
            overflowBlocks.unshift(block);
            overflowCurrentLength += blockEncodingLength(block);
            if (blocks.length < 1)
              throw new Error(`block will cause CAR to exceed shard size: ${block.cid}`);
          }
          controller.enqueue(await encodeCAR(blocks, slices));
          overflowCurrentLength = 0;
          const overflowSlices = new DigestMap();
          for (const block of overflowBlocks) {
            const overflowBlockHeaderLength = blockHeaderEncodingLength(block);
            overflowSlices.set(block.cid.multihash, [
              headerLength2 + overflowCurrentLength + overflowBlockHeaderLength,
              block.bytes.length
            ]);
            overflowCurrentLength += overflowBlockHeaderLength + block.bytes.length;
          }
          controller.enqueue(await encodeCAR(overflowBlocks, overflowSlices, rootCID));
        } else {
          const diff = headerLength2 - headerEncodingLength();
          for (const slice3 of slices.values()) {
            slice3[0] += diff;
          }
          controller.enqueue(await encodeCAR(blocks, slices, rootCID));
        }
      }
    });
  }
};
var defaultFileComparator = (a, b, getComparedValue = (file) => file.name) => {
  return ascending(a, b, getComparedValue);
};
function ascending(a, b, getComparedValue) {
  const ask = getComparedValue(a);
  const bsk = getComparedValue(b);
  if (ask === bsk)
    return 0;
  else if (ask < bsk)
    return -1;
  return 1;
}
var encodeCAR = async (blocks, slices, root2) => Object.assign(await encode30(blocks, root2), { slices });

// node_modules/@storacha/upload-client/dist/index.js
var isSubArray = (bytes3) => bytes3.byteOffset !== 0 || bytes3.buffer.byteLength !== bytes3.byteLength;
async function uploadFile(conf, file, options = {}) {
  const shardSize = options.shardSize ?? SHARD_SIZE;
  if (file.size != null && file.size < shardSize) {
    const { blocks, cid } = await encodeFile2(file, options);
    return await uploadBlocks(conf, blocks, { rootCID: cid, ...options });
  }
  return await uploadBlockStream(conf, createFileEncoderStream(file, options), options);
}
async function uploadDirectory(conf, files, options = {}) {
  const { customOrder = false } = options;
  const entries3 = customOrder ? files : [...files].sort(defaultFileComparator);
  let size5 = 0;
  let isKnownSize = true;
  for (const entry of entries3) {
    if (entry.size == null) {
      isKnownSize = false;
      break;
    }
    size5 += entry.size;
  }
  const shardSize = options.shardSize ?? SHARD_SIZE;
  if (isKnownSize && size5 < shardSize) {
    const { blocks, cid } = await encodeDirectory2(entries3, options);
    return await uploadBlocks(conf, blocks, { rootCID: cid, ...options });
  }
  return await uploadBlockStream(conf, createDirectoryEncoderStream(entries3, options), options);
}
async function uploadCAR(conf, car, options = {}) {
  const shardSize = options.shardSize ?? SHARD_SIZE;
  if (car.size != null && car.size < shardSize) {
    const { blocks: blocks2, roots } = await decode34(car);
    return await uploadBlocks(conf, blocks2, { rootCID: roots[0], ...options });
  }
  const blocks = new BlockStream(car);
  options.rootCID = options.rootCID ?? (await blocks.getRoots())[0];
  return await uploadBlockStream(conf, blocks, options);
}
async function uploadBlockStream(conf, blocks, { pieceHasher = multihash_exports, ...options } = {}) {
  const configure6 = typeof conf === "function" ? conf : () => conf;
  const shardIndexes = [];
  const shards = [];
  let root2 = null;
  if (options.dedupe == null || options.dedupe === true) {
    blocks = blocks.pipeThrough(new BlockDeduplicationStream());
  }
  await blocks.pipeThrough(new ShardingStream(options)).pipeThrough(
    /** @type {TransformStream<import('./types.js').IndexedCARFile, import('./types.js').CARMetadata>} */
    new TransformStream({
      async transform(car, controller) {
        const bytes3 = new Uint8Array(await car.arrayBuffer());
        const digest4 = await sha2562.digest(bytes3);
        const conf2 = await configure6([
          {
            can: ability,
            nb: input(digest4, bytes3.length)
          }
        ]);
        await add8(conf2, digest4, bytes3, options);
        const cid = create2(code18, digest4);
        let piece;
        if (pieceHasher) {
          const multihashDigest = await pieceHasher.digest(bytes3);
          piece = create2(code3, multihashDigest);
          const content2 = create2(code3, digest4);
          const result = await storefront_exports2.filecoinOffer({
            issuer: conf2.issuer,
            audience: conf2.audience,
            // Resource of invocation is the issuer did for being self issued
            with: conf2.issuer.did(),
            proofs: conf2.proofs
          }, content2, piece, options);
          if (result.out.error) {
            throw new Error("failed to offer piece for aggregation into filecoin deal", { cause: result.out.error });
          }
        }
        const { version: version2, roots, size: size5, slices } = car;
        controller.enqueue({ version: version2, roots, size: size5, cid, piece, slices });
      }
    })
  ).pipeTo(new WritableStream({
    write(meta) {
      root2 = root2 || meta.roots[0];
      shards.push(meta.cid);
      for (const [s, p] of meta.slices) {
        if (isSubArray(s.bytes)) {
          meta.slices.set(decode5(s.bytes.slice()), p);
        }
      }
      meta.slices.set(meta.cid.multihash, [0, meta.size]);
      shardIndexes.push(meta.slices);
      if (options.onShardStored)
        options.onShardStored(meta);
    }
  }));
  if (!root2)
    throw new Error("missing root CID");
  const indexBytes = await indexShardedDAG(root2, shards, shardIndexes);
  if (!indexBytes.ok) {
    throw new Error("failed to archive DAG index", { cause: indexBytes.error });
  }
  const indexDigest = await sha2562.digest(indexBytes.ok);
  const indexLink = create2(code18, indexDigest);
  const [blobAddConf, indexAddConf, uploadAddConf] = await Promise.all([
    configure6([
      {
        can: ability,
        nb: input(indexDigest, indexBytes.ok.length)
      }
    ]),
    configure6([
      {
        can: ability6,
        nb: input6(indexLink)
      }
    ]),
    configure6([
      {
        can: ability7,
        nb: input7(root2, shards)
      }
    ])
  ]);
  await add8(blobAddConf, indexDigest, indexBytes.ok, options);
  await add9(indexAddConf, indexLink, options);
  await add10(uploadAddConf, root2, shards, options);
  return root2;
}
async function uploadBlocks(conf, blocks, { pieceHasher = multihash_exports, ...options } = {}) {
  const configure6 = typeof conf === "function" ? conf : () => conf;
  if (options.dedupe == null || options.dedupe === true) {
    blocks = dedupe(blocks);
  }
  let car;
  const blockStream = new ReadableStream({
    pull(controller) {
      for (const b of blocks) {
        controller.enqueue(b);
      }
      controller.close();
    }
  });
  await blockStream.pipeThrough(new ShardingStream({ ...options, shardSize: Infinity })).pipeTo(new WritableStream({
    write: (c) => {
      car = c;
    }
  }));
  if (!car)
    throw new Error("missing CAR output");
  const root2 = car.roots[0];
  const bytes3 = new Uint8Array(await car.arrayBuffer());
  const digest4 = await sha2562.digest(bytes3);
  const [shardLink, indexLink] = await Promise.all([
    (async () => {
      var _a15;
      const conf2 = await configure6([
        {
          can: ability,
          nb: input(digest4, bytes3.length)
        }
      ]);
      await add8(conf2, digest4, bytes3, options);
      const cid = create2(code18, digest4);
      let piece;
      if (pieceHasher) {
        const multihashDigest = await pieceHasher.digest(bytes3);
        piece = create2(code3, multihashDigest);
        const result = await storefront_exports2.filecoinOffer({
          issuer: conf2.issuer,
          audience: conf2.audience,
          // Resource of invocation is the issuer did for being self issued
          with: conf2.issuer.did(),
          proofs: conf2.proofs
        }, create2(code3, digest4), piece, options);
        if (result.out.error) {
          throw new Error("failed to offer piece for aggregation into filecoin deal", { cause: result.out.error });
        }
      }
      const { version: version2, roots, size: size5, slices } = car;
      (_a15 = options.onShardStored) == null ? void 0 : _a15.call(options, { version: version2, roots, size: size5, piece, cid, slices });
      return cid;
    })(),
    (async () => {
      const index3 = sharded_dag_index_exports.create(root2);
      for (const [slice3, pos] of car.slices) {
        index3.setSlice(digest4, slice3, pos);
      }
      index3.setSlice(digest4, digest4, [0, car.size]);
      const indexBytes = await index3.archive();
      if (!indexBytes.ok) {
        throw new Error("failed to archive DAG index", {
          cause: indexBytes.error
        });
      }
      const indexDigest = await sha2562.digest(indexBytes.ok);
      const indexLink2 = create2(code18, indexDigest);
      const conf2 = await configure6([
        {
          can: ability,
          nb: input(indexDigest, indexBytes.ok.length)
        }
      ]);
      await add8(conf2, indexDigest, indexBytes.ok, options);
      return indexLink2;
    })()
  ]);
  await Promise.all([
    (async () => {
      const conf2 = await configure6([
        {
          can: ability6,
          nb: input6(indexLink)
        }
      ]);
      await add9(conf2, indexLink, options);
    })(),
    (async () => {
      const conf2 = await configure6([
        {
          can: ability7,
          nb: input7(root2, [shardLink])
        }
      ]);
      await add10(conf2, root2, [shardLink], options);
    })()
  ]);
  return root2;
}

// node_modules/environment/index.js
var _a3;
var isBrowser = ((_a3 = globalThis.window) == null ? void 0 : _a3.document) !== void 0;
var _a4, _b;
var isNode = ((_b = (_a4 = globalThis.process) == null ? void 0 : _a4.versions) == null ? void 0 : _b.node) !== void 0;
var _a5, _b2;
var isBun = ((_b2 = (_a5 = globalThis.process) == null ? void 0 : _a5.versions) == null ? void 0 : _b2.bun) !== void 0;
var _a6, _b3;
var isDeno = ((_b3 = (_a6 = globalThis.Deno) == null ? void 0 : _a6.version) == null ? void 0 : _b3.deno) !== void 0;
var _a7, _b4;
var isElectron = ((_b4 = (_a7 = globalThis.process) == null ? void 0 : _a7.versions) == null ? void 0 : _b4.electron) !== void 0;
var _a8, _b5;
var isJsDom = ((_b5 = (_a8 = globalThis.navigator) == null ? void 0 : _a8.userAgent) == null ? void 0 : _b5.includes("jsdom")) === true;
var isWebWorker = typeof WorkerGlobalScope !== "undefined" && globalThis instanceof WorkerGlobalScope;
var isDedicatedWorker = typeof DedicatedWorkerGlobalScope !== "undefined" && globalThis instanceof DedicatedWorkerGlobalScope;
var isSharedWorker = typeof SharedWorkerGlobalScope !== "undefined" && globalThis instanceof SharedWorkerGlobalScope;
var isServiceWorker = typeof ServiceWorkerGlobalScope !== "undefined" && globalThis instanceof ServiceWorkerGlobalScope;
var _a9, _b6;
var platform = (_b6 = (_a9 = globalThis.navigator) == null ? void 0 : _a9.userAgentData) == null ? void 0 : _b6.platform;
var _a10, _b7, _c, _d;
var isMacOs = platform === "macOS" || ((_a10 = globalThis.navigator) == null ? void 0 : _a10.platform) === "MacIntel" || ((_c = (_b7 = globalThis.navigator) == null ? void 0 : _b7.userAgent) == null ? void 0 : _c.includes(" Mac ")) === true || ((_d = globalThis.process) == null ? void 0 : _d.platform) === "darwin";
var _a11, _b8;
var isWindows = platform === "Windows" || ((_a11 = globalThis.navigator) == null ? void 0 : _a11.platform) === "Win32" || ((_b8 = globalThis.process) == null ? void 0 : _b8.platform) === "win32";
var _a12, _b9, _c2, _d2, _e;
var isLinux = platform === "Linux" || ((_b9 = (_a12 = globalThis.navigator) == null ? void 0 : _a12.platform) == null ? void 0 : _b9.startsWith("Linux")) === true || ((_d2 = (_c2 = globalThis.navigator) == null ? void 0 : _c2.userAgent) == null ? void 0 : _d2.includes(" Linux ")) === true || ((_e = globalThis.process) == null ? void 0 : _e.platform) === "linux";
var _a13, _b10, _c3;
var isIos = platform === "iOS" || ((_a13 = globalThis.navigator) == null ? void 0 : _a13.platform) === "MacIntel" && ((_b10 = globalThis.navigator) == null ? void 0 : _b10.maxTouchPoints) > 1 || /iPad|iPhone|iPod/.test((_c3 = globalThis.navigator) == null ? void 0 : _c3.platform);
var _a14, _b11, _c4, _d3;
var isAndroid = platform === "Android" || ((_a14 = globalThis.navigator) == null ? void 0 : _a14.platform) === "Android" || ((_c4 = (_b11 = globalThis.navigator) == null ? void 0 : _b11.userAgent) == null ? void 0 : _c4.includes(" Android ")) === true || ((_d3 = globalThis.process) == null ? void 0 : _d3.platform) === "android";

// node_modules/@storacha/client/dist/service.js
var accessServiceURL = new URL("https://up.storacha.network");
var accessServicePrincipal = parse2("did:web:up.storacha.network");
var envName = isBrowser ? "Browser" : isNode ? "Node" : isBun ? "Bun" : isDeno ? "Deno" : isElectron ? "Electron" : "Unknown";
var defaultHeaders = {
  "X-Client": `Storacha/1 (js; ${envName})`
};
var accessServiceConnection = (options = {}) => connect({
  id: options.id ?? accessServicePrincipal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: options.url ?? accessServiceURL,
    method: "POST",
    headers: { ...defaultHeaders, ...options.headers }
  })
});
var uploadServiceURL = new URL("https://up.storacha.network");
var uploadServicePrincipal = parse2("did:web:up.storacha.network");
var uploadServiceConnection = (options = {}) => connect({
  id: options.id ?? uploadServicePrincipal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: options.url ?? uploadServiceURL,
    method: "POST",
    headers: { ...defaultHeaders, ...options.headers }
  })
});
var filecoinServiceURL = new URL("https://up.storacha.network");
var filecoinServicePrincipal = parse2("did:web:up.storacha.network");
var filecoinServiceConnection = (options = {}) => connect({
  id: options.id ?? filecoinServicePrincipal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: options.url ?? filecoinServiceURL,
    method: "POST",
    headers: { ...defaultHeaders, ...options.headers }
  })
});
var gatewayServiceURL = new URL("https://gateway.storacha.network");
var gatewayServicePrincipal = parse2("did:web:w3s.link");
var gatewayServiceConnection = ({ id, url } = {}) => connect({
  id: id ?? gatewayServicePrincipal,
  codec: car_exports2.outbound,
  channel: http_exports.open({
    url: url ?? gatewayServiceURL,
    method: "POST"
  })
});
var serviceConf = () => ({
  access: accessServiceConnection(),
  upload: uploadServiceConnection(),
  filecoin: filecoinServiceConnection(),
  gateway: gatewayServiceConnection()
});

// node_modules/@storacha/client/dist/base.js
var Base = class {
  /**
   * @param {import('@storacha/access').AgentData} agentData
   * @param {object} [options]
   * @param {import('./types.js').ServiceConf} [options.serviceConf]
   * @param {URL} [options.receiptsEndpoint]
   */
  constructor(agentData, options = {}) {
    /**
     * @type {Agent}
     * @protected
     */
    __publicField(this, "_agent");
    /**
     * @type {import('./types.js').ServiceConf}
     * @protected
     */
    __publicField(this, "_serviceConf");
    this._serviceConf = options.serviceConf ?? serviceConf();
    this._agent = new Agent(agentData, {
      servicePrincipal: this._serviceConf.access.id,
      // @ts-expect-error I know but it will be HTTP for the forseeable.
      url: this._serviceConf.access.channel.url,
      connection: this._serviceConf.access
    });
    this._receiptsEndpoint = options.receiptsEndpoint ?? receiptsEndpoint;
  }
  /**
   * The current user agent (this device).
   *
   * @type {Agent}
   */
  get agent() {
    return this._agent;
  }
  /**
   * @protected
   * @param {import('./types.js').Ability[]} abilities
   */
  async _invocationConfig(abilities) {
    const resource = this._agent.currentSpace();
    if (!resource) {
      throw new Error("missing current space: use createSpace() or setCurrentSpace()");
    }
    const issuer = this._agent.issuer;
    const proofs2 = await this._agent.proofs(abilities.map((can) => ({ can, with: resource })));
    const audience = this._serviceConf.upload.id;
    return { issuer, with: resource, proofs: proofs2, audience };
  }
};

// node_modules/@storacha/client/dist/account.js
var account_exports = {};
__export(account_exports, {
  Account: () => Account2,
  AccountPlan: () => AccountPlan,
  externalLogin: () => externalLogin,
  fromEmail: () => fromEmail,
  list: () => list9,
  login: () => login
});

// node_modules/@storacha/client/dist/types.js
var types_exports2 = {};
__export(types_exports2, {
  AppName: () => AppName,
  Client: () => Client,
  email: () => email,
  fromEmail: () => fromEmail,
  fromString: () => fromString3,
  toEmail: () => toEmail
});
__reExport(types_exports2, __toESM(require_lib(), 1));

// node_modules/@storacha/client/dist/capability/access.js
var access_exports3 = {};
__export(access_exports3, {
  AccessClient: () => AccessClient,
  DIDMailto: () => dist_exports,
  accountAccess: () => accountAccess2,
  claim: () => claim4,
  createPendingAccessRequest: () => createPendingAccessRequest2,
  delegate: () => delegate5,
  request: () => request2,
  spaceAccess: () => spaceAccess2
});

// node_modules/@storacha/client/dist/result.js
var result_exports = {};
__export(result_exports, {
  Failure: () => Failure,
  error: () => error,
  fail: () => fail2,
  ok: () => ok,
  panic: () => panic,
  try: () => unwrap,
  unwrap: () => unwrap
});
var API34 = __toESM(require_lib(), 1);
var unwrap = ({ ok: ok2, error: error4 }) => {
  if (error4) {
    throw error4;
  } else {
    return (
      /** @type {T} */
      ok2
    );
  }
};

// node_modules/@storacha/client/dist/capability/access.js
var AccessClient = class extends Base {
  /* c8 ignore start - testing websocket code is hard */
  /**
   * Authorize the current agent to use capabilities granted to the passed
   * email account.
   *
   * @deprecated Use `request` instead.
   *
   * @param {`${string}@${string}`} email
   * @param {object} [options]
   * @param {AbortSignal} [options.signal]
   * @param {Iterable<{ can: API.Ability }>} [options.capabilities]
   */
  async authorize(email2, options) {
    const account = fromEmail(email2);
    const authorization = unwrap(await request2(this, { account }));
    const access2 = unwrap(await authorization.claim(options));
    await unwrap(await access2.save());
    return access2.proofs;
  }
  /* c8 ignore stop */
  /**
   * Claim delegations granted to the account associated with this agent.
   *
   * @param {object} [input]
   * @param {API.DID} [input.audience]
   */
  async claim(input11) {
    const access2 = unwrap(await claim4(this, input11));
    await unwrap(await access2.save());
    return access2.proofs;
  }
  /**
   * Requests specified `access` level from the account from the given account.
   *
   * @param {object} input
   * @param {API.AccountDID} input.account
   * @param {API.Access} [input.access]
   * @param {AbortSignal} [input.signal]
   */
  async request(input11) {
    return await request2(this, input11);
  }
  /**
   * Shares access with delegates.
   *
   * @param {object} input
   * @param {API.Delegation[]} input.delegations
   * @param {API.SpaceDID} [input.space]
   * @param {API.Delegation[]} [input.proofs]
   */
  async delegate(input11) {
    return await delegate5(this, input11);
  }
};
var claim4 = async ({ agent }, input11) => access_exports2.claim(agent, input11);
var request2 = async ({ agent }, input11) => access_exports2.request(agent, input11);
var createPendingAccessRequest2 = ({ agent }, input11) => access_exports2.createPendingAccessRequest(agent, input11);
var delegate5 = async ({ agent }, input11) => access_exports2.delegate(agent, input11);
var { spaceAccess: spaceAccess2, accountAccess: accountAccess2 } = access_exports2;

// node_modules/@storacha/client/dist/capability/plan.js
var PlanClient = class extends Base {
  /**
   * Required delegated capabilities:
   * - `plan/get`
   *
   * @param {import('@storacha/access').AccountDID} account
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async get(account, options) {
    const out = await get18({ agent: this.agent }, { ...options, account });
    if (!out.ok) {
      throw new Error(`failed ${get7.can} invocation`, {
        cause: out.error
      });
    }
    return out.ok;
  }
  /**
   * Required delegated capabilities:
   * - `plan/set`
   *
   * @param {API.AccountDID} account
   * @param {API.DID} product
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async set(account, product, options) {
    const out = await set6({ agent: this.agent }, { ...options, account, product });
    if (!out.ok) {
      throw new Error(`failed ${set.can} invocation`, {
        cause: out.error
      });
    }
    return out.ok;
  }
  /**
   *
   * @param {API.AccountDID} account
   * @param {string} returnURL
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async createAdminSession(account, returnURL, options) {
    const out = await createAdminSession2({ agent: this.agent }, { ...options, account, returnURL });
    if (!out.ok) {
      throw new Error(`failed ${createAdminSession.can} invocation`, {
        cause: out.error
      });
    }
    return out.ok;
  }
};
var get18 = async ({ agent }, { account, nonce, proofs: proofs2 = [] }) => {
  const receipt = await agent.invokeAndExecute(get7, {
    with: account,
    proofs: proofs2,
    nonce
  });
  return receipt.out;
};
var set6 = async ({ agent }, { account, product, nonce, proofs: proofs2 = [] }) => {
  const receipt = await agent.invokeAndExecute(set, {
    with: account,
    nb: { product },
    nonce,
    proofs: proofs2
  });
  return receipt.out;
};
var createAdminSession2 = async ({ agent }, { account, returnURL, nonce, proofs: proofs2 = [] }) => {
  const receipt = await agent.invokeAndExecute(createAdminSession, {
    with: account,
    proofs: proofs2,
    nonce,
    nb: {
      returnURL
    }
  });
  return receipt.out;
};

// node_modules/@storacha/client/dist/capability/subscription.js
var SubscriptionClient = class extends Base {
  /**
   * List subscriptions for the passed account.
   *
   * Required delegated capabilities:
   * - `subscription/list`
   *
   * @param {import('@storacha/access').AccountDID} account
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  /* c8 ignore next */
  async list(account, options) {
    const out = await list8({ agent: this.agent }, { ...options, account });
    if (!out.ok) {
      throw new Error(`failed ${subscription_exports.list.can} invocation`, {
        cause: out.error
      });
    }
    return out.ok;
  }
};
var list8 = async ({ agent }, { account, nonce, proofs: proofs2 = [] }) => {
  const receipt = await agent.invokeAndExecute(subscription_exports.list, {
    with: account,
    proofs: proofs2,
    nb: void 0,
    nonce
  });
  return receipt.out;
};

// node_modules/@storacha/client/dist/account.js
var list9 = ({ agent }, { account } = {}) => {
  const query = (
    /** @type {API.CapabilityQuery} */
    {
      with: account ?? /did:mailto:.*/,
      can: "*"
    }
  );
  const proofs2 = agent.proofs([query]);
  const accounts = {};
  const attestations = {};
  for (const proof of proofs2) {
    const access2 = delegation_exports.allows(proof);
    for (const [resource, abilities] of Object.entries(access2)) {
      if (AccountDID2.is(resource) && abilities["*"]) {
        const id = (
          /** @type {API.DidMailto} */
          resource
        );
        const account2 = accounts[id] || (accounts[id] = new Account2({ id, agent, proofs: [] }));
        account2.addProof(proof);
      }
      for (
        const settings of
        /** @type {{proof?:API.Link}[]} */
        abilities["ucan/attest"] || []
      ) {
        const id = settings.proof;
        if (id) {
          attestations[`${id}`] = proof;
        }
      }
    }
  }
  for (const account2 of Object.values(accounts)) {
    for (const proof of account2.proofs) {
      const attestation = attestations[`${proof.cid}`];
      if (attestation) {
        account2.addProof(attestation);
      }
    }
  }
  return accounts;
};
var login = async ({ agent }, email2, options = {}) => {
  const account = fromEmail(email2);
  const session2 = list9({ agent }, { account })[account];
  if (session2) {
    return { ok: session2 };
  }
  const result = await request2({ agent }, {
    account,
    access: accountAccess2,
    appName: options.appName,
    sso: options.sso
  });
  const { ok: access2, error: error4 } = result;
  if (error4) {
    return { error: error4 };
  } else {
    const { ok: ok2, error: error5 } = await access2.claim({ signal: options.signal });
    if (error5) {
      return { error: error5 };
    } else {
      return { ok: new Account2({ id: account, proofs: ok2.proofs, agent }) };
    }
  }
};
var externalLogin = async ({ agent }, { request: request3, expiration, ...options }) => {
  const access2 = createPendingAccessRequest2({ agent }, { request: request3, expiration });
  const { ok: ok2, error: error4 } = await access2.claim({ signal: options.signal });
  if (error4) {
    return { error: error4 };
  }
  let attestedProof;
  for (const p of ok2.proofs) {
    if (isUCANAttest(p)) {
      attestedProof = p.capabilities[0].nb.proof;
      break;
    }
  }
  if (!attestedProof) {
    return { error: new Error("missing attestation") };
  }
  let account;
  for (const p of ok2.proofs) {
    if (p.cid.toString() === attestedProof.toString()) {
      try {
        account = dist_exports.fromString(p.issuer.did());
      } catch (err) {
        return { error: new Error("invalid account DID", { cause: err }) };
      }
      break;
    }
  }
  if (!account) {
    return { error: new Error("missing attested delegation") };
  }
  return { ok: new Account2({ id: account, proofs: ok2.proofs, agent }) };
};
var isUCANAttest = (d) => d.capabilities[0].can === attest.can;
var Account2 = class {
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.model = model;
    this.plan = new AccountPlan(model);
  }
  get agent() {
    return this.model.agent;
  }
  get proofs() {
    return this.model.proofs;
  }
  did() {
    return this.model.id;
  }
  toEmail() {
    return toEmail(this.did());
  }
  /**
   * @param {API.Delegation} proof
   */
  addProof(proof) {
    this.proofs.push(proof);
  }
  toJSON() {
    return {
      id: this.did(),
      proofs: this.proofs.sort((a, b) => a.cid.toString().localeCompare(b.cid.toString())).map((proof) => proof.toJSON())
    };
  }
  /**
   * Provisions given `space` with this account.
   *
   * @param {API.SpaceDID} space
   * @param {object} input
   * @param {API.ProviderDID} [input.provider]
   * @param {API.Agent} [input.agent]
   */
  provision(space2, input11 = {}) {
    return add4(this.agent, {
      ...input11,
      account: this.did(),
      consumer: space2,
      proofs: this.proofs
    });
  }
  /**
   * Saves account in the agent store so it can be accessed across sessions.
   *
   * @param {object} input
   * @param {API.Agent} [input.agent]
   */
  async save({ agent = this.agent } = {}) {
    return await importAuthorization(agent, this);
  }
};
var AccountPlan = class {
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.model = model;
  }
  /**
   * Gets information about the plan associated with this account.
   *
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async get(options) {
    return await get18(this.model, {
      ...options,
      account: this.model.id,
      proofs: this.model.proofs
    });
  }
  /**
   * Sets the plan associated with this account.
   *
   * @param {import('@ucanto/interface').DID} productDID
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async set(productDID, options) {
    return await set6(this.model, {
      ...options,
      account: this.model.id,
      product: productDID,
      proofs: this.model.proofs
    });
  }
  /**
   * Waits for a payment plan to be selected.
   * This method continuously checks the account's payment plan status
   * at a specified interval until a valid plan is selected, or when the timeout is reached,
   * or when the abort signal is aborted.
   *
   * @param {object} [options]
   * @param {number} [options.interval] - The polling interval in milliseconds (default is 1000ms).
   * @param {number} [options.timeout] - The maximum time to wait in milliseconds before throwing a timeout error (default is 15 minutes).
   * @param {AbortSignal} [options.signal] - An optional AbortSignal to cancel the waiting process.
   * @returns {Promise<import('@storacha/access').PlanGetSuccess>} - Resolves once a payment plan is selected within the timeout.
   * @throws {Error} - Throws an error if there is an issue retrieving the payment plan or if the timeout is exceeded.
   */
  async wait(options) {
    var _a15;
    const startTime = Date.now();
    const interval = (options == null ? void 0 : options.interval) || 1e3;
    const timeout = (options == null ? void 0 : options.timeout) || 60 * 15 * 1e3;
    while (true) {
      const res = await this.get();
      if (res.ok)
        return res.ok;
      if (res.error) {
        if (res.error.name === "PlanNotFound") {
          continue;
        }
        throw new Error(`Error retrieving payment plan: ${JSON.stringify(res.error)}`);
      }
      if (Date.now() - startTime > timeout) {
        throw new Error("Timeout: Payment plan selection took too long.");
      }
      if ((_a15 = options == null ? void 0 : options.signal) == null ? void 0 : _a15.aborted) {
        throw new Error("Aborted: Payment plan selection was aborted.");
      }
      console.log("Waiting for payment plan to be selected...");
      await new Promise((resolve) => setTimeout(resolve, interval));
    }
  }
  /**
   *
   * @param {import('@storacha/access').AccountDID} accountDID
   * @param {string} returnURL
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async createAdminSession(accountDID, returnURL, options) {
    return await createAdminSession2(this.model, {
      ...options,
      account: accountDID,
      returnURL
    });
  }
  /**
   *
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async subscriptions(options) {
    return await list8(this.model, {
      ...options,
      account: this.model.id,
      proofs: this.model.proofs
    });
  }
};

// node_modules/@storacha/client/dist/capability/usage.js
var UsageClient = class extends Base {
  /**
   * Get a usage report for the passed space in the given time period.
   *
   * Required delegated capabilities:
   * - `usage/report`
   *
   * @param {import('../types.js').SpaceDID} space
   * @param {{ from: Date, to: Date }} period
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async report(space2, period, options) {
    const out = await report2({ agent: this.agent }, { ...options, space: space2, period });
    if (!out.ok) {
      throw new Error(`failed ${usage_exports.report.can} invocation`, {
        cause: out.error
      });
    }
    return out.ok;
  }
};
var report2 = async ({ agent }, { space: space2, period, nonce, proofs: proofs2 = [] }) => {
  const receipt = await agent.invokeAndExecute(usage_exports.report, {
    with: space2,
    proofs: proofs2,
    nonce,
    nb: {
      period: {
        from: Math.floor(period.from.getTime() / 1e3),
        to: Math.ceil(period.to.getTime() / 1e3)
      }
    }
  });
  return receipt.out;
};

// node_modules/@storacha/client/dist/space.js
var _model;
var Space = class {
  /**
   * @param {Model} model
   */
  constructor(model) {
    __privateAdd(this, _model);
    __privateSet(this, _model, model);
    this.usage = new StorageUsage(model);
  }
  /**
   * The given space name.
   */
  get name() {
    var _a15;
    return String(((_a15 = __privateGet(this, _model).meta) == null ? void 0 : _a15.name) ?? "");
  }
  get access() {
    var _a15;
    return ((_a15 = __privateGet(this, _model).meta) == null ? void 0 : _a15.access) ?? { type: "public" };
  }
  /**
   * The DID of the space.
   */
  did() {
    return __privateGet(this, _model).id;
  }
  /**
   * User defined space metadata.
   */
  meta() {
    return __privateGet(this, _model).meta;
  }
};
_model = new WeakMap();
var _model2;
var StorageUsage = class {
  /**
   * @param {Model} model
   */
  constructor(model) {
    __privateAdd(this, _model2);
    __privateSet(this, _model2, model);
  }
  /**
   * Get the current usage in bytes.
   */
  async get() {
    const { agent } = __privateGet(this, _model2);
    const space2 = __privateGet(this, _model2).id;
    const now2 = /* @__PURE__ */ new Date();
    const period = {
      // we may not have done a snapshot for this month _yet_, so get report
      // from last month -> now
      from: startOfLastMonth(now2),
      to: now2
    };
    const result = await report2({ agent }, { space: space2, period });
    if (result.error)
      return result;
    const provider = (
      /** @type {API.ProviderDID} */
      agent.connection.id.did()
    );
    const report3 = result.ok[provider];
    return {
      /* c8 ignore next */
      ok: (report3 == null ? void 0 : report3.size.final) == null ? void 0 : BigInt(report3.size.final)
    };
  }
};
_model2 = new WeakMap();
var startOfMonth = (now2) => {
  const d = new Date(now2);
  d.setUTCDate(1);
  d.setUTCHours(0);
  d.setUTCMinutes(0);
  d.setUTCSeconds(0);
  d.setUTCMilliseconds(0);
  return d;
};
var startOfLastMonth = (now2) => {
  const d = startOfMonth(now2);
  d.setUTCMonth(d.getUTCMonth() - 1);
  return d;
};

// node_modules/@storacha/client/dist/delegation.js
var _meta;
var AgentDelegation = class extends Delegation {
  /**
   * @param {import('./types.js').UCANBlock<C>} root
   * @param {Map<string, import('./types.js').Block>} [blocks]
   * @param {Record<string, any>} [meta]
   */
  constructor(root2, blocks, meta = {}) {
    super(root2, blocks);
    /* c8 ignore stop */
    /** @type {Record<string, any>} */
    __privateAdd(this, _meta);
    __privateSet(this, _meta, meta);
  }
  /**
   * User defined delegation metadata.
   */
  meta() {
    return __privateGet(this, _meta);
  }
};
_meta = new WeakMap();

// node_modules/@storacha/client/dist/capability/blob.js
var BlobClient = class extends Base {
  /**
   * Store a Blob to the resource.
   *
   * Required delegated capabilities:
   * - `space/blob/add`
   *
   * @param {Blob} blob - blob data.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async add(blob4, options = {}) {
    options = {
      receiptsEndpoint: this._receiptsEndpoint.toString(),
      connection: this._serviceConf.upload,
      ...options
    };
    const conf = await this._invocationConfig([blob_exports.add.can]);
    const bytes3 = new Uint8Array(await blob4.arrayBuffer());
    const digest4 = await sha2562.digest(bytes3);
    return { digest: digest4, ...await blob_exports4.add(conf, digest4, bytes3, options) };
  }
  /**
   * List blobs stored to the resource.
   *
   * Required delegated capabilities:
   * - `space/blob/list`
   *
   * @param {import('../types.js').ListRequestOptions} [options]
   */
  async list(options = {}) {
    const conf = await this._invocationConfig([blob_exports.list.can]);
    options.connection = this._serviceConf.upload;
    return blob_exports4.list(conf, options);
  }
  /**
   * Remove a stored blob by multihash digest.
   *
   * Required delegated capabilities:
   * - `space/blob/remove`
   *
   * @param {import('multiformats').MultihashDigest} digest - digest of blob to remove.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async remove(digest4, options = {}) {
    const conf = await this._invocationConfig([blob_exports.remove.can]);
    options.connection = this._serviceConf.upload;
    return blob_exports4.remove(conf, digest4, options);
  }
  /**
   * Gets a stored blob by multihash digest.
   *
   * @param {import('multiformats').MultihashDigest} digest - digest of blob to get.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async get(digest4, options = {}) {
    const conf = await this._invocationConfig([blob_exports.get.can]);
    options.connection = this._serviceConf.upload;
    return blob_exports4.get(conf, digest4, options);
  }
  /**
   * Replicate a blob to the specified number of nodes.
   *
   * @param {object} blob - details of the blob to replicate
   * @param {import('multiformats').MultihashDigest} blob.digest - hash of the blob
   * @param {number} blob.size - size of the blob in bytes
   * @param {import('../types.js').Delegation<[AssertLocation]>} site - location commitment specifying where the blob can be obtained.
   * @param {number} replicas - total number of replicas to provision.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async replicate(blob4, site, replicas, options = {}) {
    const conf = await this._invocationConfig([blob_exports.replicate.can]);
    options.connection = this._serviceConf.upload;
    return blob_exports4.replicate(conf, blob4, site, replicas, options);
  }
};

// node_modules/@storacha/client/dist/capability/index.js
var IndexClient = class extends Base {
  /**
   * Register an "index" to the resource.
   *
   * Required delegated capabilities:
   * - `space/index/add`
   *
   * @param {import('../types.js').CARLink} index - CID of the CAR file that contains the index data.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async add(index3, options = {}) {
    const conf = await this._invocationConfig([space_exports3.add.can]);
    options.connection = this._serviceConf.upload;
    return index_exports.add(conf, index3, options);
  }
};

// node_modules/@storacha/client/dist/capability/upload.js
var UploadClient = class extends Base {
  /**
   * Register an "upload" to the resource.
   *
   * Required delegated capabilities:
   * - `upload/add`
   *
   * @param {import('../types.js').UnknownLink} root - Root data CID for the DAG that was stored.
   * @param {import('../types.js').CARLink[]} shards - CIDs of CAR files that contain the DAG.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async add(root2, shards, options = {}) {
    const conf = await this._invocationConfig([upload_exports.add.can]);
    options.connection = this._serviceConf.upload;
    return upload_exports2.add(conf, root2, shards, options);
  }
  /**
   * Get details of an "upload".
   *
   * Required delegated capabilities:
   * - `upload/get`
   *
   * @param {import('../types.js').UnknownLink} root - Root data CID for the DAG that was stored.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async get(root2, options = {}) {
    const conf = await this._invocationConfig([upload_exports.get.can]);
    options.connection = this._serviceConf.upload;
    return upload_exports2.get(conf, root2, options);
  }
  /**
   * List uploads registered to the resource.
   *
   * Required delegated capabilities:
   * - `upload/list`
   *
   * @param {import('../types.js').ListRequestOptions} [options]
   */
  async list(options = {}) {
    const conf = await this._invocationConfig([upload_exports.list.can]);
    options.connection = this._serviceConf.upload;
    return upload_exports2.list(conf, options);
  }
  /**
   * Remove an upload by root data CID.
   *
   * Required delegated capabilities:
   * - `upload/remove`
   *
   * @param {import('../types.js').UnknownLink} root - Root data CID to remove.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async remove(root2, options = {}) {
    const conf = await this._invocationConfig([upload_exports.remove.can]);
    options.connection = this._serviceConf.upload;
    return upload_exports2.remove(conf, root2, options);
  }
};

// node_modules/@storacha/client/dist/capability/space.js
var SpaceClient = class extends Base {
  /**
   * Get information about a space.
   *
   * Required delegated capabilities:
   * - `space/info`
   *
   * @param {import('../types.js').DID} space - DID of the space to retrieve info about.
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async info(space2, options) {
    return await this._agent.getSpaceInfo(space2, options);
  }
  /**
   * Record egress data for a served resource.
   * It will execute the capability invocation to find the customer and then record the egress data for the resource.
   *
   * Required delegated capabilities:
   * - `space/content/serve/egress/record`
   *
   * @param {object} egressData
   * @param {import('../types.js').SpaceDID} egressData.space
   * @param {API.UnknownLink} egressData.resource
   * @param {number} egressData.bytes
   * @param {string} egressData.servedAt
   * @param {object} [options]
   * @param {string} [options.nonce]
   * @param {API.Delegation[]} [options.proofs]
   * @returns {Promise<API.EgressRecordSuccess>}
   */
  async egressRecord(egressData, options) {
    const out = await egressRecord2({ agent: this.agent }, { ...egressData }, { ...options });
    if (!out.ok) {
      throw new Error(`failed ${space_exports.egressRecord.can} invocation`, {
        cause: out.error
      });
    }
    return (
      /** @type {API.EgressRecordSuccess} */
      out.ok
    );
  }
};
var egressRecord2 = async ({ agent }, { space: space2, resource, bytes: bytes3, servedAt }, { nonce, proofs: proofs2 = [] }) => {
  const receipt = await agent.invokeAndExecute(space_exports.egressRecord, {
    with: space2,
    proofs: proofs2,
    nonce,
    nb: {
      resource,
      bytes: bytes3,
      servedAt: Math.floor(new Date(servedAt).getTime() / 1e3)
    }
  });
  return receipt.out;
};

// node_modules/@storacha/client/dist/capability/filecoin.js
var FilecoinClient = class extends Base {
  /**
   * Offer a Filecoin "piece" to the resource.
   *
   * Required delegated capabilities:
   * - `filecoin/offer`
   *
   * @param {import('multiformats').UnknownLink} content
   * @param {import('@storacha/capabilities/types').PieceLink} piece
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async offer(content2, piece, options) {
    const conf = await this._invocationConfig([filecoin_exports.offer.can]);
    return storefront_exports2.filecoinOffer(conf, content2, piece, {
      ...options,
      connection: this._serviceConf.filecoin
    });
  }
  /**
   * Request info about a content piece in Filecoin deals
   *
   * Required delegated capabilities:
   * - `filecoin/info`
   *
   * @param {import('@storacha/capabilities/types').PieceLink} piece
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async info(piece, options) {
    const conf = await this._invocationConfig([filecoin_exports.info.can]);
    return storefront_exports2.filecoinInfo(conf, piece, {
      ...options,
      connection: this._serviceConf.filecoin
    });
  }
};

// node_modules/@storacha/client/dist/coupon.js
var CouponAPI = class extends Base {
  /**
   * Redeems coupon from the the the archive. Throws an error if the coupon
   * password is invalid or if provided archive is not a valid.
   *
   * @param {Uint8Array} archive
   * @param {object} [options]
   * @param {string} [options.password]
   */
  async redeem(archive4, options = {}) {
    const { agent } = this;
    const coupon = unwrap(await extract3(archive4));
    return unwrap(await redeem(coupon, { ...options, agent }));
  }
  /**
   * Issues a coupon for the given delegation.
   *
   * @param {Omit<CouponOptions, 'issuer'>} options
   */
  async issue({ proofs: proofs2 = [], ...options }) {
    const { agent } = this;
    return await issue3({
      ...options,
      issuer: agent.issuer,
      proofs: [...proofs2, ...agent.proofs(options.capabilities)]
    });
  }
};
var extract3 = async (archive4) => {
  const { ok: ok2, error: error4 } = await delegation_exports.extract(archive4);
  return ok2 ? ok(new Coupon({ proofs: [ok2] })) : error(error4);
};
var archive3 = async (coupon) => {
  const [delegation] = coupon.proofs;
  return await delegation_exports.archive(delegation);
};
var issue3 = async ({ password = "", ...options }) => {
  const audience = await deriveSigner(password);
  const delegation = await delegate({
    ...options,
    audience
  });
  return new Coupon({ proofs: [delegation] });
};
var redeem = async (coupon, { agent, password = "" }) => {
  const audience = await deriveSigner(password);
  const [delegation] = coupon.proofs;
  if (delegation.audience.did() !== audience.did()) {
    return error(new RangeError(password === "" ? "Extracting account requires a password" : "Provided password is invalid"));
  } else {
    const authorization = await delegate({
      issuer: audience,
      audience: agent,
      capabilities: delegation.capabilities,
      expiration: delegation.expiration,
      notBefore: delegation.notBefore,
      proofs: [delegation]
    });
    return ok(new GrantedAccess({ agent, proofs: [authorization] }));
  }
};
var deriveSigner = async (password) => {
  const { digest: digest4 } = await sha2562.digest(new TextEncoder().encode(password));
  return await ed25519_exports.Signer.derive(digest4);
};
var Coupon = class {
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.model = model;
  }
  get proofs() {
    return this.model.proofs;
  }
  /**
   *
   * @param {API.Agent} agent
   * @param {object} [options]
   * @param {string} [options.password]
   */
  redeem(agent, options = {}) {
    return redeem(this, { ...options, agent });
  }
  archive() {
    return archive3(this);
  }
};

// node_modules/@storacha/client/dist/client.js
var Client = class extends Base {
  /**
   * @param {import('@storacha/access').AgentData} agentData
   * @param {object} [options]
   * @param {import('./types.js').ServiceConf} [options.serviceConf]
   * @param {URL} [options.receiptsEndpoint]
   */
  constructor(agentData, options) {
    super(agentData, options);
    this.capability = {
      access: new AccessClient(agentData, options),
      filecoin: new FilecoinClient(agentData, options),
      index: new IndexClient(agentData, options),
      plan: new PlanClient(agentData, options),
      space: new SpaceClient(agentData, options),
      blob: new BlobClient(agentData, options),
      subscription: new SubscriptionClient(agentData, options),
      upload: new UploadClient(agentData, options),
      usage: new UsageClient(agentData, options)
    };
    this.coupon = new CouponAPI(agentData, options);
  }
  did() {
    return this._agent.did();
  }
  /* c8 ignore start */
  /**
   * @deprecated - Use client.login instead.
   *
   * Authorize the current agent to use capabilities granted to the passed
   * email account.
   *
   * @param {`${string}@${string}`} email
   * @param {object} [options]
   * @param {AbortSignal} [options.signal]
   * @param {Iterable<{ can: import('./types.js').Ability }>} [options.capabilities]
   */
  async authorize(email2, options) {
    await this.capability.access.authorize(email2, options);
  }
  /* c8 ignore stop */
  /**
   * @param {Account.EmailAddress} email
   * @param {object} [options]
   * @param {AbortSignal} [options.signal]
   * @param {import('@storacha/client/types').AppName} [options.appName]
   * @param {import('@storacha/client/types').SSORequestParams} [options.sso] - SSO authentication request (all fields required if provided)
   */
  async login(email2, options = {}) {
    const account = unwrap(await login(this, email2, options));
    unwrap(await account.save());
    return account;
  }
  /**
   * List all accounts that agent has stored access to.
   *
   * @returns {Record<DIDMailto.DidMailto, Account.Account>} A dictionary with `did:mailto` as keys and `Account` instances as values.
   */
  accounts() {
    return list9(this);
  }
  /**
   * Uploads a file to the service and returns the root data CID for the
   * generated DAG.
   *
   * Required delegated capabilities:
   * - `filecoin/offer`
   * - `space/blob/add`
   * - `space/index/add`
   * - `upload/add`
   *
   * @param {import('./types.js').BlobLike} file - File data.
   * @param {import('./types.js').UploadFileOptions} [options]
   */
  async uploadFile(file, options = {}) {
    const conf = await this._invocationConfig([
      blob_exports.add.can,
      space_exports3.add.can,
      filecoin_exports.offer.can,
      upload_exports.add.can
    ]);
    options = {
      receiptsEndpoint: this._receiptsEndpoint.toString(),
      connection: this._serviceConf.upload,
      ...options
    };
    return uploadFile(conf, file, options);
  }
  /**
   * Uploads a directory of files to the service and returns the root data CID
   * for the generated DAG. All files are added to a container directory, with
   * paths in the file names preserved.
   *
   * Required delegated capabilities:
   * - `filecoin/offer`
   * - `space/blob/add`
   * - `space/index/add`
   * - `upload/add`
   *
   * @param {import('./types.js').FileLike[]} files - File data.
   * @param {import('./types.js').UploadDirectoryOptions} [options]
   */
  async uploadDirectory(files, options = {}) {
    const conf = await this._invocationConfig([
      blob_exports.add.can,
      space_exports3.add.can,
      filecoin_exports.offer.can,
      upload_exports.add.can
    ]);
    options = {
      receiptsEndpoint: this._receiptsEndpoint.toString(),
      connection: this._serviceConf.upload,
      ...options
    };
    return uploadDirectory(conf, files, options);
  }
  /**
   * Uploads a CAR file to the service.
   *
   * The difference between this function and `capability.blob.add` is that
   * the CAR file is automatically sharded, an index is generated, uploaded and
   * registered (see `capability.index.add`) and finally an an "upload" is
   * registered, linking the individual shards (see `capability.upload.add`).
   *
   * Use the `onShardStored` callback to obtain the CIDs of the CAR file shards.
   *
   * Required delegated capabilities:
   * - `filecoin/offer`
   * - `space/blob/add`
   * - `space/index/add`
   * - `upload/add`
   *
   * @param {import('./types.js').BlobLike} car - CAR file.
   * @param {import('./types.js').UploadOptions} [options]
   */
  async uploadCAR(car, options = {}) {
    const conf = await this._invocationConfig([
      blob_exports.add.can,
      space_exports3.add.can,
      filecoin_exports.offer.can,
      upload_exports.add.can
    ]);
    options = {
      receiptsEndpoint: this._receiptsEndpoint.toString(),
      connection: this._serviceConf.upload,
      ...options
    };
    return uploadCAR(conf, car, options);
  }
  /**
   * Get a receipt for an executed task by its CID.
   *
   * @template {import('./types.js').Capability} C
   * @template {Record<string, any>} S
   * @param {import('./types.js').UCANLink<[C]>} taskCid
   * @param {import('./types.js').ReceiptGetOptions<S> & import('./types.js').Retryable} [options]
   * @returns {Promise<import('./types.js').InferReceipt<C, S>>}
   */
  async getReceipt(taskCid, options) {
    return receipts_exports.poll(taskCid, {
      endpoint: new URL(this._receiptsEndpoint),
      ...options
    });
  }
  /**
   * Return the default provider.
   */
  defaultProvider() {
    return this._agent.connection.id.did();
  }
  /**
   * The current space.
   */
  currentSpace() {
    const agent = this._agent;
    const id = agent.currentSpace();
    if (!id)
      return;
    const meta = agent.spaces.get(id);
    return new Space({ id, meta, agent });
  }
  /**
   * Use a specific space.
   *
   * @param {import('./types.js').DID} did
   */
  async setCurrentSpace(did2) {
    await this._agent.setCurrentSpace(
      /** @type {`did:key:${string}`} */
      did2
    );
  }
  /**
   * Spaces available to this agent.
   */
  spaces() {
    return [...this._agent.spaces].map(([id, meta]) => {
      return new Space({ id, meta, agent: this._agent });
    });
  }
  /**
   * Creates a new space with a given name.
   * If an account is not provided, the space is created without any delegation and is not saved, hence it is a temporary space.
   * When an account is provided in the options argument, then it creates a delegated recovery account
   * by provisioning the space, saving it and then delegating access to the recovery account.
   * In addition, it authorizes the listed Gateway Services to serve content from the created space.
   * It is done by delegating the `space/content/serve/*` capability to the Gateway Service.
   * User can skip the Gateway authorization by setting the `skipGatewayAuthorization` option to `true`.
   * If no gateways are specified or the `skipGatewayAuthorization` flag is not set, the client will automatically grant access
   * to the Storacha Gateway by default (https://w3s.link/).
   *
   * @typedef {import('./types.js').ConnectionView<import('./types.js').ContentServeService>} ConnectionView
   *
   * @typedef {object} SpaceCreateOptions
   * @property {Account.Account} [account] - The account configured as the recovery account for the space.
   * @property {Array<ConnectionView>} [authorizeGatewayServices] - The DID Key or DID Web of the Gateway to authorize to serve content from the created space.
   * @property {boolean} [skipGatewayAuthorization] - Whether to skip the Gateway authorization. It means that the content of the space will not be served by any Gateway.
   * @property {import('@storacha/access').SpaceAccessType} [access] - Access type for the space - determines client-side encryption behavior.
   *
   * @param {string} name - The name of the space to create.
   * @param {SpaceCreateOptions} [options] - Options for the space creation.
   * @returns {Promise<import("./space.js").OwnedSpace>} The created space owned by the agent.
   */
  async createSpace(name15, options = {}) {
    const { access: access2, account, skipGatewayAuthorization, authorizeGatewayServices } = options;
    const space2 = await this._agent.createSpace(name15, { access: access2 });
    if (account) {
      const provisionResult = await account.provision(space2.did());
      if (provisionResult.error) {
        throw new Error(`failed to provision account: ${provisionResult.error.message}`, { cause: provisionResult.error });
      }
      await space2.save();
      const recovery = await space2.createRecovery(account.did());
      const delegationResult = await this.capability.access.delegate({
        space: space2.did(),
        delegations: [recovery]
      });
      if (delegationResult.error) {
        throw new Error(`failed to authorize recovery account: ${delegationResult.error.message}`, { cause: delegationResult.error });
      }
    }
    if (skipGatewayAuthorization !== true) {
      let gatewayServices = authorizeGatewayServices;
      if (!gatewayServices || gatewayServices.length === 0) {
        gatewayServices = [this._serviceConf.gateway];
      }
      await space2.save();
      for (const serviceConnection of gatewayServices) {
        await authorizeContentServe(this, space2, serviceConnection);
      }
    }
    return space2;
  }
  /**
   * Share an existing space with another Storacha account via email address delegation.
   * Delegates access to the space to the specified email account with the following permissions:
   * - space/* - for managing space metadata
   * - blob/* - for managing blobs
   * - store/* - for managing stores
   * - upload/*- for registering uploads
   * - access/* - for re-delegating access to other devices
   * - filecoin/* - for submitting to the filecoin pipeline
   * - usage/* - for querying usage
   * The default expiration is set to infinity.
   *
   * @typedef {object} ShareOptions
   * @property {import('./types.js').ServiceAbility[]} abilities - Abilities to delegate to the delegate account.
   * @property {number} expiration - Expiration time in seconds.
   
   * @param {import("./types.js").EmailAddress} delegateEmail - Email of the account to share the space with.
   * @param {import('./types.js').SpaceDID} spaceDID - The DID of the space to share.
   * @param {ShareOptions} [options] - Options for the delegation.
   *
   * @returns {Promise<import('./delegation.js').AgentDelegation<any>>} Resolves with the AgentDelegation instance once the space is successfully shared.
   * @throws {Error} - Throws an error if there is an issue delegating access to the space.
   */
  async shareSpace(delegateEmail, spaceDID, options = {
    abilities: [
      "space/*",
      "store/*",
      "upload/*",
      "access/*",
      "usage/*",
      "filecoin/*"
    ],
    expiration: Infinity
  }) {
    const { abilities, ...restOptions } = options;
    const currentSpace = this.agent.currentSpace();
    try {
      await this.agent.setCurrentSpace(spaceDID);
      const { root: root2, blocks } = await this.agent.delegate({
        ...restOptions,
        abilities,
        audience: {
          did: () => fromEmail(email(delegateEmail))
        },
        // @ts-expect-error audienceMeta is not defined in ShareOptions
        audienceMeta: options.audienceMeta ?? {}
      });
      const delegation = new AgentDelegation(root2, blocks, {
        audience: delegateEmail
      });
      const sharingResult = await this.capability.access.delegate({
        space: spaceDID,
        delegations: [delegation]
      });
      if (sharingResult.error) {
        throw new Error(`failed to share space with ${delegateEmail}: ${sharingResult.error.message}`, {
          cause: sharingResult.error
        });
      }
      return delegation;
    } finally {
      if (currentSpace && currentSpace !== spaceDID) {
        await this.agent.setCurrentSpace(currentSpace);
      }
    }
  }
  /* c8 ignore stop */
  /**
   * Add a space from a received proof.
   *
   * @param {import('./types.js').Delegation} proof
   */
  async addSpace(proof) {
    return await this._agent.importSpaceFromDelegation(proof);
  }
  /**
   * Get all the proofs matching the capabilities.
   *
   * Proofs are delegations with an _audience_ matching the agent DID.
   *
   * @param {import('./types.js').Capability[]} [caps] - Capabilities to
   * filter by. Empty or undefined caps with return all the proofs.
   */
  proofs(caps) {
    return this._agent.proofs(caps);
  }
  /**
   * Add a proof to the agent. Proofs are delegations with an _audience_
   * matching the agent DID.
   *
   * @param {import('./types.js').Delegation} proof
   */
  async addProof(proof) {
    await this._agent.addProof(proof);
  }
  /**
   * Get delegations created by the agent for others.
   *
   * @param {import('./types.js').Capability[]} [caps] - Capabilities to
   * filter by. Empty or undefined caps with return all the delegations.
   */
  delegations(caps) {
    const delegations = [];
    for (const { delegation, meta } of this._agent.delegationsWithMeta(caps)) {
      delegations.push(new AgentDelegation(delegation.root, delegation.blocks, meta));
    }
    return delegations;
  }
  /**
   * Create a delegation to the passed audience for the given abilities with
   * the _current_ space as the resource.
   *
   * @param {import('./types.js').Principal} audience
   * @param {import('./types.js').ServiceAbility[]} abilities
   * @param {Omit<import('./types.js').UCANOptions, 'audience'> & { audienceMeta?: import('./types.js').AgentMeta }} [options]
   */
  async createDelegation(audience, abilities, options = {}) {
    const audienceMeta = options.audienceMeta ?? {
      name: "agent",
      type: "device"
    };
    const { root: root2, blocks } = await this._agent.delegate({
      ...options,
      abilities,
      audience,
      audienceMeta
    });
    return new AgentDelegation(root2, blocks, { audience: audienceMeta });
  }
  /**
   * Revoke a delegation by CID.
   *
   * If the delegation was issued by this agent (and therefore is stored in the
   * delegation store) you can just pass the CID. If not, or if the current agent's
   * delegation store no longer contains the delegation, you MUST pass a chain of
   * proofs that proves your authority to revoke this delegation as `options.proofs`.
   *
   * @param {import('@ucanto/interface').UCANLink} delegationCID
   * @param {object} [options]
   * @param {import('@ucanto/interface').Delegation[]} [options.proofs]
   */
  async revokeDelegation(delegationCID, options = {}) {
    return this._agent.revoke(delegationCID, {
      proofs: options.proofs
    });
  }
  /**
   * Removes association of a content CID with the space. Optionally, also removes
   * association of CAR shards with space.
   *
   * ⚠️ If `shards` option is `true` all shards will be deleted even if there is another upload(s) that
   * reference same shards, which in turn could corrupt those uploads.
   *
   * Required delegated capabilities:
   * - `space/blob/remove`
   * - `store/remove`
   * - `upload/get`
   * - `upload/remove`
   *
   * @param {import('multiformats').UnknownLink} contentCID
   * @param {object} [options]
   * @param {boolean} [options.shards]
   */
  async remove(contentCID, options = {}) {
    var _a15;
    if (!options.shards) {
      await this.capability.upload.remove(contentCID);
      return;
    }
    const upload3 = await this.capability.upload.get(contentCID);
    if ((_a15 = upload3.shards) == null ? void 0 : _a15.length) {
      await Promise.allSettled(upload3.shards.map((shard) => this.capability.blob.remove(shard.multihash)));
    }
    await this.capability.upload.remove(contentCID);
  }
};
var authorizeContentServe = async (client, space2, connection7, options = {}) => {
  const currentSpace = client.currentSpace();
  try {
    await client.setCurrentSpace(space2.did());
    const audience = {
      did: () => options.audience ?? connection7.id.did()
    };
    const delegation = await client.createDelegation(audience, [space_exports.contentServe.can], {
      expiration: options.expiration ?? Infinity
    });
    const accessProofs = client.proofs([
      { can: access_exports.access.can, with: space2.did() }
    ]);
    const verificationResult = await access_exports.delegate.invoke({
      issuer: client.agent.issuer,
      audience,
      with: space2.did(),
      proofs: [...accessProofs, delegation],
      nb: {
        delegations: {
          [delegation.cid.toString()]: delegation.cid
        }
      }
    }).execute(connection7);
    if (verificationResult.out.error) {
      throw new Error(`failed to publish delegation for audience ${audience.did()}: ${verificationResult.out.error.message}`, {
        cause: verificationResult.out.error
      });
    }
    return { ok: { ...verificationResult.out.ok, delegation } };
  } finally {
    if (currentSpace) {
      await client.setCurrentSpace(currentSpace.did());
    }
  }
};

// node_modules/@storacha/client/dist/ability.js
var setOfAbilities = new Set(abilitiesAsStrings);
function asAbilities(abilities) {
  for (const ability11 of abilities) {
    if (!setOfAbilities.has(
      /** @type {import('@storacha/capabilities/types').ServiceAbility} */
      ability11
    )) {
      throw new Error(`${ability11} is not a supported capability`);
    }
  }
  return (
    /** @type {import('@storacha/capabilities/types').ServiceAbility[]} */
    abilities
  );
}

// node_modules/@storacha/client/dist/index.js
async function create15(options = {}) {
  const store3 = options.store ?? new StoreIndexedDB("w3up-client");
  const raw = await store3.load();
  if (raw) {
    const data2 = AgentData.fromExport(raw, { store: store3 });
    if (options.principal && data2.principal.did() !== options.principal.did()) {
      throw new Error(`store cannot be used with ${options.principal.did()}, stored principal and passed principal must match`);
    }
    return new Client(data2, options);
  }
  const principal2 = options.principal ?? await generate3();
  const data = await AgentData.create({ principal: principal2 }, { store: store3 });
  return new Client(data, options);
}
export {
  account_exports as Account,
  Client,
  result_exports as Result,
  asAbilities,
  authorizeContentServe,
  create15 as create
};
/*! Bundled license information:

@noble/ed25519/lib/esm/index.js:
  (*! noble-ed25519 - MIT License (c) 2019 Paul Miller (paulmillr.com) *)

@scure/base/lib/esm/index.js:
  (*! scure-base - MIT License (c) 2022 Paul Miller (paulmillr.com) *)

@scure/bip39/esm/index.js:
  (*! scure-bip39 - MIT License (c) 2022 Patricio Palladino, Paul Miller (paulmillr.com) *)
*/
//# sourceMappingURL=@storacha_client.js.map
